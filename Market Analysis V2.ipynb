{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "1DYdMWKeEnLW"
      },
      "id": "1DYdMWKeEnLW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compare_dfs"
      ],
      "metadata": {
        "id": "Ke3ctgS_-wt8"
      },
      "id": "Ke3ctgS_-wt8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_df"
      ],
      "metadata": {
        "id": "Icb9MU9nNtv-"
      },
      "id": "Icb9MU9nNtv-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afb574cc"
      },
      "source": [
        "def save_df(df: pd.DataFrame, base_path: str, filename: str):\n",
        "    \"\"\"\n",
        "    Overwrites a file in the base_path and creates a timestamped copy in an 'archive' subdirectory.\n",
        "    If the archive file for the current day already exists, it will not be overwritten.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        base_path (str): The base directory where the file will be saved and archived.\n",
        "        filename (str): The name of the file (e.g., 'gen_lookup.csv').\n",
        "    \"\"\"\n",
        "    include_index_map = {\n",
        "      \"listings.csv\": False,\n",
        "      \"gen_lookup.csv\": False,\n",
        "      \"notes.csv\": True,\n",
        "      \"allocations.csv\": True,\n",
        "    }\n",
        "    # Determine if index should be included for the current filename\n",
        "    include_idx = include_index_map.get(filename, False)\n",
        "\n",
        "    # Construct the full path for the original file\n",
        "    original_filepath = os.path.join(base_path, filename)\n",
        "\n",
        "    # Save (overwrite) the original file\n",
        "    df.to_csv(original_filepath, index=include_idx)\n",
        "    print(f\"Overwrote: {original_filepath}\")\n",
        "\n",
        "    # Create the archive directory path\n",
        "    archive_dir = os.path.join(base_path, 'archive')\n",
        "    os.makedirs(archive_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the archive filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d')\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    archive_filename = f\"{name}_{timestamp}{ext}\"\n",
        "    archive_filepath = os.path.join(archive_dir, archive_filename)\n",
        "\n",
        "    # Check if the archive file already exists before saving\n",
        "    if not os.path.exists(archive_filepath):\n",
        "        # Save the archived file\n",
        "        df.to_csv(archive_filepath, index=include_idx)\n",
        "        print(f\"Archived to: {archive_filepath}\")\n",
        "    else:\n",
        "        print(f\"Archive file already exists for today: {archive_filepath}. Skipping archive save.\")\n",
        "\n",
        "# Example usage:\n",
        "# base_directory = \"/content/drive/Shareddrives/market_analysis_v2/\"\n",
        "# save_df(gen_lookup, base_directory, \"gen_lookup.csv\")"
      ],
      "id": "afb574cc",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## const and helpers"
      ],
      "metadata": {
        "id": "3l2VZo-bNrKq"
      },
      "id": "3l2VZo-bNrKq"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "# --- Carsales/General Scrapes (CS) Constants ---\n",
        "YEAR_MIN, YEAR_MAX = 1980, 2035\n",
        "ORDER: List[str] = ['href', 'year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']\n",
        "\n",
        "YEAR_RE  = r'\\b(19[89]\\d|20[0-3]\\d)\\b'\n",
        "PRICE_RE = r'^(?:AU\\$|\\$)\\s*[\\d,]+(?:\\.\\d{2})?\\b' # Made currency symbol mandatory\n",
        "ODOM_RE  = r'^\\s*\\d+(?:,?\\d{3})*K?\\s*km\\s*$' # Added optional 'K' for Facebook odometer format\n",
        "URL_RE   = r'^(?:https?://|www\\.)'\n",
        "TX, SELLER = {'automatic', 'manual'}, {'private', 'dealer used'}\n",
        "\n",
        "THRESH: Dict[str, float] = {\n",
        "    'year_make_model': 0.50,\n",
        "    \"listed_price\":           0.60,\n",
        "    'transmission':    0.80,\n",
        "    'odometer':        0.60,\n",
        "    'seller_type':     0.70,\n",
        "}\n",
        "\n",
        "# --- Facebook Marketplace (FB) Constants ---\n",
        "FB_ORDER: List[str] = ['href', 'year_make_model', 'listed_price', 'odometer', 'location']\n",
        "THRESH_FB: Dict[str, float] = {\n",
        "    'href':            0.80,\n",
        "    'year_make_model': 0.50,\n",
        "    'listed_price':    0.60,\n",
        "    'odometer':        0.60,\n",
        "    'location':        0.40,\n",
        "}\n",
        "\n",
        "# --- Predicates (Validation Rules) ---\n",
        "def _ratio(mask: pd.Series) -> float:\n",
        "    return float(mask.mean()) if len(mask) else 0.0\n",
        "\n",
        "def _yr_ok(s: pd.Series) -> pd.Series:\n",
        "    years = pd.to_numeric(s.astype(str).str.extract(YEAR_RE, expand=False), errors='coerce')\n",
        "    return years.between(YEAR_MIN, YEAR_MAX)\n",
        "\n",
        "PRED = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    \"listed_price\":           lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'transmission':    lambda s: s.astype(str).str.strip().str.lower().isin(TX),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "    'seller_type':     lambda s: s.astype(str).str.strip().str.lower().isin(SELLER),\n",
        "}\n",
        "\n",
        "PRED_FB = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    'listed_price':    lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "}\n",
        "\n",
        "# --- Core Identification Functions ---\n",
        "def identify_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Carsales/General columns.\"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in ORDER}\n",
        "\n",
        "    href_col = cols[0]\n",
        "\n",
        "    # Exclude URL-like columns from other detection logic\n",
        "    url_ratio = {c: _ratio(df[c].astype(str).str.contains(URL_RE, case=False, na=False)) for c in cols}\n",
        "    urlish = {c for c, r in url_ratio.items() if r >= 0.50}\n",
        "    blocked = {href_col} | urlish\n",
        "\n",
        "    remaining = [c for c in cols if c not in blocked]\n",
        "    picks = {t: None for t in PRED}\n",
        "\n",
        "    for t in PRED:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED[t](df[c])) for c in remaining}\n",
        "        best_col, best_score = max(scores.items(), key=lambda kv: kv[1])\n",
        "        if best_score >= THRESH[t]:\n",
        "            picks[t] = best_col\n",
        "            remaining.remove(best_col)\n",
        "\n",
        "    trim_col = None\n",
        "    ymm = picks.get('year_make_model')\n",
        "    if ymm in cols:\n",
        "        i = cols.index(ymm)\n",
        "        if i + 1 < len(cols):\n",
        "            trim_col = cols[i + 1]\n",
        "\n",
        "    return {'href': href_col, **picks, 'trim': trim_col}\n",
        "\n",
        "def identify_fb_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Facebook Marketplace columns.\n",
        "    Note: 'href' is assumed to be the first column and is handled by clean_fb directly.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in FB_ORDER}\n",
        "\n",
        "    picks = {t: None for t in FB_ORDER}\n",
        "    remaining = set(cols)\n",
        "\n",
        "    # 'href' is now handled externally by clean_fb and is assumed to be the first column\n",
        "    # So we set it to None here or simply don't try to identify it.\n",
        "    # We explicitly remove the first column from 'remaining' as it's the href\n",
        "    if cols and cols[0] in remaining:\n",
        "        remaining.remove(cols[0])\n",
        "    picks['href'] = None # No longer identified by this function\n",
        "\n",
        "    # Identify 'year_make_model', 'listed_price', 'odometer'\n",
        "    for t in ['year_make_model', 'listed_price', 'odometer']:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED_FB[t](df[c])) for c in remaining}\n",
        "        if scores:\n",
        "            best_col, score = max(scores.items(), key=lambda kv: kv[1])\n",
        "            if score >= THRESH_FB[t]:\n",
        "                picks[t] = best_col\n",
        "                remaining.remove(best_col)\n",
        "\n",
        "    # Assign 'location', often found in column 'c' or as the last remaining column\n",
        "    if picks['location'] is None:\n",
        "        if 'c' in remaining:\n",
        "            picks['location'] = 'c'\n",
        "            remaining.remove('c')\n",
        "        elif len(remaining) == 1:\n",
        "            picks['location'] = remaining.pop()\n",
        "\n",
        "    return picks"
      ],
      "metadata": {
        "id": "gECV1vdedUm0"
      },
      "id": "gECV1vdedUm0",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_cs"
      ],
      "metadata": {
        "id": "AUnP9GT2Nn7C"
      },
      "id": "AUnP9GT2Nn7C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5c70d3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_cs(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_cs function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Carsales/General web scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', etc.) using `identify_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Transforms 'odometer' values from 'km' to '000 km' (e.g., 180,000 km -> 180).\n",
        "    4.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_carsales_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_columns(df)\n",
        "    for col in ['year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']:\n",
        "        src = mapping.get(col)\n",
        "        if src is not None and src != out['href'].name:\n",
        "            out[col] = df[src]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_cols = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_cols.columns:\n",
        "            out['year'] = pd.to_numeric(\n",
        "                split_cols[0].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_cols[1] if 1 in split_cols.columns else pd.NA\n",
        "        out['model'] = split_cols[2] if 2 in split_cols.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        # Revert: Remove .str.lower(), keep http(s)://www. prefixes removal\n",
        "        out['href'] = out['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True).str.split('?').str[0]\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    if 'odometer' in out.columns:\n",
        "        out['odometer'] = out['odometer'] // 1000\n",
        "\n",
        "    final_cols = ['href', 'year', 'make', 'model', \"listed_price\", 'trim', 'odometer', 'seller_type']\n",
        "    if save_raw:\n",
        "        final_cols.insert(0, 'raw')\n",
        "    return out[[c for c in final_cols if c in out.columns]]"
      ],
      "execution_count": 39,
      "outputs": [],
      "id": "aa5c70d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_fb"
      ],
      "metadata": {
        "id": "hfCSFOAWNlyg"
      },
      "id": "hfCSFOAWNlyg"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_fb(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_fb function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Facebook Marketplace scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', 'location') using `identify_fb_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Filters out listings with 'listed_price' explicitly marked as \"free\".\n",
        "    4.  **Data Quality Filtering:** Drops rows with missing (`pd.NA`) values in critical columns\n",
        "        ('listed_price', 'odometer', 'year') to ensure data integrity. Also removes listings\n",
        "        with a placeholder 'listed_price' of 12345.\n",
        "    5.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_facebook_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_fb_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_fb_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_fb_columns(df)\n",
        "    for canonical_col, src_col in mapping.items():\n",
        "        if canonical_col != 'href' and src_col is not None and src_col in df.columns:\n",
        "            out[canonical_col] = df[src_col]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_df = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_df.columns:\n",
        "            out['year'] = split_df[0].astype(str).str.replace(r'[^0-9]', '', regex=True).replace('', pd.NA).astype(float).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_df[1] if 1 in split_df.columns else pd.NA\n",
        "        out['model'] = split_df[2] if 2 in split_df.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        # Revert: Remove .str.lower(), keep http(s)://www. prefixes removal\n",
        "        out['href'] = out['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True).str.split('?').str[0]\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            if col == 'listed_price':\n",
        "                out = out[out[col].astype(str).str.lower() != \"free\"]\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^0-9]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    cols_to_check_for_na = []\n",
        "    if 'listed_price' in out.columns: cols_to_check_for_na.append('listed_price')\n",
        "    if 'odometer' in out.columns: cols_to_check_for_na.append('odometer')\n",
        "    if 'year' in out.columns: cols_to_check_for_na.append('year')\n",
        "\n",
        "    if cols_to_check_for_na:\n",
        "        out = out.dropna(subset=cols_to_check_for_na)\n",
        "\n",
        "    final_columns = ['href', 'year', 'make', 'model', \"listed_price\", 'odometer', 'location']\n",
        "    if save_raw:\n",
        "        final_columns.insert(0, 'raw')\n",
        "    return out[[c for c in final_columns if c in out.columns]]"
      ],
      "metadata": {
        "id": "N5MMcCRVEbDl"
      },
      "id": "N5MMcCRVEbDl",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enrich_df"
      ],
      "metadata": {
        "id": "XXQnhOXJNj7S"
      },
      "id": "XXQnhOXJNj7S"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def enrich_df(df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Final clean after clean_cs or clean_fb, including generation assignment.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to enrich.\n",
        "        gen_lookup (pd.DataFrame): A lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The enriched DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Add/Update date_scraped ---\n",
        "    current_timestamp = pd.Timestamp.now().normalize()\n",
        "    if 'date_scraped' not in df.columns:\n",
        "        # Initialize as datetime type with NaT values if column doesn't exist\n",
        "        df['date_scraped'] = pd.Series(pd.NaT, index=df.index, dtype='datetime64[ns]')\n",
        "    else:\n",
        "        # Ensure it's datetime type, coercing errors if it exists but isn't datetime\n",
        "        df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "\n",
        "    # Now fill NaT values with current_timestamp\n",
        "    df['date_scraped'] = df['date_scraped'].fillna(current_timestamp)\n",
        "\n",
        "    # --- 2. Normalise make & model ---\n",
        "    for col in [\"make\", \"model\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .astype(str)\n",
        "                .str.lower()\n",
        "                .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
        "            )\n",
        "\n",
        "    # --- Remove 'https://' or 'http://' and 'www.' from href ---\n",
        "    if 'href' in df.columns:\n",
        "        df['href'] = df['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True)\n",
        "\n",
        "    # --- 3. Ensure year is numeric ---\n",
        "    if \"year\" in df.columns:\n",
        "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # --- 4. Calculate age ---\n",
        "    if 'year' in df.columns:\n",
        "        df['age'] = 2026 - df['year']\n",
        "\n",
        "    # --- 5. Assign generation manually (no merge, no year_start/year_end contamination) ---\n",
        "    df[\"gen\"] = pd.NA\n",
        "\n",
        "    for idx, row in gen_lookup.iterrows():\n",
        "        mask = (\n",
        "            (df[\"make\"] == row[\"make\"]) &\n",
        "            (df[\"model\"] == row[\"model\"]) &\n",
        "            (df[\"year\"].between(row[\"year_start\"], row[\"year_end\"], inclusive=\"both\"))\n",
        "        )\n",
        "        df.loc[mask, \"gen\"] = row[\"gen\"]\n",
        "\n",
        "    df[\"gen\"] = df[\"gen\"].astype(\"Int64\")\n",
        "\n",
        "    # --- 6. Create model_gen ---\n",
        "    df[\"model_gen\"] = df.apply(\n",
        "        lambda r: f\"{r['model']}_{r['gen']}\" if pd.notna(r[\"gen\"]) else None,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "9EoYNNjuIakO"
      },
      "id": "9EoYNNjuIakO",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_bad_listings"
      ],
      "metadata": {
        "id": "ifMZe8Y4NhWe"
      },
      "id": "ifMZe8Y4NhWe"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_bad_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies filters to remove bad or undesirable listings from the DataFrame.\n",
        "    This function is intended to be called after initial cleaning and data type conversions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to filter, expected to have 'year', 'listed_price', and 'odometer' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Price filters as specified by the user\n",
        "    if 'listed_price' in df_filtered.columns:\n",
        "        # Ensure listed_price is numeric for comparison\n",
        "        df_filtered['listed_price'] = pd.to_numeric(df_filtered['listed_price'], errors='coerce')\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] != 12345]\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] > 3000]\n",
        "\n",
        "    # Calculate age temporarily for the odometer filter if 'year' is available\n",
        "    # Assuming 2026 is the reference year for age calculation based on other parts of the notebook\n",
        "    if 'year' in df_filtered.columns:\n",
        "        df_filtered['year'] = pd.to_numeric(df_filtered['year'], errors='coerce') # Ensure year is numeric\n",
        "        temp_age = 2026 - df_filtered['year']\n",
        "    else:\n",
        "        temp_age = pd.Series(pd.NA, index=df_filtered.index) # Create a Series of NA for consistent operations\n",
        "\n",
        "    # Odometer filter: odometer > 2 * age\n",
        "    if 'odometer' in df_filtered.columns:\n",
        "        # Ensure odometer is numeric\n",
        "        df_filtered['odometer'] = pd.to_numeric(df_filtered['odometer'], errors='coerce')\n",
        "\n",
        "        # Create a mask for rows where both odometer and temp_age are valid for comparison\n",
        "        mask_valid_comparison = df_filtered['odometer'].notna() & temp_age.notna()\n",
        "\n",
        "        # Filter out rows where (odometer is NOT > 2 * age) AND (the comparison is valid)\n",
        "        # We keep rows where (odometer > 2 * age) OR (the comparison cannot be made due to NA values)\n",
        "        df_filtered = df_filtered[~((df_filtered['odometer'] <= 2 * temp_age) & mask_valid_comparison)]\n",
        "\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "PuyPlWYcDWcc"
      },
      "id": "PuyPlWYcDWcc",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare_new_listings"
      ],
      "metadata": {
        "id": "xIoIjpiuNfDH"
      },
      "id": "xIoIjpiuNfDH"
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_new_listings(listings: pd.DataFrame, gen_lookup: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Processes new listing files, cleans, enriches, and compares them against existing listings.\n",
        "\n",
        "    Args:\n",
        "        listings (pd.DataFrame): Existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): Lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, int, int, int, int]: A tuple containing:\n",
        "            - enriched_new_listings (pd.DataFrame): DataFrame of newly processed and enriched listings.\n",
        "            - unq_new (int): Total count of truly new unique listings.\n",
        "            - unq_updated (int): Total count of updated unique listings.\n",
        "            - unq_unchanged (int): Total count of unchanged unique listings.\n",
        "            - unq_tot (int): Total count of all unique listings processed from new files.\n",
        "    \"\"\"\n",
        "    # Sets to track unique hrefs across all processed files\n",
        "    unique_new_hrefs = set()\n",
        "    unique_updated_hrefs = set()\n",
        "    unique_unchanged_hrefs = set()\n",
        "    unique_total_hrefs = set()\n",
        "\n",
        "    enriched_new_listings = pd.DataFrame()\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "\n",
        "    for file_path in cs_files + fb_files:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None and not df_cleaned.empty:\n",
        "            unique_total_hrefs.update(df_cleaned['href'].tolist())\n",
        "\n",
        "        # Checking how many new, updated, unchanged listings\n",
        "        df_comparison = pd.merge(\n",
        "            df_cleaned,\n",
        "            listings,\n",
        "            on='href',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_existing')\n",
        "        )\n",
        "\n",
        "        # Identify new listings\n",
        "        new_listings_df = df_comparison[df_comparison['listed_price_existing'].isnull()]\n",
        "        n_new = len(new_listings_df)\n",
        "        if not new_listings_df.empty:\n",
        "            unique_new_hrefs.update(new_listings_df['href'].tolist())\n",
        "\n",
        "        # Identify matched listings\n",
        "        matched_listings_df = df_comparison[df_comparison['listed_price_existing'].notnull()]\n",
        "\n",
        "        # From matched_listings, identify updated listings\n",
        "        updated_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] != matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_updated = len(updated_listings_df)\n",
        "        if not updated_listings_df.empty:\n",
        "            unique_updated_hrefs.update(updated_listings_df['href'].tolist())\n",
        "\n",
        "        # From matched_listings, identify unchanged listings\n",
        "        unchanged_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] == matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_unchanged = len(unchanged_listings_df)\n",
        "        if not unchanged_listings_df.empty:\n",
        "            unique_unchanged_hrefs.update(unchanged_listings_df['href'].tolist())\n",
        "\n",
        "        # Calculate total listings for the current file\n",
        "        n_total_listings = len(df_cleaned)\n",
        "\n",
        "        # Print the comparison result for the current file\n",
        "        print(f\"{file_path}    \\t {n_new=}   \\t {n_updated=} \\t {n_unchanged=} \\t Tot {n_total_listings}\")\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            enriched_new_listings = pd.concat([enriched_new_listings, df_enriched], ignore_index=True)\n",
        "\n",
        "    # Calculate unique total counts at the end\n",
        "    unq_new = len(unique_new_hrefs)\n",
        "    unq_updated = len(unique_updated_hrefs)\n",
        "    unq_unchanged = len(unique_unchanged_hrefs)\n",
        "    unq_tot = len(unique_total_hrefs)\n",
        "\n",
        "    print(f\"\\t \\t \\t \\t {unq_new=} \\t {unq_updated=}\\t {unq_unchanged=} {unq_tot=}\")\n",
        "\n",
        "    # Check for missing values in enriched_new_listings after concatenation\n",
        "    if not enriched_new_listings.empty:\n",
        "        for col in ['model_gen', 'age', 'odometer']:\n",
        "            if col in enriched_new_listings.columns and enriched_new_listings[col].isna().any():\n",
        "                missing_count = enriched_new_listings[col].isna().sum()\n",
        "                print(f\"WARNING: Column '{col}' in enriched_new_listings has {missing_count} missing values.\")\n",
        "\n",
        "\n",
        "    return enriched_new_listings"
      ],
      "metadata": {
        "id": "elwfHrt9xAZ9"
      },
      "id": "elwfHrt9xAZ9",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## integrate_listings"
      ],
      "metadata": {
        "id": "DNtbnWgdNbxT"
      },
      "id": "DNtbnWgdNbxT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23a71e21"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "import glob # Import glob for file pattern matching\n",
        "\n",
        "def integrate_listings(listings_df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Integrates new car listings from '/content/carsales*.csv' and '/content/facebook*.csv' files into an existing listings DataFrame.\n",
        "\n",
        "    Args:\n",
        "        listings_df (pd.DataFrame): The existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): The lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame (`listings_1`) with integrated, cleaned, and enriched listings,\n",
        "                      with existing listings handled by keeping the most recent entry.\n",
        "    \"\"\"\n",
        "    processed_dfs = []\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "    new_file_paths = cs_files + fb_files\n",
        "\n",
        "    for file_path in new_file_paths:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            processed_dfs.append(df_enriched)\n",
        "\n",
        "    if processed_dfs:\n",
        "        new_listings_df = pd.concat(processed_dfs, ignore_index=True)\n",
        "\n",
        "        # Define all possible columns that might exist in either DataFrame\n",
        "        # Get columns from existing listings and new listings, handling potential differences\n",
        "        all_cols = list(set(listings_df.columns) | set(new_listings_df.columns))\n",
        "\n",
        "        # Reindex both DataFrames to ensure they have the same columns\n",
        "        listings_aligned = listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "        new_listings_aligned = new_listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "\n",
        "        # Ensure 'date_scraped' is in datetime format for proper sorting\n",
        "        listings_aligned['date_scraped'] = pd.to_datetime(listings_aligned['date_scraped'], errors='coerce')\n",
        "        new_listings_aligned['date_scraped'] = pd.to_datetime(new_listings_aligned['date_scraped'], errors='coerce')\n",
        "\n",
        "        # Explicitly cast dtypes of new_listings_aligned to match listings_aligned for common columns\n",
        "        # This helps prevent FutureWarning and ensures consistent types across the concatenated DataFrame\n",
        "        for col in all_cols:\n",
        "            if col in listings_aligned.columns and col in new_listings_aligned.columns:\n",
        "                if listings_aligned[col].dtype != new_listings_aligned[col].dtype:\n",
        "                    try:\n",
        "                        if pd.api.types.is_numeric_dtype(listings_aligned[col]):\n",
        "                            if str(listings_aligned[col].dtype) == 'Int64':\n",
        "                                new_listings_aligned[col] = new_listings_aligned[col].astype('Int64')\n",
        "                            else:\n",
        "                                new_listings_aligned[col] = pd.to_numeric(new_listings_aligned[col], errors='coerce').astype(listings_aligned[col].dtype)\n",
        "                        else:\n",
        "                            new_listings_aligned[col] = new_listings_aligned[col].astype(listings_aligned[col].dtype)\n",
        "                    except (TypeError, ValueError):\n",
        "                        pass # Keep original dtype if casting causes error\n",
        "\n",
        "        # Concatenate the aligned Dataframes\n",
        "        listings_1 = pd.concat([listings_aligned, new_listings_aligned], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No new listings\")\n",
        "        return listings_df # Return the original listings_df if no new listings were processed\n",
        "\n",
        "\n",
        "    # Sort by href, then listed_price (lowest first), then date_scraped (most recent first), then drop duplicates keeping the first\n",
        "    listings_1 = listings_1.sort_values(by=['href', 'listed_price', 'date_scraped'], ascending=[True, True, True])\n",
        "    listings_1 = listings_1.drop_duplicates(subset=['href'], keep='first')\n",
        "    listings_1 = remove_bad_listings(listings_1)\n",
        "\n",
        "    # Ensure 'gen' column is Int64 after all operations\n",
        "    listings_1['gen'] = listings_1['gen'].astype('Int64')\n",
        "\n",
        "    print(f\"Final DataFrame has {len(listings_1)} unique listings after merging and de-duplication.\")\n",
        "    return listings_1"
      ],
      "id": "23a71e21",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## allocate_listings"
      ],
      "metadata": {
        "id": "SUj2yn4nNTDB"
      },
      "id": "SUj2yn4nNTDB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import date\n",
        "from typing import Optional, List\n",
        "\n",
        "def allocate_listings(listings_lr: pd.DataFrame, notes: pd.DataFrame, allocations: pd.DataFrame, clients_to_process: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Allocates car listings based on universal standards, client-specific criteria, and notes/allocation history.\n",
        "\n",
        "    Business Logic for allocate_listings function:\n",
        "\n",
        "    This function identifies and allocates suitable car listings to clients based on a multi-step filtering process:\n",
        "\n",
        "    1.  **Client Identification:** Determines which clients to process based on `clients_to_process` or processes all global clients if none are specified.\n",
        "    2.  **Universal Filters:** Applies initial filters to `listings_lr`:\n",
        "        *   `odometer` must be greater than 4 times the `age` of the car.\n",
        "        *   `listed_price` must be less than 95% of the `market_value`.\n",
        "    3.  **Client-Specific Allocation:** Iterates through each active client and applies their specific criteria:\n",
        "        *   `max_listing_price`: Listing price must be at or below the client's maximum price.\n",
        "        *   `max_odometer`: Odometer reading must be at or below the client's maximum odometer.\n",
        "        *   `model_gens_allowed`: The `model_gen` of the listing must match one of the client's allowed model generations (allowing for partial matches).\n",
        "    4.  **Duplicate Allocation Prevention:** New allocation records are created. Before adding them to the `allocations` DataFrame, the function checks if an (href, client) pair already exists in the historical `allocations` to prevent re-allocating an already processed listing.\n",
        "    5.  **Output:** Returns an updated `allocations` DataFrame containing all previous allocations plus any newly proposed and unique allocations.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): The DataFrame of car listings with regression results (market_value, excess_value).\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        clients_to_process (Optional[List[str]]): List of client names to process. If None, all global clients are processed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An updated allocations DataFrame containing newly proposed allocations.\n",
        "    \"\"\"\n",
        "\n",
        "    global clients # Access the global list of client configuration dictionaries\n",
        "\n",
        "    # Determine which clients to actually process\n",
        "    effective_clients_info = []\n",
        "    if clients_to_process is None:\n",
        "        effective_clients_info = clients # Process all clients\n",
        "    else:\n",
        "        # Filter global clients to get the dictionaries for specified client names\n",
        "        effective_clients_info = [c_info for c_info in clients if c_info['client'] in clients_to_process]\n",
        "\n",
        "    if not effective_clients_info:\n",
        "        print(\"No clients specified or found to process for allocations.\")\n",
        "        return allocations\n",
        "\n",
        "    # Make copies to avoid modifying original DataFrames\n",
        "    listings_filtered = listings_lr.copy()\n",
        "    notes_filtered = notes.copy() # notes_filtered is still used to filter out listings, but will not be changed here\n",
        "    current_allocations = allocations.copy()\n",
        "\n",
        "    # 1. Apply Universal Filters\n",
        "    listings_filtered = listings_filtered[\n",
        "        (listings_filtered['odometer'] > 4 * listings_filtered['age']) &\n",
        "        (listings_filtered['listed_price'] < 0.95 * listings_filtered['market_value'])\n",
        "    ]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after universal filters.\")\n",
        "        return allocations\n",
        "\n",
        "    # Ensure 'excess_value' is present for sorting\n",
        "    if 'excess_value' not in listings_filtered.columns:\n",
        "        print(\"Error: 'excess_value' column is missing for sorting.\")\n",
        "        return allocations\n",
        "\n",
        "    new_allocation_records = []\n",
        "    current_timestamp = pd.Timestamp.now()\n",
        "\n",
        "    # 2. Iterate through each specified client for allocations\n",
        "    for client_info in effective_clients_info:\n",
        "        current_client_name = client_info['client']\n",
        "        max_price = client_info['max_listing_price']\n",
        "        max_odometer = client_info['max_odometer']\n",
        "        model_gens_allowed = client_info['model_gens']\n",
        "\n",
        "        # Client-specific criteria\n",
        "        price_cond = listings_filtered['listed_price'] <= max_price\n",
        "        odometer_cond = listings_filtered['odometer'] <= max_odometer\n",
        "\n",
        "        # Model generation condition (using str.startswith for broader matching)\n",
        "        model_gen_cond = pd.Series(False, index=listings_filtered.index)\n",
        "        if 'model_gen' in listings_filtered.columns and model_gens_allowed:\n",
        "            for allowed_gen_pattern in model_gens_allowed:\n",
        "                model_gen_cond = model_gen_cond | (\n",
        "                    listings_filtered['model_gen'].astype(str).str.startswith(allowed_gen_pattern)\n",
        "                )\n",
        "\n",
        "        client_eligible_listings = listings_filtered[\n",
        "            price_cond & odometer_cond & model_gen_cond\n",
        "        ].copy()\n",
        "\n",
        "        if not client_eligible_listings.empty:\n",
        "            for _, listing_row in client_eligible_listings.iterrows():\n",
        "                href = listing_row['href']\n",
        "                new_allocation_records.append({\n",
        "                    'href': href,\n",
        "                    'client': current_client_name,\n",
        "                    'allocation': True,\n",
        "                    'timestamp': current_timestamp\n",
        "                })\n",
        "\n",
        "    if new_allocation_records:\n",
        "        new_allocations_df = pd.DataFrame(new_allocation_records)\n",
        "        new_allocations_df['timestamp'] = pd.to_datetime(new_allocations_df['timestamp'])\n",
        "        new_allocations_df['allocation'] = new_allocations_df['allocation'].astype('boolean')\n",
        "\n",
        "\n",
        "        # Filter out new allocations that are already present in the existing 'allocations' DataFrame\n",
        "        existing_allocation_keys = allocations[['href', 'client']].drop_duplicates()\n",
        "        merged_df = pd.merge(\n",
        "            new_allocations_df,\n",
        "            existing_allocation_keys,\n",
        "            on=['href', 'client'],\n",
        "            how='left',\n",
        "            indicator=True\n",
        "        )\n",
        "        truly_new_allocations = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "\n",
        "        # 3. Concatenate the truly new allocation records with the existing allocations DataFrame\n",
        "        allocations = pd.concat([allocations, truly_new_allocations], ignore_index=True)\n",
        "        allocations['allocation'] = allocations['allocation'].astype('boolean')\n",
        "        print(f\"Added {len(truly_new_allocations)} new allocation entries.\")\n",
        "    else:\n",
        "        print(\"No new allocations found based on current criteria.\")\n",
        "\n",
        "    return allocations"
      ],
      "metadata": {
        "id": "HmF0gPBJzvkW"
      },
      "id": "HmF0gPBJzvkW",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get_best_listings"
      ],
      "metadata": {
        "id": "6JCPqZfk9Sy6"
      },
      "id": "6JCPqZfk9Sy6"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_listings(listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes: pd.DataFrame, clients: List[Dict], newer_than_date: Optional[str] = None, n_top: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies the best car listings for each client based on allocation, status, and excess value.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results (e.g., market_value, excess_value).\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        clients (List[Dict]): List of client configuration dictionaries.\n",
        "        newer_than_date (Optional[str]): If provided, filter listings scraped on or after this date. Format 'YYYY-MM-DD'.\n",
        "        n_top (int): Number of top listings to select for each client.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of unique hrefs representing the best listings across all clients.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Make copies of the input DataFrames\n",
        "    df = listings_lr.copy()\n",
        "    allocations_copy = allocations.copy()\n",
        "    notes_copy = notes.copy()\n",
        "\n",
        "    # 2. Convert date columns to datetime objects\n",
        "    df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "    notes_copy['timestamp'] = pd.to_datetime(notes_copy['timestamp'], errors='coerce')\n",
        "\n",
        "    # 3. Process notes_copy to determine the latest status for each unique href\n",
        "    # Sort notes by href and timestamp (descending) to get the latest status per href\n",
        "    notes_copy_sorted = notes_copy.sort_values(by=['href', 'timestamp'], ascending=[True, False])\n",
        "    # Drop duplicates, keeping the first (which will be the latest status for each href)\n",
        "    latest_notes_status = notes_copy_sorted.drop_duplicates(subset=['href'], keep='first')\n",
        "    latest_notes_status = latest_notes_status[['href', 'status']].rename(columns={'status': 'last_status'})\n",
        "\n",
        "    # 4. Merge df with this last_status information\n",
        "    df = pd.merge(df, latest_notes_status, on='href', how='left')\n",
        "\n",
        "    # Create client allocation flags\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        alloc_mask = (allocations_copy['client'] == client_name) & (allocations_copy['allocation'] == True)\n",
        "        allocated_hrefs = allocations_copy[alloc_mask]['href'].unique()\n",
        "        df[f'client_{client_name}'] = df['href'].isin(allocated_hrefs)\n",
        "\n",
        "    # 5. Implement date filtering\n",
        "    if newer_than_date:\n",
        "        newer_than_date_dt = pd.to_datetime(newer_than_date)\n",
        "        df = df[df['date_scraped'] >= newer_than_date_dt]\n",
        "    else:\n",
        "        # If no specific date is given, filter for the latest date_scraped\n",
        "        if not df.empty:\n",
        "            latest_scraped_date = df['date_scraped'].max()\n",
        "            df = df[df['date_scraped'] == latest_scraped_date]\n",
        "\n",
        "    # 6. Further filter df to keep only listings where last_status is either None (missing) or 'seen'\n",
        "    # Convert 'last_status' to string to handle both NaN and actual string values consistently\n",
        "    df = df[df['last_status'].isna() | (df['last_status'] == 'seen')]\n",
        "\n",
        "    # 7. Ensure the excess_value column in df is numeric and drop any rows where it is missing\n",
        "    df['excess_value'] = pd.to_numeric(df['excess_value'], errors='coerce')\n",
        "    df.dropna(subset=['excess_value'], inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No listings remain after initial filtering.\")\n",
        "        return []\n",
        "\n",
        "    all_best_hrefs = []\n",
        "\n",
        "    # 8. Iterate through each client and select top listings\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        client_col = f'client_{client_name}'\n",
        "\n",
        "        # Filter for listings allocated to the current client\n",
        "        client_df = df[df[client_col] == True].copy()\n",
        "\n",
        "        if not client_df.empty:\n",
        "            # Sort by excess_value in descending order and get the top n_top hrefs\n",
        "            top_listings_for_client = client_df.sort_values(by='excess_value', ascending=False).head(n_top)\n",
        "            all_best_hrefs.extend(top_listings_for_client['href'].tolist())\n",
        "\n",
        "    # 9. Convert all_best_hrefs to a Series, remove duplicate hrefs, and return as a list\n",
        "    unique_best_hrefs = pd.Series(all_best_hrefs).drop_duplicates().tolist()\n",
        "\n",
        "    return unique_best_hrefs"
      ],
      "metadata": {
        "id": "wFQzH6L-LQ8B"
      },
      "id": "wFQzH6L-LQ8B",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## write_yaml"
      ],
      "metadata": {
        "id": "rTmF-kcqNN7P"
      },
      "id": "rTmF-kcqNN7P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e025f4f7"
      },
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "from datetime import datetime, date\n",
        "import numpy as np\n",
        "from google.colab import files # Import files for download functionality\n",
        "from typing import List, Tuple\n",
        "\n",
        "def write_yaml(listings_to_print: List[str], listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes_df: pd.DataFrame, out_file: Optional[str] = None, download: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Consolidates listing data from shortlist and notes DataFrames and saves it to a YAML file.\n",
        "    Dynamically adds client eligibility flags from shortlist columns.\n",
        "    If a listing has no status, it adds a 'seen' status with author 'beep_boop'.\n",
        "\n",
        "    Args:\n",
        "        listings_to_print (List[str]): List of hrefs to include in the YAML output.\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes_df (pd.DataFrame): DataFrame containing notes associated with listings.\n",
        "        out_file (Optional[str]): The filename to save the YAML to. If None, defaults to 'shortlist.yaml'.\n",
        "        download (bool): If True, the generated YAML file will be prompted for download.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame, potentially with new 'seen' entries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to convert pandas-specific types to standard Python equivalents\n",
        "    def to_python_type(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "        if isinstance(value, pd.Timestamp):\n",
        "            return value.to_pydatetime() # Convert pandas Timestamp to datetime object\n",
        "        if isinstance(value, (pd.Int64Dtype, np.int64)):\n",
        "            return int(value)\n",
        "        if isinstance(value, (pd.Float64Dtype, np.float64)):\n",
        "            return float(value)\n",
        "        if isinstance(value, (date, datetime)): # Use datetime.date and datetime\n",
        "            return value\n",
        "        return value\n",
        "\n",
        "    # Prepare notes_df (make a copy to ensure any internal modifications are to this copy)\n",
        "    current_notes_df = notes_df.copy()\n",
        "    current_notes_df['timestamp'] = pd.to_datetime(current_notes_df['timestamp'], errors='coerce')\n",
        "    current_notes_df.dropna(subset=['timestamp'], inplace=True)\n",
        "\n",
        "    # Filter listings_lr to include only the listings specified in listings_to_print\n",
        "    # Then sort by 'excess_value' in descending order\n",
        "    current_shortlist = listings_lr[listings_lr['href'].isin(listings_to_print)].copy()\n",
        "    if 'excess_value' in current_shortlist.columns:\n",
        "        current_shortlist = current_shortlist.sort_values(by='excess_value', ascending=False)\n",
        "    else:\n",
        "        print(\"Warning: 'excess_value' column not found, cannot sort by it.\")\n",
        "\n",
        "    all_listings_data = []\n",
        "\n",
        "    for idx, row in current_shortlist.iterrows():\n",
        "        href = row['href']\n",
        "        initial_status_for_listing = None # This will hold the status *before* any 'seen' logic\n",
        "        current_notes_for_listing = [] # Notes associated *before* any 'seen' logic\n",
        "\n",
        "        matching_notes = current_notes_df[current_notes_df['href'] == href]\n",
        "\n",
        "        if not matching_notes.empty:\n",
        "            matching_notes_sorted = matching_notes.sort_values(by='timestamp', ascending=False)\n",
        "            initial_status_for_listing = to_python_type(matching_notes_sorted.iloc[0]['status'])\n",
        "            current_notes_for_listing = [to_python_type(n) for n in matching_notes_sorted['note'].tolist() if pd.notna(n)]\n",
        "\n",
        "        # Create a dictionary named listing_data with the specified order and format\n",
        "        listing_data = {\n",
        "            'title': f\"{to_python_type(row['year'])}, {to_python_type(row['model_gen'])}, {int(to_python_type(row['odometer']))}k\",\n",
        "            'seller': to_python_type(row['seller']), # Modified to use seller\n",
        "            'listed_price': to_python_type(row['listed_price']),\n",
        "            'excess_value': int(to_python_type(row['excess_value'])), # Convert to int here\n",
        "            'href': to_python_type(row['href'])\n",
        "        }\n",
        "\n",
        "        # Dynamically add client eligibility from the allocations DataFrame\n",
        "        eligible_clients = allocations[\n",
        "            (allocations['href'] == href) & (allocations['allocation'] == True)\n",
        "        ]['client'].unique().tolist()\n",
        "        listing_data['clients'] = eligible_clients\n",
        "\n",
        "        # Add status and notes based on initial values\n",
        "        listing_data['status'] = initial_status_for_listing\n",
        "        listing_data['notes'] = current_notes_for_listing\n",
        "\n",
        "        all_listings_data.append(listing_data)\n",
        "\n",
        "        # Now, if the listing had no status, add 'seen' to current_notes_df for the *next* iteration\n",
        "        if initial_status_for_listing is None:\n",
        "            current_notes_df = add_note(current_notes_df, 'beep_boop', href, status='seen')\n",
        "\n",
        "    output_filename = out_file if out_file is not None else 'shortlist.yaml'\n",
        "    # Initialize yaml_content list\n",
        "    yaml_content = []\n",
        "    for listing in all_listings_data:\n",
        "        yaml_content.append('---\\n') # Add separator before each listing\n",
        "        yaml_content.append(yaml.dump(listing, allow_unicode=True, sort_keys=False))\n",
        "        yaml_content.append('\\n') # Add an extra newline after each dumped listing for readability\n",
        "\n",
        "    yaml_content_str = \"\".join(yaml_content)\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.write(yaml_content_str)\n",
        "\n",
        "    if download:\n",
        "        files.download(output_filename)\n",
        "        print(f\"The YAML file '{output_filename}' has been generated and prompted for download with {len(all_listings_data)} listings.\")\n",
        "    else:\n",
        "        print(f\"The YAML file '{output_filename}' has been generated with {len(all_listings_data)} listings (download skipped).\")\n",
        "\n",
        "    return current_notes_df # Return the potentially updated notes_df\n"
      ],
      "id": "e025f4f7",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apply_regression"
      ],
      "metadata": {
        "id": "1aAGDDUkOKsL"
      },
      "id": "1aAGDDUkOKsL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23f91c4e"
      },
      "source": [
        "def apply_regression(df: pd.DataFrame) -> (pd.DataFrame, pd.Series):\n",
        "    \"\"\"\n",
        "    Applies Huber regression to the input DataFrame to predict car prices.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing car listings.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame, pd.Series): A tuple containing:\n",
        "            - The DataFrame with 'market_value' and 'excess_value' columns added.\n",
        "            - A Series of unscaled regression coefficients.\n",
        "    \"\"\"\n",
        "    listings_lr = df.copy()\n",
        "\n",
        "    # 1) Coerce numeric types\n",
        "    listings_lr['year'] = pd.to_numeric(listings_lr['year'], errors='coerce')\n",
        "    listings_lr['odometer'] = pd.to_numeric(listings_lr['odometer'], errors='coerce')\n",
        "    listings_lr[\"listed_price\"] = pd.to_numeric(listings_lr[\"listed_price\"], errors='coerce')\n",
        "\n",
        "    # 2) One-hot encode model_gen\n",
        "    listings_lr[\"model_gen\"] = listings_lr[\"model_gen\"].astype(str)\n",
        "    dummies = pd.get_dummies(listings_lr[\"model_gen\"], prefix=\"mg_\", prefix_sep=\"\")\n",
        "\n",
        "    # remove base category \"civic_9\" if it exists\n",
        "    base_col = \"mg_civic_9\" # Corrected base column name to match dummy format\n",
        "    if base_col in dummies.columns:\n",
        "        dummies = dummies.drop(columns=[base_col])\n",
        "\n",
        "    listings_lr = pd.concat([listings_lr, dummies], axis=1)\n",
        "\n",
        "    # 3) Build X, y & keep mask\n",
        "    predictor_cols = ['age', 'odometer'] + list(dummies.columns)\n",
        "    X = listings_lr[predictor_cols].astype(float)\n",
        "    y = listings_lr[\"listed_price\"].astype(float)\n",
        "\n",
        "    keep = X.notna().all(axis=1) & y.notna()\n",
        "\n",
        "    X_keep = X.loc[keep]\n",
        "    y_keep = y.loc[keep]\n",
        "\n",
        "    # 4) Scale predictors\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_keep)\n",
        "\n",
        "    # 5) Fit Huber Regression\n",
        "    huber = HuberRegressor(max_iter=1000, epsilon=1.5)\n",
        "    huber.fit(X_scaled, y_keep)\n",
        "\n",
        "    # 6) Predict & store results\n",
        "    pred = huber.predict(X_scaled)\n",
        "    listings_lr.loc[keep, \"market_value\"] = pred\n",
        "    listings_lr.loc[keep, \"excess_value\"] = pred - listings_lr.loc[keep, \"listed_price\"]\n",
        "\n",
        "    # 7) Recover coefficients on the original (unscaled) feature scale\n",
        "    coef_scaled = huber.coef_\n",
        "    mu = scaler.mean_\n",
        "    sigma = scaler.scale_\n",
        "\n",
        "    original_intercept = huber.intercept_ - np.sum(coef_scaled * (mu / sigma))\n",
        "    original_coefs = coef_scaled / sigma\n",
        "\n",
        "    coef_unscaled = pd.Series(\n",
        "        np.concatenate([[original_intercept], original_coefs]),\n",
        "        index=[\"intercept\"] + predictor_cols\n",
        "    )\n",
        "\n",
        "    listings_lr = listings_lr.loc[:, ~listings_lr.columns.str.startswith(\"mg_\")]\n",
        "\n",
        "    return listings_lr, coef_unscaled"
      ],
      "id": "23f91c4e",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##add_note"
      ],
      "metadata": {
        "id": "h6aKUgtSfCr3"
      },
      "id": "h6aKUgtSfCr3"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def add_note(notes_df: pd.DataFrame, author: str, href: str, status: str = None, note: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a new note or updates a status for a specific href in the `notes_df` DataFrame.\n",
        "    Checks if the status/note already exists and adds it only if new.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The DataFrame containing historical notes and statuses.\n",
        "        author (str): The author of the note/status update.\n",
        "        href (str): The href of the listing to update.\n",
        "        status (str, optional): The new status. Defaults to None.\n",
        "        note (str, optional): The new note. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    current_notes = notes_df.copy()\n",
        "\n",
        "    # Ensure 'timestamp' column is datetime for comparison\n",
        "    current_notes['timestamp'] = pd.to_datetime(current_notes['timestamp'], errors='coerce')\n",
        "\n",
        "    current_timestamp = pd.Timestamp.now(tz='UTC')\n",
        "    new_entries = []\n",
        "    href = href.replace('https://', '').replace('http://', '').replace('www.', '') # Clean href\n",
        "\n",
        "    # Filter existing records for the current href\n",
        "    existing_notes_for_href = current_notes[current_notes['href'] == href].copy()\n",
        "    existing_notes_for_href.sort_values(by='timestamp', ascending=False, inplace=True)\n",
        "\n",
        "    latest_status_in_notes = None\n",
        "    if not existing_notes_for_href.empty:\n",
        "        latest_status_in_notes = existing_notes_for_href.iloc[0]['status']\n",
        "\n",
        "    existing_note_texts = set(existing_notes_for_href['note'].dropna().tolist())\n",
        "\n",
        "    # Check and add new status if provided and different\n",
        "    if status is not None and status != latest_status_in_notes:\n",
        "        new_entries.append({\n",
        "            'href': href,\n",
        "            'timestamp': current_timestamp,\n",
        "            'author': author,\n",
        "            'status': status,\n",
        "            'note': pd.NA\n",
        "        })\n",
        "\n",
        "    # Check and add new note if provided and not already existing\n",
        "    if note is not None and note not in existing_note_texts:\n",
        "        new_entries.append({\n",
        "            'href': href,\n",
        "            'timestamp': current_timestamp,\n",
        "            'author': author,\n",
        "            'status': pd.NA, # Status is not changing, just adding a note\n",
        "            'note': note\n",
        "        })\n",
        "\n",
        "    if new_entries:\n",
        "        new_notes_df = pd.DataFrame(new_entries)\n",
        "        new_notes_df['timestamp'] = pd.to_datetime(new_notes_df['timestamp'])\n",
        "        # Ensure consistent column order and dtypes. If current_notes is empty, ensure new_notes_df has the correct columns.\n",
        "        if current_notes.empty:\n",
        "            # Define columns if current_notes is empty, assuming standard notes DataFrame columns\n",
        "            # This is a fallback and might need adjustment if notes columns vary significantly.\n",
        "            new_notes_df = new_notes_df.reindex(columns=['href', 'timestamp', 'author', 'status', 'note'])\n",
        "        else:\n",
        "            new_notes_df = new_notes_df.reindex(columns=current_notes.columns)\n",
        "\n",
        "        current_notes = pd.concat([current_notes, new_notes_df], ignore_index=True)\n",
        "\n",
        "        # --- User's requested logic for YAML export ---\n",
        "        if status in [\"message_left\", \"follow_up\"]:\n",
        "\n",
        "            # Get the latest status for each href in the *updated* notes DataFrame\n",
        "            latest_notes_status = current_notes.sort_values(by='timestamp', ascending=False).drop_duplicates(subset=['href'], keep='first')\n",
        "\n",
        "            # Filter for 'message_left' or 'follow_up' statuses\n",
        "            hrefs_for_yaml = latest_notes_status[\n",
        "                latest_notes_status['status'].isin([\"message_left\", \"follow_up\"])\n",
        "            ]['href'].unique().tolist()\n",
        "\n",
        "            if hrefs_for_yaml:\n",
        "                print(f\"Found {len(hrefs_for_yaml)} listings with current status 'message_left' or 'follow_up'.\")\n",
        "                # Access global variables for write_yaml\n",
        "                # These are assumed to be defined in the global scope of the notebook.\n",
        "                from __main__ import listings_lr, allocations, write_yaml\n",
        "\n",
        "                output_path = \"/content/drive/Shareddrives/market_analysis_v2/message_left.yaml\"\n",
        "                write_yaml(\n",
        "                    listings_to_print=hrefs_for_yaml,\n",
        "                    listings_lr=listings_lr,\n",
        "                    allocations=allocations,\n",
        "                    notes_df=current_notes, # Pass the updated notes DataFrame\n",
        "                    out_file=output_path,\n",
        "                    download=False # Do not prompt for download in a background task\n",
        "                )\n",
        "        # --- End of user's requested logic ---\n",
        "\n",
        "    return current_notes"
      ],
      "metadata": {
        "id": "3AqI6jYKe9GJ"
      },
      "id": "3AqI6jYKe9GJ",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_notes"
      ],
      "metadata": {
        "id": "rylENVkd9dWD"
      },
      "id": "rylENVkd9dWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def update_notes(notes_df: pd.DataFrame, update_yaml: list, author: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes shortlist data and updates the `notes_df` DataFrame\n",
        "    with new status changes and notes using the `add_note` function.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The DataFrame containing historical notes and statuses.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "        author (str): The author of the notes.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame.\n",
        "    \"\"\"\n",
        "    updated_notes_df = notes_df.copy()\n",
        "    initial_notes_count = len(updated_notes_df)\n",
        "\n",
        "    for listing in update_yaml:\n",
        "        current_href = listing['href']\n",
        "        current_status_from_yaml = listing['status']\n",
        "        notes_list_from_yaml = listing['notes'] if listing['notes'] is not None else []\n",
        "\n",
        "        # Update status using add_note\n",
        "        updated_notes_df = add_note(updated_notes_df, author, current_href, status=current_status_from_yaml)\n",
        "\n",
        "        # Update individual notes using add_note\n",
        "        for note_text in notes_list_from_yaml:\n",
        "            if pd.notna(note_text): # Ensure note_text is not NaN before passing\n",
        "                updated_notes_df = add_note(updated_notes_df, author, current_href, note=note_text)\n",
        "\n",
        "    final_notes_count = len(updated_notes_df)\n",
        "    added_entries_count = final_notes_count - initial_notes_count\n",
        "\n",
        "    if added_entries_count > 0:\n",
        "        print(f\"Total {added_entries_count} new entries added to notes DataFrame through update_notes.\")\n",
        "    else:\n",
        "        print(\"No new notes or status updates to add via update_notes.\")\n",
        "\n",
        "    return updated_notes_df"
      ],
      "metadata": {
        "id": "2dhYD0Mv9dy4"
      },
      "id": "2dhYD0Mv9dy4",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_seller"
      ],
      "metadata": {
        "id": "0sQh5O7bDJyK"
      },
      "id": "0sQh5O7bDJyK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def update_seller(listings_df: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the 'seller' column in listings_df based on information from update_yaml.\n",
        "\n",
        "    Args:\n",
        "        update_yaml (list): A list of dictionaries parsed from a YAML file, potentially containing\n",
        "                                   'href' and 'seller' information.\n",
        "        listings_df (pd.DataFrame): The DataFrame of car listings to be updated.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified listings_df with updated 'seller' information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    updated_listings_df = listings_df.copy()\n",
        "\n",
        "    # Ensure 'seller' column is of object (string) type to accommodate string assignments\n",
        "    # This prevents FutureWarning when assigning strings to a float64 column that might contain NaNs\n",
        "    if 'seller' in updated_listings_df.columns and updated_listings_df['seller'].dtype != object:\n",
        "        updated_listings_df['seller'] = updated_listings_df['seller'].astype(\"string\")\n",
        "\n",
        "    for item in update_yaml:\n",
        "        seller = item.get('seller')\n",
        "        href = item.get('href')\n",
        "\n",
        "        if seller is not None and href is not None:\n",
        "            # Clean the href string using re.sub for regex replacement\n",
        "            cleaned_href = re.sub(r'^(https?://)?(www\\.)?', '', str(href))\n",
        "\n",
        "            # Update the 'seller' column for matching 'href' entries\n",
        "            updated_listings_df.loc[updated_listings_df['href'] == cleaned_href, 'seller'] = seller\n",
        "\n",
        "    return updated_listings_df\n",
        "\n",
        "print(\"Defined `update_seller` function.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ogNU9_UDJPo",
        "outputId": "05397d2d-0d41-44ad-dd39-e9d0fe42193f"
      },
      "id": "4ogNU9_UDJPo",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined `update_seller` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_allocations"
      ],
      "metadata": {
        "id": "c3Gz3FZoSO0o"
      },
      "id": "c3Gz3FZoSO0o"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def update_allocations(allocations: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the allocations DataFrame based on information from the update_yaml.\n",
        "\n",
        "    Business Logic for update_allocations function:\n",
        "\n",
        "    This function processes updates from a YAML file (typically `shortlist-edited.yaml`) to modify\n",
        "    the `allocations` DataFrame. The key business rules applied are:\n",
        "\n",
        "    1.  **De-allocation of Unspecified Clients:** For listings present in the `update_yaml`,\n",
        "        if a client was previously allocated to that listing but is *not* present\n",
        "        in the `clients` list for that listing in the `update_yaml`, their `allocation` status in the\n",
        "        `allocations` DataFrame will be set to `False`. This ensures that clients no longer interested\n",
        "        in a particular listing (as indicated by the YAML) are de-allocated.\n",
        "    2.  **Preservation of Other Allocations:** Allocations for listings not mentioned in the `update_yaml`,\n",
        "        or allocations for clients still specified in the `update_yaml`, remain unchanged.\n",
        "\n",
        "    Args:\n",
        "        allocations (pd.DataFrame): The existing DataFrame of allocations.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified allocations DataFrame.\n",
        "    \"\"\"\n",
        "    # 1. Create a copy of the input allocations DataFrame named allocations_copy.\n",
        "    allocations_copy = allocations.copy()\n",
        "\n",
        "    # 2. Iterate through update_yaml to create a dictionary update_lookup where keys are href values\n",
        "    # and values are dictionaries containing the status and a list of clients for that href from the YAML.\n",
        "    update_lookup = {}\n",
        "    for item in update_yaml:\n",
        "        href = item.get('href')\n",
        "        status = item.get('status')\n",
        "        clients_from_yaml = item.get('clients', [])\n",
        "        # Fix: Ensure clients_from_yaml is always a list, even if 'clients' key has a None value\n",
        "        if clients_from_yaml is None:\n",
        "            clients_from_yaml = []\n",
        "        if href:\n",
        "            # Clean the href string, consistent with enrich_df and update_seller\n",
        "            cleaned_href = href.replace('https://', '').replace('http://', '').replace('www.', '')\n",
        "            update_lookup[cleaned_href] = {\n",
        "                'status': status,\n",
        "                'clients': clients_from_yaml\n",
        "            }\n",
        "\n",
        "    # All hrefs from update_lookup are considered for de-allocation logic, regardless of status\n",
        "    hrefs_in_yaml = set(update_lookup.keys())\n",
        "\n",
        "    # 3. Create a set current_active_allocations containing (href, client) tuples for all entries\n",
        "    # in the modified allocations_copy where allocation is True.\n",
        "    current_active_allocations = set(\n",
        "        allocations_copy[allocations_copy['allocation'] == True]\n",
        "        [['href', 'client']].apply(tuple, axis=1)\n",
        "    )\n",
        "\n",
        "    # 4. Create a set yaml_should_be_active containing (href, client) tuples for all clients\n",
        "    # associated with hrefs present in update_lookup.\n",
        "    yaml_should_be_active = set()\n",
        "    for href in hrefs_in_yaml:\n",
        "        data = update_lookup[href]\n",
        "        for client in data['clients']:\n",
        "            yaml_should_be_active.add((href, client))\n",
        "\n",
        "    # Filter current_active_allocations to only include hrefs that are in the update_yaml scope\n",
        "    active_allocations_in_yaml_scope = {\n",
        "        (h, c) for h, c in current_active_allocations if h in hrefs_in_yaml\n",
        "    }\n",
        "\n",
        "    # 5. Determine the set of (href, client) pairs that need to be de-allocated.\n",
        "    # These are allocations that were active within the scope of hrefs mentioned in YAML,\n",
        "    # but are not present in the 'clients' list for those hrefs in the YAML.\n",
        "    to_deallocate = active_allocations_in_yaml_scope - yaml_should_be_active\n",
        "\n",
        "    # 6. Iterate through the identified (href, client) pairs and set the allocation column to\n",
        "    # False for those specific entries in allocations_copy.\n",
        "    for href, client in to_deallocate:\n",
        "        allocations_copy.loc[\n",
        "            (allocations_copy['href'] == href) & (allocations_copy['client'] == client),\n",
        "            'allocation'\n",
        "        ] = False\n",
        "\n",
        "    # Ensure the 'allocation' column is boolean type\n",
        "    allocations_copy['allocation'] = allocations_copy['allocation'].astype('boolean')\n",
        "\n",
        "    # 7. Return the modified allocations_copy DataFrame.\n",
        "    print(f\"De-allocated {len(to_deallocate)} entries based on YAML updates.\")\n",
        "    return allocations_copy"
      ],
      "metadata": {
        "id": "apPs89oFSOgu"
      },
      "id": "apPs89oFSOgu",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##email_client"
      ],
      "metadata": {
        "id": "-j6txk8TrYiJ"
      },
      "id": "-j6txk8TrYiJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# def email_client(hrefs, listings_lr, coefficients, notes):\n",
        "\n",
        "# # print rows\n",
        "# for _, row in best_n.iterrows():\n",
        "#     print(f\"{model_name} ({row['rank']})\")\n",
        "#     print(f\"Link: {row['href']}\")\n",
        "#     print(f\"Market Value: ${row['predicted_price']:,.0f}\")\n",
        "#     print(f\"Listed Price: ${row[\"listed_price\"]:,}\")\n",
        "#     try:\n",
        "#         print(f\"Negotiated Price: ${row['nego_price']:,.0f}\")\n",
        "#     except KeyError as e:\n",
        "#         pass\n",
        "#     print(f\"Year: {row['year']:.0f}\")\n",
        "#     print(f\"Odometer: {row['odometer']:,.0f},000km\")\n",
        "#     print(f\"Notes:\\n\")\n",
        "\n",
        "\n",
        "# # Produce scatterplot\n",
        "# # Function to format price axis\n",
        "# def price_format(x, _):\n",
        "#     return f'${int(x):,}'\n",
        "\n",
        "# # Plotting\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# # Compute age\n",
        "# other_listings['age'] = 2026 - other_listings['year']\n",
        "# best_n['age'] = 2026 - best_n['year']\n",
        "\n",
        "# # Scatter (Year vs Price)\n",
        "# ax1.scatter(other_listings['year'], other_listings[\"listed_price\"],\n",
        "#             label='Data', color='lightsteelblue', s=20)\n",
        "\n",
        "# for _, row in best_n.iterrows():\n",
        "#     ax1.scatter(row['year'], row[\"listed_price\"], s=70, facecolors='none', linewidths=1.2)\n",
        "#     ax1.text(row['year'], row[\"listed_price\"], str(int(row['rank'])),\n",
        "#              ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#              color='red', alpha=0.7)\n",
        "#     if not pd.isna(row['nego_price']):\n",
        "#         ax1.text(row['year'], row['nego_price'], str(int(row['rank'])),\n",
        "#                  ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#                  color='green', alpha=0.7)\n",
        "\n",
        "# # Regression line (fix odometer at mean)\n",
        "# year_range = np.linspace(other_listings['year'].min(),\n",
        "#                          other_listings['year'].max(), 100)\n",
        "\n",
        "# age_range = 2026 - year_range  # convert back to age for model input\n",
        "# mean_odometer = other_listings['odometer'].mean()\n",
        "\n",
        "# X_line = pd.DataFrame({\n",
        "#     'const': 1,\n",
        "#     'age': age_range,\n",
        "#     'odometer': [mean_odometer] * 100\n",
        "# })\n",
        "\n",
        "# y_line = model.predict(X_line)\n",
        "\n",
        "# ax1.plot(year_range, y_line, label='Regression line')\n",
        "\n",
        "# ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# ax1.yaxis.set_major_formatter(FuncFormatter(price_format))\n",
        "# ax1.set_xlabel('Model Year')\n",
        "# ax1.set_ylabel(\"listed_price\")\n",
        "# ax1.set_title(f\"{model_name} Price vs Year\")\n",
        "\n",
        "# # Scatter (Odometer vs Price)\n",
        "# ax2.scatter(other_listings['odometer'], other_listings[\"listed_price\"],\n",
        "#             label='Data', color='lightsteelblue', s=20)\n",
        "\n",
        "# for _, row in best_n.iterrows():\n",
        "#     ax2.scatter(row['odometer'], row[\"listed_price\"], s=70, facecolors='none', linewidths=1.2)\n",
        "#     ax2.text(row['odometer'], row[\"listed_price\"], str(int(row['rank'])),\n",
        "#              ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#              color='red', alpha=0.7)\n",
        "#     if not pd.isna(row['nego_price']):\n",
        "#         ax2.text(row['odometer'], row['nego_price'], str(int(row['rank'])),\n",
        "#                  ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#                  color='green', alpha=0.7)\n",
        "\n",
        "# # Regression line (fix age at mean)\n",
        "# odometer_range = np.linspace(other_listings['odometer'].min(),\n",
        "#                              other_listings['odometer'].max(), 100)\n",
        "\n",
        "# mean_age = other_listings['age'].mean()\n",
        "\n",
        "# X_line2 = pd.DataFrame({\n",
        "#     'const': 1,\n",
        "#     'age': [mean_age] * 100,\n",
        "#     'odometer': odometer_range\n",
        "# })\n",
        "\n",
        "# y_line2 = model.predict(X_line2)\n",
        "# ax2.plot(odometer_range, y_line2, label='Regression line')\n",
        "\n",
        "# ax2.yaxis.set_major_formatter(FuncFormatter(price_format))\n",
        "# ax2.set_xlabel('Odometer (kms)')\n",
        "# ax2.set_ylabel(\"listed_price\")\n",
        "# ax2.set_title(f\"{model_name} Price vs Mileage\")\n",
        "\n",
        "\n",
        "# # Legend handles\n",
        "# live_listing_handle = Line2D([], [], marker='o', color='lightsteelblue', linestyle='None', markersize=6, label=f'Listing as of {df1.iloc[0][\"date_scraped\"]}')\n",
        "# listed_price_handle = Line2D([], [], marker='o', color='red', linestyle='None',\n",
        "#                                  markersize=8, label='Listed Price')\n",
        "# negotiated_price_handle = Line2D([], [], marker='o', color='green', linestyle='None',\n",
        "#                                  markersize=8, label='Negotiated Price')\n",
        "\n",
        "# # Apply legend to both subplots\n",
        "# ax1.legend(handles=[live_listing_handle, listed_price_handle, negotiated_price_handle])\n",
        "# ax2.legend(handles=[live_listing_handle, listed_price_handle, negotiated_price_handle])\n",
        "\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "udv17eUKrcwl"
      },
      "id": "udv17eUKrcwl",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ZAdTyLbASdqQ"
      },
      "id": "ZAdTyLbASdqQ"
    },
    {
      "cell_type": "code",
      "source": [
        "clients=[\n",
        "    {\n",
        "        \"client\":\"anita_c\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"jazz_3\",\n",
        "            \"i30_2\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"magesh_t\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"i30_2\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"raymon_s\",\n",
        "        \"max_listing_price\":11000,\n",
        "        \"max_odometer\":210,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"civic_8\",\n",
        "            \"i30_2\",\n",
        "            \"city_1\",\n",
        "            \"city_2\",\n",
        "            \"corolla_10\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "Eb8E8j5Fsaxy"
      },
      "id": "Eb8E8j5Fsaxy",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2ea91a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2ea91a",
        "outputId": "40459b06-fed1-4e2a-8756-9aa3ca7514ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, Optional, List\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataframes\n",
        "gen_lookup = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/gen_lookup.csv\")\n",
        "listings = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/listings.csv\")\n",
        "notes = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/notes.csv\", index_col=0)\n",
        "allocations = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/allocations.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "YJKBICNE_iDu"
      },
      "id": "YJKBICNE_iDu",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statuses = {\n",
        "    None:\"No status saved\",\n",
        "    \"seen\": \"listing has been printed to YAML at least once\",\n",
        "    \"rejected\": \"listing not suitable for any buyer\",\n",
        "    \"sold\": \"sold or on hold\",\n",
        "    \"shortlisted\": \"VA checked listing and looks good\",\n",
        "    \"contacted\": \"Roger has contacted the seller\",\n",
        "    \"message_left\": \"self explanatory\",\n",
        "    \"follow_up\": \"Roger to call seller\",\n",
        "    \"inspection\": \"Inspection booked\",\n",
        "    \"deposit\": \"Deposit left with seller\",\n",
        "    \"purchased\": \"Self explainatory\",\n",
        "    \"bad_inspection\": \"Not recommended after inspection (Roger/Andrew)\",\n",
        "}"
      ],
      "metadata": {
        "id": "whpy5rTojg7Y"
      },
      "id": "whpy5rTojg7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working"
      ],
      "metadata": {
        "id": "d3osOXG0ExJz"
      },
      "id": "d3osOXG0ExJz"
    },
    {
      "cell_type": "code",
      "source": [
        "z = compare_new_listings(listings, gen_lookup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiTdqt2VxNCE",
        "outputId": "a6850289-12eb-437f-c21d-667fc42fe08d"
      },
      "id": "BiTdqt2VxNCE",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/carsales (1).csv    \t n_new=2   \t n_updated=1 \t n_unchanged=5 \t Tot 8\n",
            "/content/carsales (5).csv    \t n_new=9   \t n_updated=3 \t n_unchanged=10 \t Tot 22\n",
            "/content/carsales (7).csv    \t n_new=3   \t n_updated=1 \t n_unchanged=18 \t Tot 22\n",
            "/content/carsales (4).csv    \t n_new=10   \t n_updated=3 \t n_unchanged=9 \t Tot 22\n",
            "/content/carsales (10).csv    \t n_new=5   \t n_updated=0 \t n_unchanged=17 \t Tot 22\n",
            "/content/carsales (9).csv    \t n_new=5   \t n_updated=0 \t n_unchanged=17 \t Tot 22\n",
            "/content/carsales (2).csv    \t n_new=6   \t n_updated=7 \t n_unchanged=9 \t Tot 22\n",
            "/content/carsales (3).csv    \t n_new=10   \t n_updated=1 \t n_unchanged=11 \t Tot 22\n",
            "/content/carsales (6).csv    \t n_new=9   \t n_updated=2 \t n_unchanged=11 \t Tot 22\n",
            "/content/carsales (8).csv    \t n_new=8   \t n_updated=0 \t n_unchanged=14 \t Tot 22\n",
            "/content/carsales.csv    \t n_new=6   \t n_updated=2 \t n_unchanged=6 \t Tot 14\n",
            "/content/facebook (4).csv    \t n_new=32   \t n_updated=1 \t n_unchanged=11 \t Tot 44\n",
            "/content/facebook.csv    \t n_new=33   \t n_updated=2 \t n_unchanged=17 \t Tot 52\n",
            "/content/facebook (3).csv    \t n_new=43   \t n_updated=1 \t n_unchanged=8 \t Tot 52\n",
            "/content/facebook (1).csv    \t n_new=27   \t n_updated=5 \t n_unchanged=20 \t Tot 52\n",
            "/content/facebook (2).csv    \t n_new=42   \t n_updated=1 \t n_unchanged=9 \t Tot 52\n",
            "\t \t \t \t unq_new=248 \t unq_updated=30\t unq_unchanged=190 unq_tot=468\n",
            "WARNING: Column 'model_gen' in enriched_new_listings has 9 missing values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d173a02",
        "outputId": "389e9663-dc48-4f9d-867c-dd903cd2ed84"
      },
      "source": [
        "# Add new listings to listings dataframe\n",
        "listings = integrate_listings(listings, gen_lookup)"
      ],
      "id": "6d173a02",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final DataFrame has 1097 unique listings after merging and de-duplication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29cd174a"
      },
      "source": [
        "listings_lr, coefficients = apply_regression(listings)"
      ],
      "id": "29cd174a",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = allocate_listings(listings_lr, notes, allocations)\n",
        "best_listings = get_best_listings(listings_lr, allocations, notes, clients)"
      ],
      "metadata": {
        "id": "xHaWD0sWsCbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40461d9e-dfb9-429a-d095-76bd5f1680c1"
      },
      "id": "xHaWD0sWsCbT",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 0 new allocation entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "new_cell",
        "outputId": "427b3acd-88cc-4fb8-f0ea-b0eb0bf000f4"
      },
      "source": [
        "# Call the updated output_shortlist function\n",
        "notes = write_yaml(best_listings, listings_lr, allocations, notes)\n"
      ],
      "id": "new_cell",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7aa55c8c-cf75-4a32-b1dc-634af59652f3\", \"shortlist.yaml\", 3935)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The YAML file 'shortlist.yaml' has been generated and prompted for download with 20 listings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efded001",
        "outputId": "dc4e2c85-21d0-4f99-816b-610ccbe8f880"
      },
      "source": [
        "with open('/content/shortlist-edited.yaml', 'r') as file:\n",
        "    update_yaml = list(yaml.safe_load_all(file))\n",
        "\n",
        "print(\"YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\")"
      ],
      "id": "efded001",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "author = \"roger\"\n",
        "notes = update_notes(notes, update_yaml, author)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOr7wbC_8yU",
        "outputId": "9b5bbf97-942c-4c24-adb7-4f8a6753e8ec"
      },
      "id": "4rOr7wbC_8yU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 12 new entries added to notes DataFrame through update_notes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listings = update_seller(listings, update_yaml)"
      ],
      "metadata": {
        "id": "14AXxgkRA5TY"
      },
      "id": "14AXxgkRA5TY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = update_allocations(allocations, update_yaml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izxraNyocXHW",
        "outputId": "18ed45f2-e3c1-481a-bab1-b06097b3fec9"
      },
      "id": "izxraNyocXHW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De-allocated 0 entries based on YAML updates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/listings.csv\")"
      ],
      "metadata": {
        "id": "4lWaF7AIHLjV"
      },
      "id": "4lWaF7AIHLjV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assume:\n",
        "# a = old DataFrame\n",
        "# listings = new DataFrame\n",
        "# \"href\" uniquely identifies rows\n",
        "\n",
        "# 1. align on key\n",
        "a_idx = a.set_index(\"href\")\n",
        "l_idx = listings.set_index(\"href\")\n",
        "\n",
        "# 2. shared rows only\n",
        "common_idx = a_idx.index.intersection(l_idx.index)\n",
        "a_common = a_idx.loc[common_idx]\n",
        "l_common = l_idx.loc[common_idx]\n",
        "\n",
        "# 3. define change mask ONCE (NaN-safe)\n",
        "change_mask = (\n",
        "    a_common.ne(l_common)\n",
        "    & ~(a_common.isna() & l_common.isna())\n",
        ")\n",
        "\n",
        "# 4. new / removed rows\n",
        "n_new_rows = len(l_idx.index.difference(a_idx.index))\n",
        "n_removed_rows = len(a_idx.index.difference(l_idx.index))\n",
        "\n",
        "# 5. changed rows (any column)\n",
        "\n",
        "print(f\"Expected (new - removed rows)={len(listings)-len(a)}\")\n",
        "n_changed_rows = change_mask.any(axis=1).sum()\n",
        "\n",
        "print(f\"{n_new_rows=}\")\n",
        "print(f\"{n_removed_rows=}\")\n",
        "print(f\"{n_changed_rows=:.0f}\")\n",
        "\n",
        "# 6. row changes per column\n",
        "row_changes_per_column = change_mask.sum()\n",
        "\n",
        "print(row_changes_per_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rfPyJEhFDW1",
        "outputId": "0183c9af-6e07-42b1-aabd-07009f8b6c32"
      },
      "id": "6rfPyJEhFDW1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected (new - removed rows)=229\n",
            "n_new_rows=230\n",
            "n_removed_rows=1\n",
            "n_changed_rows=27\n",
            "age              1\n",
            "date_scraped    27\n",
            "gen              0\n",
            "listed_price    27\n",
            "location         1\n",
            "make             1\n",
            "model            1\n",
            "model_gen        1\n",
            "odometer         1\n",
            "seller           0\n",
            "seller_type      0\n",
            "trim             1\n",
            "year             1\n",
            "dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roger"
      ],
      "metadata": {
        "id": "VfqlksLNmY4W"
      },
      "id": "VfqlksLNmY4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# After\n",
        "\n",
        "new_notes = add_note(\n",
        "    new_notes,\n",
        "    \"roger\",\n",
        "    \"\", #href\n",
        "    status=\"message_left\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "1NTRYmb5mbH_",
        "outputId": "b96cf1ee-d9dd-405e-a6ae-01b063cb1f19"
      },
      "id": "1NTRYmb5mbH_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'new_notes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2952586502.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m new_notes = add_note(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnew_notes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"roger\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#href\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contacted\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_notes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H4KefG6nb8O"
      },
      "id": "0H4KefG6nb8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b8c6e6"
      },
      "source": [
        "# Task\n",
        "The user has approved the plan. I will now proceed to define the `get_best_listings` function, call it, and then confirm that the `best_listings` variable is populated as expected.\n",
        "\n",
        "```python\n",
        "def get_best_listings(listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes: pd.DataFrame, clients: List[Dict], newer_than_date: Optional[str] = None, n_top: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies the best car listings for each client based on allocation, status, and excess value.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results ('excess_value', 'date_scraped').\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        clients (List[Dict]): A list of client dictionaries containing client-specific criteria.\n",
        "        newer_than_date (Optional[str]): A date string (YYYY-MM-DD) to filter listings scraped\n",
        "                                         on or after this date. If None, only listings with the\n",
        "                                         latest 'date_scraped' are considered.\n",
        "        n_top (int): The number of top listings to select for each client.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A deduplicated list of 'href's of the best listings across all clients.\n",
        "    \"\"\"\n",
        "    # 1. Make copies of the input DataFrames to avoid modifying the originals.\n",
        "    df = listings_lr.copy()\n",
        "    allocations_copy = allocations.copy()\n",
        "    notes_copy = notes.copy()\n",
        "\n",
        "    # Ensure date columns are in datetime format\n",
        "    df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "    notes_copy['timestamp'] = pd.to_datetime(notes_copy['timestamp'], errors='coerce')\n",
        "    if newer_than_date:\n",
        "        newer_than_date_dt = pd.to_datetime(newer_than_date)\n",
        "\n",
        "    # 2. Process the notes DataFrame to determine the last_status for each unique href.\n",
        "    # Sort notes by timestamp to get the latest status for each href\n",
        "    notes_sorted = notes_copy.sort_values(by='timestamp', ascending=False)\n",
        "    last_status_per_href = notes_sorted.drop_duplicates(subset='href', keep='first')[['href', 'status']]\n",
        "    last_status_per_href.rename(columns={'status': 'last_status'}, inplace=True)\n",
        "\n",
        "    # 3. Merge listings_lr with the last_status information.\n",
        "    df = pd.merge(df, last_status_per_href, on='href', how='left')\n",
        "\n",
        "    # Create client-specific allocation columns\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        # Filter allocations for the current client and where allocation is True\n",
        "        client_allocations = allocations_copy[\n",
        "            (allocations_copy['client'] == client_name) & (allocations_copy['allocation'] == True)\n",
        "        ]['href'].drop_duplicates()\n",
        "\n",
        "        # Create a boolean column indicating if the listing is allocated to this client\n",
        "        df[f'client_{client_name}'] = df['href'].isin(client_allocations)\n",
        "\n",
        "    # 4. Apply date filtering:\n",
        "    if newer_than_date:\n",
        "        # Filter for listings scraped on or after newer_than_date\n",
        "        df = df[df['date_scraped'] >= newer_than_date_dt]\n",
        "    else:\n",
        "        # If no specific date, filter to include only those listings with the latest date_scraped\n",
        "        latest_date = df['date_scraped'].max()\n",
        "        df = df[df['date_scraped'] == latest_date]\n",
        "\n",
        "    # 5. Further filter the remaining listings to keep only those with no status or last_status=\"seen\".\n",
        "    df = df[df['last_status'].isna() | (df['last_status'] == 'seen')]\n",
        "\n",
        "    # Ensure 'excess_value' is present and numeric for sorting\n",
        "    if 'excess_value' not in df.columns:\n",
        "        print(\"Error: 'excess_value' column is missing in the filtered listings.\")\n",
        "        return []\n",
        "    df['excess_value'] = pd.to_numeric(df['excess_value'], errors='coerce')\n",
        "    df.dropna(subset=['excess_value'], inplace=True) # Drop rows where excess_value became NaN\n",
        "\n",
        "    all_best_hrefs = []\n",
        "\n",
        "    # 6. Iterate through each client and select top N listings.\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        client_allocation_col = f'client_{client_name}'\n",
        "\n",
        "        # Filter for listings allocated to the current client\n",
        "        client_eligible_listings = df[df[client_allocation_col]].copy()\n",
        "\n",
        "        if not client_eligible_listings.empty:\n",
        "            # Select the top n_top listings based on the highest excess_value\n",
        "            top_listings_for_client = client_eligible_listings.sort_values(\n",
        "                by='excess_value', ascending=False\n",
        "            ).head(n_top)\n",
        "            all_best_hrefs.extend(top_listings_for_client['href'].tolist())\n",
        "\n",
        "    # 7. Collect all selected 'href's, deduplicate them, and return the final list.\n",
        "    return list(pd.Series(all_best_hrefs).drop_duplicates())\n",
        "\n",
        "# Call the newly defined get_best_listings function\n",
        "# Assuming listings_lr, allocations, notes, and clients are already defined in the environment.\n",
        "best_listings = get_best_listings(listings_lr, allocations, notes, clients)\n",
        "\n",
        "# Final confirmation: Print the number of best listings found and the first few if any\n",
        "print(f\"Found {len(best_listings)} best listings.\")\n",
        "if best_listings:\n",
        "    print(\"First 5 best listings (hrefs):\")\n",
        "    for i, href in enumerate(best_listings[:5]):\n",
        "        print(f\"- {href}\")\n",
        "```"
      ],
      "id": "c1b8c6e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fafcfbc"
      },
      "source": [
        "## Define get_best_listings function\n",
        "\n",
        "### Subtask:\n",
        "Create the `get_best_listings` function that identifies the best car listings for each client based on allocation, status, and excess value.\n"
      ],
      "id": "1fafcfbc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ce0f763"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `get_best_listings` function as per the instructions, including handling data types, filtering, merging, and client-specific logic.\n",
        "\n"
      ],
      "id": "9ce0f763"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b0c2c26"
      },
      "source": [
        "def get_best_listings(listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes: pd.DataFrame, clients: List[Dict], newer_than_date: Optional[str] = None, n_top: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies the best car listings for each client based on allocation, status, and excess value.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results (e.g., market_value, excess_value).\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        clients (List[Dict]): List of client configuration dictionaries.\n",
        "        newer_than_date (Optional[str]): If provided, filter listings scraped on or after this date. Format 'YYYY-MM-DD'.\n",
        "        n_top (int): Number of top listings to select for each client.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of unique hrefs representing the best listings across all clients.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Make copies of the input DataFrames\n",
        "    df = listings_lr.copy()\n",
        "    allocations_copy = allocations.copy()\n",
        "    notes_copy = notes.copy()\n",
        "\n",
        "    # 2. Convert date columns to datetime objects\n",
        "    df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "    notes_copy['timestamp'] = pd.to_datetime(notes_copy['timestamp'], errors='coerce')\n",
        "\n",
        "    # 3. Process notes_copy to determine the latest status for each unique href\n",
        "    # Sort notes by href and timestamp (descending) to get the latest status per href\n",
        "    notes_copy_sorted = notes_copy.sort_values(by=['href', 'timestamp'], ascending=[True, False])\n",
        "    # Drop duplicates, keeping the first (which will be the latest status for each href)\n",
        "    latest_notes_status = notes_copy_sorted.drop_duplicates(subset=['href'], keep='first')\n",
        "    latest_notes_status = latest_notes_status[['href', 'status']].rename(columns={'status': 'last_status'})\n",
        "\n",
        "    # 4. Merge df with this last_status information\n",
        "    df = pd.merge(df, latest_notes_status, on='href', how='left')\n",
        "\n",
        "    # Create client allocation flags\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        alloc_mask = (allocations_copy['client'] == client_name) & (allocations_copy['allocation'] == True)\n",
        "        allocated_hrefs = allocations_copy[alloc_mask]['href'].unique()\n",
        "        df[f'client_{client_name}'] = df['href'].isin(allocated_hrefs)\n",
        "\n",
        "    # 5. Implement date filtering\n",
        "    if newer_than_date:\n",
        "        newer_than_date_dt = pd.to_datetime(newer_than_date)\n",
        "        df = df[df['date_scraped'] >= newer_than_date_dt]\n",
        "    else:\n",
        "        # If no specific date is given, filter for the latest date_scraped\n",
        "        if not df.empty:\n",
        "            latest_scraped_date = df['date_scraped'].max()\n",
        "            df = df[df['date_scraped'] == latest_scraped_date]\n",
        "\n",
        "    # 6. Further filter df to keep only listings where last_status is either None (missing) or 'seen'\n",
        "    # Convert 'last_status' to string to handle both NaN and actual string values consistently\n",
        "    df = df[df['last_status'].isna() | (df['last_status'] == 'seen')]\n",
        "\n",
        "    # 7. Ensure the excess_value column in df is numeric and drop any rows where it is missing\n",
        "    df['excess_value'] = pd.to_numeric(df['excess_value'], errors='coerce')\n",
        "    df.dropna(subset=['excess_value'], inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No listings remain after initial filtering.\")\n",
        "        return []\n",
        "\n",
        "    all_best_hrefs = []\n",
        "\n",
        "    # 8. Iterate through each client and select top listings\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        client_col = f'client_{client_name}'\n",
        "\n",
        "        # Filter for listings allocated to the current client\n",
        "        client_df = df[df[client_col] == True].copy()\n",
        "\n",
        "        if not client_df.empty:\n",
        "            # Sort by excess_value in descending order and get the top n_top hrefs\n",
        "            top_listings_for_client = client_df.sort_values(by='excess_value', ascending=False).head(n_top)\n",
        "            all_best_hrefs.extend(top_listings_for_client['href'].tolist())\n",
        "\n",
        "    # 9. Convert all_best_hrefs to a Series, remove duplicate hrefs, and return as a list\n",
        "    unique_best_hrefs = pd.Series(all_best_hrefs).drop_duplicates().tolist()\n",
        "\n",
        "    return unique_best_hrefs"
      ],
      "id": "2b0c2c26",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "118c8be7"
      },
      "source": [
        "**Reasoning**:\n",
        "The `get_best_listings` function has been defined as per the instructions in the previous step. Now I will execute the function and store the result in `best_listings` and then print the number of listings found for confirmation.\n",
        "\n"
      ],
      "id": "118c8be7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4d90c2e",
        "outputId": "3ba6aed4-495b-40c7-abad-240d2ac16c3d"
      },
      "source": [
        "best_listings = get_best_listings(listings_lr, allocations, notes, clients)\n",
        "\n",
        "print(f\"Found {len(best_listings)} best listings.\")\n",
        "if best_listings:\n",
        "    print(\"First 5 best listings (hrefs):\")\n",
        "    for i, href in enumerate(best_listings[:5]):\n",
        "        print(f\"- {href}\")"
      ],
      "id": "f4d90c2e",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 best listings.\n",
            "First 5 best listings (hrefs):\n",
            "- facebook.com/marketplace/item/2415723482183677/\n",
            "- facebook.com/marketplace/item/1205597424817761/\n",
            "- carsales.com.au/cars/details/2012-mazda-3-neo-bl-series-2-auto-my13/SSE-AD-19823528/\n",
            "- facebook.com/marketplace/item/836649155823129/\n",
            "- facebook.com/marketplace/item/1485701575847486/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3417c37e"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the `best_listings` variable has been populated with the appropriate hrefs from the `get_best_listings` function.\n"
      ],
      "id": "3417c37e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04020ce4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The `best_listings` variable has been successfully populated with 20 unique listing `href`s from the `get_best_listings` function, confirming the subtask's requirement.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `get_best_listings` function was successfully defined and executed, performing the necessary data manipulations, filtering, and selection logic.\n",
        "*   The `best_listings` variable was populated with 20 unique listing `href`s after processing the input dataframes (`listings_lr`, `allocations`, `notes`) and the `clients` list.\n",
        "*   The function correctly identified relevant listings based on allocation, status, and excess value, with the first 5 `href`s printed as confirmation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The implemented `get_best_listings` function is now ready for use in subsequent stages of the car listing optimization process.\n",
        "*   Further analysis could involve examining the common characteristics (e.g., brand, model, price range, age) of the identified \"best listings\" to refine client-specific criteria or market understanding.\n"
      ],
      "id": "04020ce4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}