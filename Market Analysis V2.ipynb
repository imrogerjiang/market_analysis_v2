{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import statements"
      ],
      "metadata": {
        "id": "ZAdTyLbASdqQ"
      },
      "id": "ZAdTyLbASdqQ"
    },
    {
      "cell_type": "code",
      "source": [
        "clients=[\n",
        "    {\n",
        "        \"client\":\"anita_c\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"jazz_3\",\n",
        "            \"i30_2\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"magesh_t\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"i30_2\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"raymon_s\",\n",
        "        \"max_listing_price\":11000,\n",
        "        \"max_odometer\":210,\n",
        "        \"model_gens\":[\n",
        "            # \"3_2\",\n",
        "            \"civic_8\",\n",
        "            # \"i30_2\",\n",
        "            # \"city_1\",\n",
        "            # \"city_2\",\n",
        "            # \"corolla_10\",\n",
        "            # \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "Eb8E8j5Fsaxy"
      },
      "id": "Eb8E8j5Fsaxy",
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "ae2ea91a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2ea91a",
        "outputId": "b75fdb95-1c9c-4916-88df-18084698a72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, Optional, List\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataframes\n",
        "gen_lookup = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/gen_lookup.csv\")\n",
        "listings = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/listings.csv\")\n",
        "notes = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/notes.csv\", index_col=0)\n",
        "allocations = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/allocations.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "YJKBICNE_iDu"
      },
      "id": "YJKBICNE_iDu",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "1DYdMWKeEnLW"
      },
      "id": "1DYdMWKeEnLW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_df"
      ],
      "metadata": {
        "id": "Icb9MU9nNtv-"
      },
      "id": "Icb9MU9nNtv-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afb574cc"
      },
      "source": [
        "def save_df(df: pd.DataFrame, base_path: str, filename: str):\n",
        "    \"\"\"\n",
        "    Overwrites a file in the base_path and creates a timestamped copy in an 'archive' subdirectory.\n",
        "    If the archive file for the current day already exists, it will not be overwritten.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        base_path (str): The base directory where the file will be saved and archived.\n",
        "        filename (str): The name of the file (e.g., 'gen_lookup.csv').\n",
        "    \"\"\"\n",
        "    include_index_map = {\n",
        "      \"listings.csv\": False,\n",
        "      \"gen_lookup.csv\": False,\n",
        "      \"notes.csv\": True,\n",
        "      \"allocations.csv\": True,\n",
        "    }\n",
        "    # Determine if index should be included for the current filename\n",
        "    include_idx = include_index_map.get(filename, False)\n",
        "\n",
        "    # Construct the full path for the original file\n",
        "    original_filepath = os.path.join(base_path, filename)\n",
        "\n",
        "    # Save (overwrite) the original file\n",
        "    df.to_csv(original_filepath, index=include_idx)\n",
        "    print(f\"Overwrote: {original_filepath}\")\n",
        "\n",
        "    # Create the archive directory path\n",
        "    archive_dir = os.path.join(base_path, 'archive')\n",
        "    os.makedirs(archive_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the archive filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d')\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    archive_filename = f\"{name}_{timestamp}{ext}\"\n",
        "    archive_filepath = os.path.join(archive_dir, archive_filename)\n",
        "\n",
        "    # Check if the archive file already exists before saving\n",
        "    if not os.path.exists(archive_filepath):\n",
        "        # Save the archived file\n",
        "        df.to_csv(archive_filepath, index=include_idx)\n",
        "        print(f\"Archived to: {archive_filepath}\")\n",
        "    else:\n",
        "        print(f\"Archive file already exists for today: {archive_filepath}. Skipping archive save.\")\n",
        "\n",
        "# Example usage:\n",
        "# base_directory = \"/content/drive/Shareddrives/market_analysis_v2/\"\n",
        "# save_df(gen_lookup, base_directory, \"gen_lookup.csv\")"
      ],
      "id": "afb574cc",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_all_dataframes"
      ],
      "metadata": {
        "id": "ADJjnCUoU09L"
      },
      "id": "ADJjnCUoU09L"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_all_dataframes(gen_lookup_df: pd.DataFrame, listings_df: pd.DataFrame, notes_df: pd.DataFrame, allocations_df: pd.DataFrame, base_path: str, save: bool = False):\n",
        "    \"\"\"\n",
        "    Compares the current state of provided DataFrames with their previously saved versions,\n",
        "    reports differences, and optionally saves the current DataFrames to disk.\n",
        "\n",
        "    Args:\n",
        "        gen_lookup_df (pd.DataFrame): The current gen_lookup DataFrame.\n",
        "        listings_df (pd.DataFrame): The current listings DataFrame.\n",
        "        notes_df (pd.DataFrame): The current notes DataFrame.\n",
        "        allocations_df (pd.DataFrame): The current allocations DataFrame.\n",
        "        base_path (str): The base directory where files are saved and loaded.\n",
        "        save (bool, optional): If True, current DataFrames are saved to disk. Defaults to False.\n",
        "    \"\"\"\n",
        "\n",
        "    df_configs = [\n",
        "        {\n",
        "            'df': gen_lookup_df,\n",
        "            'name': \"gen_lookup\",\n",
        "            'filename': \"gen_lookup.csv\",\n",
        "            'index_on_load': False\n",
        "        },\n",
        "        {\n",
        "            'df': listings_df,\n",
        "            'name': \"listings\",\n",
        "            'filename': \"listings.csv\",\n",
        "            'index_on_load': False\n",
        "        },\n",
        "        {\n",
        "            'df': notes_df,\n",
        "            'name': \"notes\",\n",
        "            'filename': \"notes.csv\",\n",
        "            'index_on_load': True\n",
        "        },\n",
        "        {\n",
        "            'df': allocations_df,\n",
        "            'name': \"allocations\",\n",
        "            'filename': \"allocations.csv\",\n",
        "            'index_on_load': True\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    for config in df_configs:\n",
        "        current_df = config['df']\n",
        "        df_name = config['name']\n",
        "        filename = config['filename']\n",
        "        index_on_load = config['index_on_load']\n",
        "        filepath = os.path.join(base_path, filename)\n",
        "\n",
        "        old_df = pd.DataFrame() # Initialize an empty DataFrame for comparison\n",
        "        try:\n",
        "            if index_on_load:\n",
        "                old_df = pd.read_csv(filepath, index_col=0)\n",
        "            else:\n",
        "                old_df = pd.read_csv(filepath)\n",
        "            print(f\"--- Comparing {df_name} ---\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"--- Comparing {df_name} ---\")\n",
        "            print(f\"No previous version of {filename} found at {filepath}. Starting with an empty DataFrame for comparison.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading previous {filename} for comparison: {e}\")\n",
        "            print(f\"Continuing with an empty DataFrame for comparison.\")\n",
        "\n",
        "        # Compare shape\n",
        "        if current_df.shape != old_df.shape:\n",
        "            print(f\"  Shape changed from {old_df.shape} to {current_df.shape}.\")\n",
        "        elif not current_df.empty and not old_df.empty:\n",
        "            print(f\"  Shape remains {current_df.shape}.\")\n",
        "        elif current_df.empty and old_df.empty:\n",
        "            print(f\"  Both current and old DataFrames are empty.\")\n",
        "        else:\n",
        "            print(f\"  Shape: old={old_df.shape}, current={current_df.shape}.\")\n",
        "\n",
        "        # Compare 'href' entries if applicable\n",
        "        if 'href' in current_df.columns or 'href' in old_df.columns:\n",
        "            current_hrefs = set(current_df['href'].astype(str).unique()) if 'href' in current_df.columns else set()\n",
        "            old_hrefs = set(old_df['href'].astype(str).unique()) if 'href' in old_df.columns else set()\n",
        "\n",
        "            new_href_entries = len(current_hrefs - old_hrefs)\n",
        "            removed_href_entries = len(old_hrefs - current_hrefs)\n",
        "\n",
        "            if new_href_entries > 0:\n",
        "                print(f\"  New 'href' entries: {new_href_entries}\")\n",
        "            if removed_href_entries > 0:\n",
        "                print(f\"  Removed 'href' entries: {removed_href_entries}\")\n",
        "            if new_href_entries == 0 and removed_href_entries == 0 and not current_df.empty and not old_df.empty:\n",
        "                print(f\"  No new or removed 'href' entries. Total unique 'hrefs': {len(current_hrefs)}.\")\n",
        "            elif current_df.empty and not old_df.empty:\n",
        "                print(f\"  All {len(old_hrefs)} 'href' entries from previous version removed (current DataFrame is empty).\")\n",
        "            elif not current_df.empty and old_df.empty:\n",
        "                print(f\"  All {len(current_hrefs)} 'href' entries are new (previous DataFrame was empty).\")\n",
        "\n",
        "        # Optional saving\n",
        "        if save:\n",
        "            # Assuming save_df is globally available or imported\n",
        "            save_df(current_df, base_path, filename)\n",
        "            print(f\"  {df_name} DataFrame saved to {filepath}.\")\n",
        "        else:\n",
        "            print(f\"  Saving {df_name} skipped (save=False).\")\n",
        "        print(\"\") # Add a blank line for readability\n"
      ],
      "metadata": {
        "id": "ivwobADmU0fk"
      },
      "id": "ivwobADmU0fk",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## const and helpers"
      ],
      "metadata": {
        "id": "3l2VZo-bNrKq"
      },
      "id": "3l2VZo-bNrKq"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "# --- Carsales/General Scrapes (CS) Constants ---\n",
        "YEAR_MIN, YEAR_MAX = 1980, 2035\n",
        "ORDER: List[str] = ['href', 'year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']\n",
        "\n",
        "YEAR_RE  = r'\\b(19[89]\\d|20[0-3]\\d)\\b'\n",
        "PRICE_RE = r'^(?:AU\\$|\\$)\\s*[\\d,]+(?:\\.\\d{2})?\\b' # Made currency symbol mandatory\n",
        "ODOM_RE  = r'^\\s*\\d+(?:,?\\d{3})*K?\\s*km\\s*$' # Added optional 'K' for Facebook odometer format\n",
        "URL_RE   = r'^(?:https?://|www\\.)'\n",
        "TX, SELLER = {'automatic', 'manual'}, {'private', 'dealer used'}\n",
        "\n",
        "THRESH: Dict[str, float] = {\n",
        "    'year_make_model': 0.50,\n",
        "    \"listed_price\":           0.60,\n",
        "    'transmission':    0.80,\n",
        "    'odometer':        0.60,\n",
        "    'seller_type':     0.70,\n",
        "}\n",
        "\n",
        "# --- Facebook Marketplace (FB) Constants ---\n",
        "FB_ORDER: List[str] = ['href', 'year_make_model', 'listed_price', 'odometer', 'location']\n",
        "THRESH_FB: Dict[str, float] = {\n",
        "    'href':            0.80,\n",
        "    'year_make_model': 0.50,\n",
        "    'listed_price':    0.60,\n",
        "    'odometer':        0.60,\n",
        "    'location':        0.40,\n",
        "}\n",
        "\n",
        "# --- Predicates (Validation Rules) ---\n",
        "def _ratio(mask: pd.Series) -> float:\n",
        "    return float(mask.mean()) if len(mask) else 0.0\n",
        "\n",
        "def _yr_ok(s: pd.Series) -> pd.Series:\n",
        "    years = pd.to_numeric(s.astype(str).str.extract(YEAR_RE, expand=False), errors='coerce')\n",
        "    return years.between(YEAR_MIN, YEAR_MAX)\n",
        "\n",
        "PRED = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    \"listed_price\":           lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'transmission':    lambda s: s.astype(str).str.strip().str.lower().isin(TX),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "    'seller_type':     lambda s: s.astype(str).str.strip().str.lower().isin(SELLER),\n",
        "}\n",
        "\n",
        "PRED_FB = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    'listed_price':    lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "}\n",
        "\n",
        "# --- Core Identification Functions ---\n",
        "def identify_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Carsales/General columns.\"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in ORDER}\n",
        "\n",
        "    href_col = cols[0]\n",
        "\n",
        "    # Exclude URL-like columns from other detection logic\n",
        "    url_ratio = {c: _ratio(df[c].astype(str).str.contains(URL_RE, case=False, na=False)) for c in cols}\n",
        "    urlish = {c for c, r in url_ratio.items() if r >= 0.50}\n",
        "    blocked = {href_col} | urlish\n",
        "\n",
        "    remaining = [c for c in cols if c not in blocked]\n",
        "    picks = {t: None for t in PRED}\n",
        "\n",
        "    for t in PRED:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED[t](df[c])) for c in remaining}\n",
        "        best_col, best_score = max(scores.items(), key=lambda kv: kv[1])\n",
        "        if best_score >= THRESH[t]:\n",
        "            picks[t] = best_col\n",
        "            remaining.remove(best_col)\n",
        "\n",
        "    trim_col = None\n",
        "    ymm = picks.get('year_make_model')\n",
        "    if ymm in cols:\n",
        "        i = cols.index(ymm)\n",
        "        if i + 1 < len(cols):\n",
        "            trim_col = cols[i + 1]\n",
        "\n",
        "    return {'href': href_col, **picks, 'trim': trim_col}\n",
        "\n",
        "def identify_fb_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Facebook Marketplace columns.\n",
        "    Note: 'href' is assumed to be the first column and is handled by clean_fb directly.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in FB_ORDER}\n",
        "\n",
        "    picks = {t: None for t in FB_ORDER}\n",
        "    remaining = set(cols)\n",
        "\n",
        "    # 'href' is now handled externally by clean_fb and is assumed to be the first column\n",
        "    # So we set it to None here or simply don't try to identify it.\n",
        "    # We explicitly remove the first column from 'remaining' as it's the href\n",
        "    if cols and cols[0] in remaining:\n",
        "        remaining.remove(cols[0])\n",
        "    picks['href'] = None # No longer identified by this function\n",
        "\n",
        "    # Identify 'year_make_model', 'listed_price', 'odometer'\n",
        "    for t in ['year_make_model', 'listed_price', 'odometer']:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED_FB[t](df[c])) for c in remaining}\n",
        "        if scores:\n",
        "            best_col, score = max(scores.items(), key=lambda kv: kv[1])\n",
        "            if score >= THRESH_FB[t]:\n",
        "                picks[t] = best_col\n",
        "                remaining.remove(best_col)\n",
        "\n",
        "    # Assign 'location', often found in column 'c' or as the last remaining column\n",
        "    if picks['location'] is None:\n",
        "        if 'c' in remaining:\n",
        "            picks['location'] = 'c'\n",
        "            remaining.remove('c')\n",
        "        elif len(remaining) == 1:\n",
        "            picks['location'] = remaining.pop()\n",
        "\n",
        "    return picks"
      ],
      "metadata": {
        "id": "gECV1vdedUm0"
      },
      "id": "gECV1vdedUm0",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_cs"
      ],
      "metadata": {
        "id": "AUnP9GT2Nn7C"
      },
      "id": "AUnP9GT2Nn7C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5c70d3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_cs(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_cs function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Carsales/General web scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', etc.) using `identify_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Transforms 'odometer' values from 'km' to '000 km' (e.g., 180,000 km -> 180).\n",
        "    4.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_carsales_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_columns(df)\n",
        "    for col in ['year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']:\n",
        "        src = mapping.get(col)\n",
        "        if src is not None and src != out['href'].name:\n",
        "            out[col] = df[src]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_cols = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_cols.columns:\n",
        "            out['year'] = pd.to_numeric(\n",
        "                split_cols[0].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_cols[1] if 1 in split_cols.columns else pd.NA\n",
        "        out['model'] = split_cols[2] if 2 in split_cols.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        out['href'] = out['href'].astype(str).str.split('?').str[0] # Remove query parameters\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    if 'odometer' in out.columns:\n",
        "        out['odometer'] = out['odometer'] // 1000\n",
        "\n",
        "    final_cols = ['href', 'year', 'make', 'model', \"listed_price\", 'trim', 'odometer', 'seller_type']\n",
        "    if save_raw:\n",
        "        final_cols.insert(0, 'raw')\n",
        "    return out[[c for c in final_cols if c in out.columns]]"
      ],
      "execution_count": 4,
      "outputs": [],
      "id": "aa5c70d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_fb"
      ],
      "metadata": {
        "id": "hfCSFOAWNlyg"
      },
      "id": "hfCSFOAWNlyg"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_fb(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_fb function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Facebook Marketplace scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', 'location') using `identify_fb_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Filters out listings with 'listed_price' explicitly marked as \"free\".\n",
        "    4.  **Data Quality Filtering:** Drops rows with missing (`pd.NA`) values in critical columns\n",
        "        ('listed_price', 'odometer', 'year') to ensure data integrity. Also removes listings\n",
        "        with a placeholder 'listed_price' of 12345.\n",
        "    5.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_facebook_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_fb_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_fb_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_fb_columns(df)\n",
        "    for canonical_col, src_col in mapping.items():\n",
        "        if canonical_col != 'href' and src_col is not None and src_col in df.columns:\n",
        "            out[canonical_col] = df[src_col]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_df = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_df.columns:\n",
        "            out['year'] = split_df[0].astype(str).str.replace(r'[^0-9]', '', regex=True).replace('', pd.NA).astype(float).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_df[1] if 1 in split_df.columns else pd.NA\n",
        "        out['model'] = split_df[2] if 2 in split_df.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        out['href'] = out['href'].astype(str).str.split('?').str[0] # Remove query parameters\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            if col == 'listed_price':\n",
        "                out = out[out[col].astype(str).str.lower() != \"free\"]\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^0-9]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    cols_to_check_for_na = []\n",
        "    if 'listed_price' in out.columns: cols_to_check_for_na.append('listed_price')\n",
        "    if 'odometer' in out.columns: cols_to_check_for_na.append('odometer')\n",
        "    if 'year' in out.columns: cols_to_check_for_na.append('year')\n",
        "\n",
        "    if cols_to_check_for_na:\n",
        "        out = out.dropna(subset=cols_to_check_for_na)\n",
        "\n",
        "    final_columns = ['href', 'year', 'make', 'model', \"listed_price\", 'odometer', 'location']\n",
        "    if save_raw:\n",
        "        final_columns.insert(0, 'raw')\n",
        "    return out[[c for c in final_columns if c in out.columns]]"
      ],
      "metadata": {
        "id": "N5MMcCRVEbDl"
      },
      "id": "N5MMcCRVEbDl",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enrich_df"
      ],
      "metadata": {
        "id": "XXQnhOXJNj7S"
      },
      "id": "XXQnhOXJNj7S"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def enrich_df(df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Final clean after clean_cs or clean_fb, including generation assignment.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to enrich.\n",
        "        gen_lookup (pd.DataFrame): A lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The enriched DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Add/Update date_scraped ---\n",
        "    current_timestamp = pd.Timestamp.now().normalize()\n",
        "    if 'date_scraped' in df.columns:\n",
        "        df['date_scraped'] = df['date_scraped'].fillna(current_timestamp)\n",
        "    else:\n",
        "        df[\"date_scraped\"] = current_timestamp\n",
        "\n",
        "    # --- 2. Normalise make & model ---\n",
        "    for col in [\"make\", \"model\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .astype(str)\n",
        "                .str.lower()\n",
        "                .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
        "            )\n",
        "\n",
        "    # --- Remove 'https://' or 'http://' and 'www.' from href ---\n",
        "    if 'href' in df.columns:\n",
        "        df['href'] = df['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True)\n",
        "\n",
        "    # --- 3. Ensure year is numeric ---\n",
        "    if \"year\" in df.columns:\n",
        "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # --- 4. Calculate age ---\n",
        "    if 'year' in df.columns:\n",
        "        df['age'] = 2026 - df['year']\n",
        "\n",
        "    # --- 5. Assign generation manually (no merge, no year_start/year_end contamination) ---\n",
        "    df[\"gen\"] = pd.NA\n",
        "\n",
        "    for idx, row in gen_lookup.iterrows():\n",
        "        mask = (\n",
        "            (df[\"make\"] == row[\"make\"]) &\n",
        "            (df[\"model\"] == row[\"model\"]) &\n",
        "            (df[\"year\"].between(row[\"year_start\"], row[\"year_end\"], inclusive=\"both\"))\n",
        "        )\n",
        "        df.loc[mask, \"gen\"] = row[\"gen\"]\n",
        "\n",
        "    df[\"gen\"] = df[\"gen\"].astype(\"Int64\")\n",
        "\n",
        "    # --- 6. Create model_gen ---\n",
        "    df[\"model_gen\"] = df.apply(\n",
        "        lambda r: f\"{r['model']}_{r['gen']}\" if pd.notna(r[\"gen\"]) else None,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "9EoYNNjuIakO"
      },
      "id": "9EoYNNjuIakO",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_bad_listings"
      ],
      "metadata": {
        "id": "ifMZe8Y4NhWe"
      },
      "id": "ifMZe8Y4NhWe"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_bad_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies filters to remove bad or undesirable listings from the DataFrame.\n",
        "    This function is intended to be called after initial cleaning and data type conversions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to filter, expected to have 'year', 'listed_price', and 'odometer' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Price filters as specified by the user\n",
        "    if 'listed_price' in df_filtered.columns:\n",
        "        # Ensure listed_price is numeric for comparison\n",
        "        df_filtered['listed_price'] = pd.to_numeric(df_filtered['listed_price'], errors='coerce')\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] != 12345]\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] > 3000]\n",
        "\n",
        "    # Calculate age temporarily for the odometer filter if 'year' is available\n",
        "    # Assuming 2026 is the reference year for age calculation based on other parts of the notebook\n",
        "    if 'year' in df_filtered.columns:\n",
        "        df_filtered['year'] = pd.to_numeric(df_filtered['year'], errors='coerce') # Ensure year is numeric\n",
        "        temp_age = 2026 - df_filtered['year']\n",
        "    else:\n",
        "        temp_age = pd.Series(pd.NA, index=df_filtered.index) # Create a Series of NA for consistent operations\n",
        "\n",
        "    # Odometer filter: odometer > 2 * age\n",
        "    if 'odometer' in df_filtered.columns:\n",
        "        # Ensure odometer is numeric\n",
        "        df_filtered['odometer'] = pd.to_numeric(df_filtered['odometer'], errors='coerce')\n",
        "\n",
        "        # Create a mask for rows where both odometer and temp_age are valid for comparison\n",
        "        mask_valid_comparison = df_filtered['odometer'].notna() & temp_age.notna()\n",
        "\n",
        "        # Filter out rows where (odometer is NOT > 2 * age) AND (the comparison is valid)\n",
        "        # We keep rows where (odometer > 2 * age) OR (the comparison cannot be made due to NA values)\n",
        "        df_filtered = df_filtered[~((df_filtered['odometer'] <= 2 * temp_age) & mask_valid_comparison)]\n",
        "\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "PuyPlWYcDWcc"
      },
      "id": "PuyPlWYcDWcc",
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare_new_listings"
      ],
      "metadata": {
        "id": "xIoIjpiuNfDH"
      },
      "id": "xIoIjpiuNfDH"
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_new_listings(listings: pd.DataFrame, gen_lookup: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Processes new listing files, cleans, enriches, and compares them against existing listings.\n",
        "\n",
        "    Args:\n",
        "        listings (pd.DataFrame): Existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): Lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, int, int, int, int]: A tuple containing:\n",
        "            - enriched_new_listings (pd.DataFrame): DataFrame of newly processed and enriched listings.\n",
        "            - tot_new (int): Total count of truly new listings.\n",
        "            - tot_updated (int): Total count of updated listings.\n",
        "            - tot_unchanged (int): Total count of unchanged listings.\n",
        "            - tot_tot (int): Total count of all listings processed from new files.\n",
        "    \"\"\"\n",
        "    tot_new, tot_updated, tot_unchanged, tot_tot = 0, 0, 0, 0\n",
        "    enriched_new_listings = pd.DataFrame()\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "\n",
        "    for file_path in cs_files + fb_files:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # Checking how many new, updated, unchanged listings\n",
        "        df_comparison = pd.merge(\n",
        "            df_cleaned,\n",
        "            listings,\n",
        "            on='href',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_existing')\n",
        "        )\n",
        "\n",
        "        # Identify new listings\n",
        "        new_listings_df = df_comparison[df_comparison['listed_price_existing'].isnull()]\n",
        "        n_new = len(new_listings_df)\n",
        "\n",
        "        # Identify matched listings\n",
        "        matched_listings_df = df_comparison[df_comparison['listed_price_existing'].notnull()]\n",
        "\n",
        "        # From matched_listings, identify updated listings\n",
        "        updated_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] != matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_updated = len(updated_listings_df)\n",
        "\n",
        "        # From matched_listings, identify unchanged listings\n",
        "        unchanged_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] == matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_unchanged = len(unchanged_listings_df)\n",
        "\n",
        "        # Calculate total listings for the current file\n",
        "        n_total_listings = len(df_cleaned)\n",
        "\n",
        "        # Print the comparison result for the current file\n",
        "        print(f\"{file_path}    \\t {n_new=}   \\t {n_updated=} \\t {n_unchanged=} \\t Tot {n_total_listings}\")\n",
        "\n",
        "        tot_new += n_new\n",
        "        tot_updated += n_updated\n",
        "        tot_unchanged += n_unchanged\n",
        "        tot_tot += n_total_listings\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            enriched_new_listings = pd.concat([enriched_new_listings, df_enriched], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f\"\\t \\t \\t \\t {tot_new=} \\t {tot_updated=}\\t {tot_unchanged=} {tot_tot=}\")\n",
        "\n",
        "    # Check for missing values in enriched_new_listings after concatenation\n",
        "    if not enriched_new_listings.empty:\n",
        "        for col in ['model_gen', 'age', 'odometer']:\n",
        "            if col in enriched_new_listings.columns and enriched_new_listings[col].isna().any():\n",
        "                missing_count = enriched_new_listings[col].isna().sum()\n",
        "                print(f\"WARNING: Column '{col}' in enriched_new_listings has {missing_count} missing values.\")\n",
        "\n",
        "\n",
        "    return enriched_new_listings"
      ],
      "metadata": {
        "id": "elwfHrt9xAZ9"
      },
      "id": "elwfHrt9xAZ9",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## integrate_listings"
      ],
      "metadata": {
        "id": "DNtbnWgdNbxT"
      },
      "id": "DNtbnWgdNbxT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23a71e21"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "import glob # Import glob for file pattern matching\n",
        "\n",
        "def integrate_listings(listings_df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Integrates new car listings from '/content/carsales*.csv' and '/content/facebook*.csv' files into an existing listings DataFrame.\n",
        "\n",
        "    Args:\n",
        "        listings_df (pd.DataFrame): The existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): The lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame (`listings_1`) with integrated, cleaned, and enriched listings,\n",
        "                      with existing listings handled by keeping the most recent entry.\n",
        "    \"\"\"\n",
        "    processed_dfs = []\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "    new_file_paths = cs_files + fb_files\n",
        "\n",
        "    for file_path in new_file_paths:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            processed_dfs.append(df_enriched)\n",
        "\n",
        "    if processed_dfs:\n",
        "        new_listings_df = pd.concat(processed_dfs, ignore_index=True)\n",
        "\n",
        "        # Define all possible columns that might exist in either DataFrame\n",
        "        # Get columns from existing listings and new listings, handling potential differences\n",
        "        all_cols = list(set(listings_df.columns) | set(new_listings_df.columns))\n",
        "\n",
        "        # Reindex both DataFrames to ensure they have the same columns\n",
        "        listings_aligned = listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "        new_listings_aligned = new_listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "\n",
        "        # Ensure 'date_scraped' is in datetime format for proper sorting\n",
        "        listings_aligned['date_scraped'] = pd.to_datetime(listings_aligned['date_scraped'], errors='coerce')\n",
        "        new_listings_aligned['date_scraped'] = pd.to_datetime(new_listings_aligned['date_scraped'], errors='coerce')\n",
        "\n",
        "        # Explicitly cast dtypes of new_listings_aligned to match listings_aligned for common columns\n",
        "        # This helps prevent FutureWarning and ensures consistent types across the concatenated DataFrame\n",
        "        for col in all_cols:\n",
        "            if col in listings_aligned.columns and col in new_listings_aligned.columns:\n",
        "                if listings_aligned[col].dtype != new_listings_aligned[col].dtype:\n",
        "                    try:\n",
        "                        if pd.api.types.is_numeric_dtype(listings_aligned[col]):\n",
        "                            if str(listings_aligned[col].dtype) == 'Int64':\n",
        "                                new_listings_aligned[col] = new_listings_aligned[col].astype('Int64')\n",
        "                            else:\n",
        "                                new_listings_aligned[col] = pd.to_numeric(new_listings_aligned[col], errors='coerce').astype(listings_aligned[col].dtype)\n",
        "                        else:\n",
        "                            new_listings_aligned[col] = new_listings_aligned[col].astype(listings_aligned[col].dtype)\n",
        "                    except (TypeError, ValueError):\n",
        "                        pass # Keep original dtype if casting causes error\n",
        "\n",
        "        # Concatenate the aligned Dataframes\n",
        "        listings_1 = pd.concat([listings_aligned, new_listings_aligned], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No new listings\")\n",
        "        return listings_df # Return the original listings_df if no new listings were processed\n",
        "\n",
        "\n",
        "    # Sort by href, then listed_price (lowest first), then date_scraped (most recent first), then drop duplicates keeping the first\n",
        "    listings_1 = listings_1.sort_values(by=['href', 'listed_price', 'date_scraped'], ascending=[True, True, False])\n",
        "    listings_1 = listings_1.drop_duplicates(subset=['href'], keep='first')\n",
        "    listings_1 = remove_bad_listings(listings_1)\n",
        "\n",
        "    # Ensure 'gen' column is Int64 after all operations\n",
        "    listings_1['gen'] = listings_1['gen'].astype('Int64')\n",
        "\n",
        "    print(f\"Final DataFrame has {len(listings_1)} unique listings after merging and de-duplication.\")\n",
        "    return listings_1"
      ],
      "id": "23a71e21",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## allocate_listings"
      ],
      "metadata": {
        "id": "SUj2yn4nNTDB"
      },
      "id": "SUj2yn4nNTDB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import date\n",
        "from typing import Optional, List\n",
        "\n",
        "def allocate_listings(listings_lr: pd.DataFrame, notes: pd.DataFrame, allocations: pd.DataFrame, clients_to_process: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Allocates car listings based on universal standards, client-specific criteria, and notes/allocation history.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): The DataFrame of car listings with regression results (market_value, excess_value).\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        clients_to_process (Optional[List[str]]): List of client names to process. If None, all global clients are processed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An updated allocations DataFrame containing newly proposed allocations.\n",
        "    \"\"\"\n",
        "\n",
        "    global clients # Access the global list of client configuration dictionaries\n",
        "\n",
        "    # Determine which clients to actually process\n",
        "    effective_clients_info = []\n",
        "    if clients_to_process is None:\n",
        "        effective_clients_info = clients # Process all clients\n",
        "    else:\n",
        "        # Filter global clients to get the dictionaries for specified client names\n",
        "        effective_clients_info = [c_info for c_info in clients if c_info['client'] in clients_to_process]\n",
        "\n",
        "    if not effective_clients_info:\n",
        "        print(\"No clients specified or found to process for allocations.\")\n",
        "        return allocations\n",
        "\n",
        "    # Make copies to avoid modifying original DataFrames\n",
        "    listings_filtered = listings_lr.copy()\n",
        "    notes_filtered = notes.copy()\n",
        "    current_allocations = allocations.copy()\n",
        "\n",
        "    # 1. Apply Universal Filters\n",
        "    listings_filtered = listings_filtered[\n",
        "        (listings_filtered['odometer'] > 4 * listings_filtered['age']) &\n",
        "        (listings_filtered['listed_price'] < 0.95 * listings_filtered['market_value'])\n",
        "    ]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after universal filters.\")\n",
        "        return allocations\n",
        "\n",
        "    # 2. Apply Date Filter (most recent listings)\n",
        "    # Ensure 'date_scraped' is datetime for comparison\n",
        "    listings_filtered['date_scraped'] = pd.to_datetime(listings_filtered['date_scraped'], errors='coerce')\n",
        "    most_recent_date = listings_filtered['date_scraped'].max()\n",
        "    listings_filtered = listings_filtered[listings_filtered['date_scraped'].dt.date == most_recent_date.date()]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after date filtering.\")\n",
        "        return allocations\n",
        "\n",
        "    # 3. Filter out listings based on 'notes' status\n",
        "    # Convert notes timestamp to datetime for proper sorting\n",
        "    notes_filtered['timestamp'] = pd.to_datetime(notes_filtered['timestamp'], errors='coerce')\n",
        "\n",
        "    # Get the most recent status for each href\n",
        "    latest_notes = notes_filtered.sort_values(by='timestamp', ascending=False).drop_duplicates(subset=['href'], keep='first')\n",
        "\n",
        "    # Identify hrefs that are 'sold', 'rejected', or 'allocated'\n",
        "    excluded_hrefs_from_notes = latest_notes[\n",
        "        latest_notes['status'].isin(['sold', 'rejected', 'allocated'])\n",
        "    ]['href'].unique()\n",
        "\n",
        "    # Filter listings_filtered to remove these excluded hrefs\n",
        "    listings_filtered = listings_filtered[~listings_filtered['href'].isin(excluded_hrefs_from_notes)]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after notes status filtering.\")\n",
        "        return allocations\n",
        "\n",
        "    # Ensure 'excess_value' is present for sorting\n",
        "    if 'excess_value' not in listings_filtered.columns:\n",
        "        print(\"Error: 'excess_value' column is missing for sorting.\")\n",
        "        return allocations\n",
        "\n",
        "    new_allocation_records = []\n",
        "    current_timestamp = pd.Timestamp.now()\n",
        "\n",
        "    # 4. Iterate through each specified client for allocations\n",
        "    for client_info in effective_clients_info:\n",
        "        current_client_name = client_info['client']\n",
        "        max_price = client_info['max_listing_price']\n",
        "        max_odometer = client_info['max_odometer']\n",
        "        model_gens_allowed = client_info['model_gens']\n",
        "\n",
        "        # Client-specific criteria\n",
        "        price_cond = listings_filtered['listed_price'] <= max_price\n",
        "        odometer_cond = listings_filtered['odometer'] <= max_odometer\n",
        "\n",
        "        # Model generation condition (using str.startswith for broader matching)\n",
        "        model_gen_cond = pd.Series(False, index=listings_filtered.index)\n",
        "        if 'model_gen' in listings_filtered.columns and model_gens_allowed:\n",
        "            for allowed_gen_pattern in model_gens_allowed:\n",
        "                model_gen_cond = model_gen_cond | (\n",
        "                    listings_filtered['model_gen'].astype(str).str.startswith(allowed_gen_pattern)\n",
        "                )\n",
        "\n",
        "        client_eligible_listings = listings_filtered[\n",
        "            price_cond & odometer_cond & model_gen_cond\n",
        "        ].copy()\n",
        "\n",
        "        if not client_eligible_listings.empty:\n",
        "            # 5. Sort by 'excess_value' descending and select top 10 (or all available)\n",
        "            top_listings_for_client = client_eligible_listings.sort_values(by='excess_value', ascending=False).head(10)\n",
        "\n",
        "            for _, listing_row in top_listings_for_client.iterrows():\n",
        "                href = listing_row['href']\n",
        "                # 6. Check if (href, client_name) pair already exists in the existing 'allocations' DataFrame\n",
        "                # This ensures we don't re-allocate already allocated items for this client.\n",
        "                # The final deduplication step also handles this more broadly.\n",
        "                new_allocation_records.append({\n",
        "                    'href': href,\n",
        "                    'client': current_client_name,\n",
        "                    'allocation': True,\n",
        "                    'timestamp': current_timestamp\n",
        "                })\n",
        "\n",
        "    if new_allocation_records:\n",
        "        new_allocations_df = pd.DataFrame(new_allocation_records)\n",
        "        new_allocations_df['timestamp'] = pd.to_datetime(new_allocations_df['timestamp'])\n",
        "        new_allocations_df['allocation'] = new_allocations_df['allocation'].astype('boolean')\n",
        "\n",
        "\n",
        "        # Filter out new allocations that are already present in the existing 'allocations' DataFrame\n",
        "        existing_allocation_keys = allocations[['href', 'client']].drop_duplicates()\n",
        "        merged_df = pd.merge(\n",
        "            new_allocations_df,\n",
        "            existing_allocation_keys,\n",
        "            on=['href', 'client'],\n",
        "            how='left',\n",
        "            indicator=True\n",
        "        )\n",
        "        truly_new_allocations = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "\n",
        "        # 7. Concatenate the truly new allocation records with the existing allocations DataFrame\n",
        "        allocations = pd.concat([allocations, truly_new_allocations], ignore_index=True)\n",
        "        allocations['allocation'] = allocations['allocation'].astype('boolean')\n",
        "        print(f\"Added {len(truly_new_allocations)} new allocation entries.\")\n",
        "    else:\n",
        "        print(\"No new allocations found based on current criteria.\")\n",
        "\n",
        "    return allocations"
      ],
      "metadata": {
        "id": "HmF0gPBJzvkW"
      },
      "id": "HmF0gPBJzvkW",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dl_yaml"
      ],
      "metadata": {
        "id": "rTmF-kcqNN7P"
      },
      "id": "rTmF-kcqNN7P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e025f4f7"
      },
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "from datetime import datetime, date\n",
        "import numpy as np\n",
        "from google.colab import files # Import files for download functionality\n",
        "\n",
        "def dl_yaml(listings_to_print: List[str], listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Consolidates listing data from shortlist and notes DataFrames and saves it to a YAML file.\n",
        "    Dynamically adds client eligibility flags from shortlist columns.\n",
        "\n",
        "    Args:\n",
        "        listings_to_print (List[str]): List of hrefs to include in the YAML output.\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes_df (pd.DataFrame): DataFrame containing notes associated with listings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to convert pandas-specific types to standard Python equivalents\n",
        "    def to_python_type(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "        if isinstance(value, pd.Timestamp):\n",
        "            return value.to_pydatetime() # Convert pandas Timestamp to datetime object\n",
        "        if isinstance(value, (pd.Int64Dtype, np.int64)):\n",
        "            return int(value)\n",
        "        if isinstance(value, (pd.Float64Dtype, np.float64)):\n",
        "            return float(value)\n",
        "        if isinstance(value, (date, datetime)): # Use datetime.date and datetime\n",
        "            return value\n",
        "        return value\n",
        "\n",
        "    # Prepare notes_df\n",
        "    prepared_notes = notes_df.copy()\n",
        "    prepared_notes['timestamp'] = pd.to_datetime(prepared_notes['timestamp'], errors='coerce')\n",
        "    prepared_notes.dropna(subset=['timestamp'], inplace=True)\n",
        "\n",
        "    # Filter listings_lr to include only the listings specified in listings_to_print\n",
        "    # Then sort by 'excess_value' in descending order\n",
        "    current_shortlist = listings_lr[listings_lr['href'].isin(listings_to_print)].copy()\n",
        "    if 'excess_value' in current_shortlist.columns:\n",
        "        current_shortlist = current_shortlist.sort_values(by='excess_value', ascending=False)\n",
        "    else:\n",
        "        print(\"Warning: 'excess_value' column not found, cannot sort by it.\")\n",
        "\n",
        "    all_listings_data = []\n",
        "\n",
        "    for idx, row in current_shortlist.iterrows():\n",
        "        href = row['href']\n",
        "        current_status = None\n",
        "        current_notes = []\n",
        "\n",
        "        matching_notes = prepared_notes[prepared_notes['href'] == href]\n",
        "\n",
        "        if not matching_notes.empty:\n",
        "            matching_notes_sorted = matching_notes.sort_values(by='timestamp', ascending=False)\n",
        "            current_status = to_python_type(matching_notes_sorted.iloc[0]['status'])\n",
        "            current_notes = [to_python_type(n) for n in matching_notes_sorted['note'].tolist() if pd.notna(n)]\n",
        "\n",
        "        # Create a dictionary named listing_data with the specified order and format\n",
        "        listing_data = {\n",
        "            'title': f\"{to_python_type(row['year'])}, {to_python_type(row['model_gen'])}, {int(to_python_type(row['odometer']))}k\",\n",
        "            'seller': to_python_type(row['seller']), # Modified to use seller\n",
        "            'listed_price': to_python_type(row['listed_price']),\n",
        "            'excess_value': int(to_python_type(row['excess_value'])), # Convert to int here\n",
        "            'href': to_python_type(row['href'])\n",
        "        }\n",
        "\n",
        "        # Dynamically add client eligibility from the allocations DataFrame\n",
        "        eligible_clients = allocations[\n",
        "            (allocations['href'] == href) & (allocations['allocation'] == True)\n",
        "        ]['client'].unique().tolist()\n",
        "        listing_data['clients'] = eligible_clients\n",
        "\n",
        "        # Add status and notes\n",
        "        listing_data['status'] = current_status\n",
        "        listing_data['notes'] = current_notes\n",
        "\n",
        "        all_listings_data.append(listing_data)\n",
        "\n",
        "    output_filename = 'shortlist.yaml'\n",
        "    # Initialize yaml_content list\n",
        "    yaml_content = []\n",
        "    for listing in all_listings_data:\n",
        "        yaml_content.append('---\\n') # Add separator before each listing\n",
        "        yaml_content.append(yaml.dump(listing, allow_unicode=True, sort_keys=False))\n",
        "        yaml_content.append('\\n') # Add an extra newline after each dumped listing for readability\n",
        "\n",
        "    yaml_content_str = \"\".join(yaml_content)\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.write(yaml_content_str)\n",
        "\n",
        "    files.download(output_filename)\n",
        "    print(f\"The YAML file '{output_filename}' has been generated and prompted for download with {len(all_listings_data)} listings.\")"
      ],
      "id": "e025f4f7",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apply_regression"
      ],
      "metadata": {
        "id": "1aAGDDUkOKsL"
      },
      "id": "1aAGDDUkOKsL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23f91c4e"
      },
      "source": [
        "def apply_regression(df: pd.DataFrame) -> (pd.DataFrame, pd.Series):\n",
        "    \"\"\"\n",
        "    Applies Huber regression to the input DataFrame to predict car prices.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing car listings.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame, pd.Series): A tuple containing:\n",
        "            - The DataFrame with 'market_value' and 'excess_value' columns added.\n",
        "            - A Series of unscaled regression coefficients.\n",
        "    \"\"\"\n",
        "    listings_lr = df.copy()\n",
        "\n",
        "    # 1) Coerce numeric types\n",
        "    listings_lr['year'] = pd.to_numeric(listings_lr['year'], errors='coerce')\n",
        "    listings_lr['odometer'] = pd.to_numeric(listings_lr['odometer'], errors='coerce')\n",
        "    listings_lr[\"listed_price\"] = pd.to_numeric(listings_lr[\"listed_price\"], errors='coerce')\n",
        "\n",
        "    # 2) One-hot encode model_gen\n",
        "    listings_lr[\"model_gen\"] = listings_lr[\"model_gen\"].astype(str)\n",
        "    dummies = pd.get_dummies(listings_lr[\"model_gen\"], prefix=\"mg_\", prefix_sep=\"\")\n",
        "\n",
        "    # remove base category \"civic_9\" if it exists\n",
        "    base_col = \"mg_civic_9\" # Corrected base column name to match dummy format\n",
        "    if base_col in dummies.columns:\n",
        "        dummies = dummies.drop(columns=[base_col])\n",
        "\n",
        "    listings_lr = pd.concat([listings_lr, dummies], axis=1)\n",
        "\n",
        "    # 3) Build X, y & keep mask\n",
        "    predictor_cols = ['age', 'odometer'] + list(dummies.columns)\n",
        "    X = listings_lr[predictor_cols].astype(float)\n",
        "    y = listings_lr[\"listed_price\"].astype(float)\n",
        "\n",
        "    keep = X.notna().all(axis=1) & y.notna()\n",
        "\n",
        "    X_keep = X.loc[keep]\n",
        "    y_keep = y.loc[keep]\n",
        "\n",
        "    # 4) Scale predictors\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_keep)\n",
        "\n",
        "    # 5) Fit Huber Regression\n",
        "    huber = HuberRegressor(max_iter=1000, epsilon=1.5)\n",
        "    huber.fit(X_scaled, y_keep)\n",
        "\n",
        "    # 6) Predict & store results\n",
        "    pred = huber.predict(X_scaled)\n",
        "    listings_lr.loc[keep, \"market_value\"] = pred\n",
        "    listings_lr.loc[keep, \"excess_value\"] = pred - listings_lr.loc[keep, \"listed_price\"]\n",
        "\n",
        "    # 7) Recover coefficients on the original (unscaled) feature scale\n",
        "    coef_scaled = huber.coef_\n",
        "    mu = scaler.mean_\n",
        "    sigma = scaler.scale_\n",
        "\n",
        "    original_intercept = huber.intercept_ - np.sum(coef_scaled * (mu / sigma))\n",
        "    original_coefs = coef_scaled / sigma\n",
        "\n",
        "    coef_unscaled = pd.Series(\n",
        "        np.concatenate([[original_intercept], original_coefs]),\n",
        "        index=[\"intercept\"] + predictor_cols\n",
        "    )\n",
        "\n",
        "    listings_lr = listings_lr.loc[:, ~listings_lr.columns.str.startswith(\"mg_\")]\n",
        "\n",
        "    return listings_lr, coef_unscaled"
      ],
      "id": "23f91c4e",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_notes"
      ],
      "metadata": {
        "id": "rylENVkd9dWD"
      },
      "id": "rylENVkd9dWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def update_notes(notes_df: pd.DataFrame, update_yaml: list, author: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes shortlist data, compares it with existing notes, and updates the notes DataFrame\n",
        "    with new status changes and notes according to specified logic.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The existing DataFrame of notes.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "        author (str): The author of the notes.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated notes DataFrame.\n",
        "    \"\"\"\n",
        "    new_notes_records = []\n",
        "\n",
        "    # Ensure the notes DataFrame's 'timestamp' column is converted to datetime objects\n",
        "    notes_df['timestamp'] = pd.to_datetime(notes_df['timestamp'], errors='coerce')\n",
        "\n",
        "    for listing in update_yaml:\n",
        "        current_href = listing['href']\n",
        "        current_status_from_yaml = listing['status']\n",
        "        notes_list_from_yaml = listing['notes']\n",
        "\n",
        "        current_timestamp = pd.Timestamp.now(tz='UTC')\n",
        "\n",
        "        # Filter existing records for the current href\n",
        "        existing_notes_for_href = notes_df[notes_df['href'] == current_href].copy()\n",
        "        existing_notes_for_href.sort_values(by='timestamp', ascending=False, inplace=True)\n",
        "\n",
        "        # Identify the latest status from existing notes\n",
        "        latest_status_in_notes = None\n",
        "        if not existing_notes_for_href.empty:\n",
        "            latest_status_in_notes = existing_notes_for_href.iloc[0]['status']\n",
        "\n",
        "        # Determine if status has changed\n",
        "        status_changed = existing_notes_for_href.empty or (current_status_from_yaml != latest_status_in_notes)\n",
        "\n",
        "        # Identify truly new notes from YAML\n",
        "        existing_note_texts = set(existing_notes_for_href['note'].dropna().tolist())\n",
        "        truly_new_notes = [note_text for note_text in notes_list_from_yaml if pd.notna(note_text) and note_text not in existing_note_texts]\n",
        "\n",
        "        # Apply the new combination logic\n",
        "        if status_changed and len(truly_new_notes) == 1:\n",
        "            # Case 1: Status changed AND exactly one new note, combine them\n",
        "            new_notes_records.append({\n",
        "                'href': current_href,\n",
        "                'timestamp': current_timestamp,\n",
        "                'author': author,\n",
        "                'status': current_status_from_yaml,\n",
        "                'note': truly_new_notes[0]\n",
        "            })\n",
        "        else:\n",
        "            # Case 2 & 3: Handle status change and notes separately or only notes\n",
        "            if status_changed:\n",
        "                new_notes_records.append({\n",
        "                    'href': current_href,\n",
        "                    'timestamp': current_timestamp,\n",
        "                    'author': author,\n",
        "                    'status': current_status_from_yaml,\n",
        "                    'note': pd.NA\n",
        "                })\n",
        "            # Add any new notes as separate entries\n",
        "            for note_text in truly_new_notes:\n",
        "                new_notes_records.append({\n",
        "                    'href': current_href,\n",
        "                    'timestamp': current_timestamp,\n",
        "                    'author': author,\n",
        "                    'status': pd.NA,\n",
        "                    'note': note_text\n",
        "                })\n",
        "\n",
        "    # Convert the new_notes_records list into a new pandas DataFrame\n",
        "    if new_notes_records:\n",
        "        new_notes_df = pd.DataFrame(new_notes_records)\n",
        "        # Ensure column order and data types are consistent before concatenation\n",
        "        new_notes_df['timestamp'] = pd.to_datetime(new_notes_df['timestamp'])\n",
        "        new_notes_df = new_notes_df[['href', 'timestamp', 'author', 'status', 'note']]\n",
        "\n",
        "        # Concatenate with the original notes DataFrame\n",
        "        updated_notes_df = pd.concat([notes_df, new_notes_df], ignore_index=True)\n",
        "        print(f\"Added {len(new_notes_df)} new entries to the notes DataFrame.\")\n",
        "        return updated_notes_df\n",
        "    else:\n",
        "        print(\"No new notes or status updates to add.\")\n",
        "        return notes_df"
      ],
      "metadata": {
        "id": "2dhYD0Mv9dy4"
      },
      "id": "2dhYD0Mv9dy4",
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_seller"
      ],
      "metadata": {
        "id": "0sQh5O7bDJyK"
      },
      "id": "0sQh5O7bDJyK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def update_seller(listings_df: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the 'seller' column in listings_df based on information from update_yaml.\n",
        "\n",
        "    Args:\n",
        "        update_yaml (list): A list of dictionaries parsed from a YAML file, potentially containing\n",
        "                                   'href' and 'seller' information.\n",
        "        listings_df (pd.DataFrame): The DataFrame of car listings to be updated.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified listings_df with updated 'seller' information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    updated_listings_df = listings_df.copy()\n",
        "\n",
        "    # Ensure 'seller' column is of object (string) type to accommodate string assignments\n",
        "    # This prevents FutureWarning when assigning strings to a float64 column that might contain NaNs\n",
        "    if 'seller' in updated_listings_df.columns and updated_listings_df['seller'].dtype != object:\n",
        "        updated_listings_df['seller'] = updated_listings_df['seller'].astype(\"string\")\n",
        "\n",
        "    for item in update_yaml:\n",
        "        seller = item.get('seller')\n",
        "        href = item.get('href')\n",
        "\n",
        "        if seller is not None and href is not None:\n",
        "            # Clean the href string using re.sub for regex replacement\n",
        "            cleaned_href = re.sub(r'^(https?://)?(www\\.)?', '', str(href))\n",
        "\n",
        "            # Update the 'seller' column for matching 'href' entries\n",
        "            updated_listings_df.loc[updated_listings_df['href'] == cleaned_href, 'seller'] = seller\n",
        "\n",
        "    return updated_listings_df\n",
        "\n",
        "print(\"Defined `update_seller` function.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ogNU9_UDJPo",
        "outputId": "9d506d38-200b-4869-ef0c-7534f34b042e"
      },
      "id": "4ogNU9_UDJPo",
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined `update_seller` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_allocations"
      ],
      "metadata": {
        "id": "c3Gz3FZoSO0o"
      },
      "id": "c3Gz3FZoSO0o"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def update_allocations(allocations: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the allocations DataFrame based on information from the update_yaml.\n",
        "\n",
        "    Args:\n",
        "        allocations (pd.DataFrame): The existing DataFrame of allocations.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified allocations DataFrame.\n",
        "    \"\"\"\n",
        "    # 2. Create a copy of the input allocations DataFrame named allocations_copy.\n",
        "    allocations_copy = allocations.copy()\n",
        "\n",
        "    # 3. Iterate through update_yaml to create a dictionary update_lookup where keys are href values\n",
        "    # and values are dictionaries containing the status and a list of clients for that href from the YAML.\n",
        "    update_lookup = {}\n",
        "    for item in update_yaml:\n",
        "        href = item.get('href')\n",
        "        status = item.get('status')\n",
        "        clients_from_yaml = item.get('clients', [])\n",
        "        # Fix: Ensure clients_from_yaml is always a list, even if 'clients' key has a None value\n",
        "        if clients_from_yaml is None:\n",
        "            clients_from_yaml = []\n",
        "        if href:\n",
        "            # Clean the href string, consistent with enrich_df and update_seller\n",
        "            cleaned_href = href.replace('https://', '').replace('http://', '').replace('www.', '')\n",
        "            update_lookup[cleaned_href] = {\n",
        "                'status': status,\n",
        "                'clients': clients_from_yaml\n",
        "            }\n",
        "\n",
        "    # 4. Identify all hrefs from update_lookup that have a 'sold' or 'rejected' status\n",
        "    # and store them in a set called sold_rejected_hrefs.\n",
        "    sold_rejected_hrefs = set()\n",
        "    for href, data in update_lookup.items():\n",
        "        if data['status'] in ['sold', 'rejected']:\n",
        "            sold_rejected_hrefs.add(href)\n",
        "\n",
        "    # 5. Remove rows from allocations_copy where the href is present in sold_rejected_hrefs.\n",
        "    allocations_copy = allocations_copy[~allocations_copy['href'].isin(sold_rejected_hrefs)].copy()\n",
        "\n",
        "    # 6. Create a set current_active_allocations containing (href, client) tuples for all entries\n",
        "    # in the modified allocations_copy where allocation is True.\n",
        "    current_active_allocations = set(\n",
        "        allocations_copy[allocations_copy['allocation'] == True]\n",
        "        [['href', 'client']].apply(tuple, axis=1)\n",
        "    )\n",
        "\n",
        "    # 7. Create a set yaml_should_be_active containing (href, client) tuples for all clients\n",
        "    # associated with hrefs in update_lookup that are *not* in sold_rejected_hrefs.\n",
        "    yaml_should_be_active = set()\n",
        "    for href, data in update_lookup.items():\n",
        "        if href not in sold_rejected_hrefs:\n",
        "            for client in data['clients']:\n",
        "                yaml_should_be_active.add((href, client))\n",
        "\n",
        "    # Identify hrefs that are present in the update_yaml\n",
        "    hrefs_in_yaml = set(update_lookup.keys())\n",
        "\n",
        "    # Filter current_active_allocations to only include hrefs that are in the update_yaml\n",
        "    active_allocations_in_yaml_scope = {\n",
        "        (h, c) for h, c in current_active_allocations if h in hrefs_in_yaml\n",
        "    }\n",
        "\n",
        "    # 8. Determine the set of (href, client) pairs that need to be de-allocated.\n",
        "    # These are allocations that were active within the scope of hrefs mentioned in YAML,\n",
        "    # but are not present in the 'clients' list for those hrefs in the YAML.\n",
        "    to_deallocate = active_allocations_in_yaml_scope - yaml_should_be_active\n",
        "\n",
        "    # 9. Iterate through the identified (href, client) pairs and set the allocation column to\n",
        "    # False for those specific entries in allocations_copy.\n",
        "    for href, client in to_deallocate:\n",
        "        allocations_copy.loc[\n",
        "            (allocations_copy['href'] == href) & (allocations_copy['client'] == client),\n",
        "            'allocation'\n",
        "        ] = False\n",
        "\n",
        "    # Ensure the 'allocation' column is boolean type\n",
        "    allocations_copy['allocation'] = allocations_copy['allocation'].astype('boolean')\n",
        "\n",
        "    # 10. Return the modified allocations_copy DataFrame.\n",
        "    print(f\"De-allocated {len(to_deallocate)} entries based on YAML updates.\")\n",
        "    return allocations_copy"
      ],
      "metadata": {
        "id": "apPs89oFSOgu"
      },
      "id": "apPs89oFSOgu",
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working"
      ],
      "metadata": {
        "id": "d3osOXG0ExJz"
      },
      "id": "d3osOXG0ExJz"
    },
    {
      "cell_type": "code",
      "source": [
        "compare_new_listings(listings, gen_lookup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "BiTdqt2VxNCE",
        "outputId": "8ed7aa7f-64d7-4e5d-84bc-130e0c9a3c90"
      },
      "id": "BiTdqt2VxNCE",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t \t \t \t tot_new=0 \t tot_updated=0\t tot_unchanged=0 tot_tot=0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-247ac6ce-aeca-47d6-96a5-b5f3e4f04671\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-247ac6ce-aeca-47d6-96a5-b5f3e4f04671')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-247ac6ce-aeca-47d6-96a5-b5f3e4f04671 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-247ac6ce-aeca-47d6-96a5-b5f3e4f04671');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"compare_new_listings(listings, gen_lookup)\",\n  \"rows\": 0,\n  \"fields\": []\n}"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d173a02",
        "outputId": "9befdd99-2428-46dd-8695-8559da1546f2"
      },
      "source": [
        "# Add new listings to listings dataframe\n",
        "updated_listings = integrate_listings(listings, gen_lookup)"
      ],
      "id": "6d173a02",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No new listings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29cd174a"
      },
      "source": [
        "listings_lr, coefficients = apply_regression(updated_listings)"
      ],
      "id": "29cd174a",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listings_lr.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZbmbasthB-j",
        "outputId": "b05b55b8-b51b-4422-e80b-46d8b93d1fd2"
      },
      "id": "bZbmbasthB-j",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 868 entries, 0 to 867\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   href          868 non-null    object \n",
            " 1   trim          516 non-null    object \n",
            " 2   model         868 non-null    object \n",
            " 3   location      265 non-null    object \n",
            " 4   year          826 non-null    float64\n",
            " 5   date_scraped  868 non-null    object \n",
            " 6   gen           819 non-null    float64\n",
            " 7   seller        0 non-null      float64\n",
            " 8   odometer      866 non-null    float64\n",
            " 9   listed_price  868 non-null    int64  \n",
            " 10  make          868 non-null    object \n",
            " 11  model_gen     868 non-null    object \n",
            " 12  age           826 non-null    float64\n",
            " 13  seller_type   558 non-null    object \n",
            " 14  market_value  824 non-null    float64\n",
            " 15  excess_value  824 non-null    float64\n",
            "dtypes: float64(7), int64(1), object(8)\n",
            "memory usage: 108.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = allocate_listings(listings_lr, notes, allocations)"
      ],
      "metadata": {
        "id": "xHaWD0sWsCbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3320bb0-fa4d-4f4b-9cc8-f9ab0ba33386"
      },
      "id": "xHaWD0sWsCbT",
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 30 new allocation entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_listings = list(set(allocations[allocations[\"allocation\"]][\"href\"]))"
      ],
      "metadata": {
        "id": "QZj2YNGTb8vt"
      },
      "id": "QZj2YNGTb8vt",
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "new_cell",
        "outputId": "5a50929f-96bc-49c9-abd2-c225975a31cb"
      },
      "source": [
        "# Call the updated output_shortlist function\n",
        "dl_yaml(new_listings, listings_lr, allocations, notes)"
      ],
      "id": "new_cell",
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fbcc1713-0493-4c51-a564-98d4612cdf6a\", \"shortlist.yaml\", 4270)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The YAML file 'shortlist.yaml' has been generated and prompted for download with 22 listings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efded001",
        "outputId": "9fc6755a-92ed-4a99-e0f2-06c450fe8ca4"
      },
      "source": [
        "import yaml\n",
        "\n",
        "with open('/content/shortlist-edited.yaml', 'r') as file:\n",
        "    update_yaml = list(yaml.safe_load_all(file))\n",
        "\n",
        "print(\"YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\")"
      ],
      "id": "efded001",
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "author = \"roger\"\n",
        "new_notes = update_notes(notes, update_yaml, author)\n",
        "new_notes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOr7wbC_8yU",
        "outputId": "495d70de-f17e-4687-c12a-aa384e678f3d"
      },
      "id": "4rOr7wbC_8yU",
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 18 new entries to the notes DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_listings = update_seller(listings, update_yaml)\n",
        "new_listings"
      ],
      "metadata": {
        "id": "14AXxgkRA5TY"
      },
      "id": "14AXxgkRA5TY",
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_allocations = update_allocations(allocations, update_yaml)\n",
        "new_allocations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "izxraNyocXHW",
        "outputId": "b2f82b26-988b-41a7-8135-b8a2dbaf1e00"
      },
      "id": "izxraNyocXHW",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De-allocated 1 entries based on YAML updates.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                        href  \\\n",
              "3          carsales.com.au/cars/details/2016-hyundai-i30-active-x-auto-my17/SSE-AD-18649338/   \n",
              "7                                             facebook.com/marketplace/item/780119465046681/   \n",
              "13         carsales.com.au/cars/details/2016-hyundai-i30-active-x-auto-my17/SSE-AD-18649338/   \n",
              "17                                            facebook.com/marketplace/item/780119465046681/   \n",
              "19               carsales.com.au/cars/details/2012-hyundai-i30-sx-auto-my11/OAG-AD-25309900/   \n",
              "22  carsales.com.au/cars/details/2010-honda-civic-limited-edition-auto-my10/SSE-AD-19803853/   \n",
              "23                                           facebook.com/marketplace/item/2185278138969437/   \n",
              "24              carsales.com.au/cars/details/2008-honda-civic-vti-auto-my08/SSE-AD-19792838/   \n",
              "25                                           facebook.com/marketplace/item/1997631307682530/   \n",
              "26                                            facebook.com/marketplace/item/857037513744343/   \n",
              "27                                           facebook.com/marketplace/item/1199747862066900/   \n",
              "28                                           facebook.com/marketplace/item/1189520169224476/   \n",
              "29              carsales.com.au/cars/details/2009-honda-civic-vti-auto-my09/SSE-AD-19568430/   \n",
              "\n",
              "      client  allocation                  timestamp  \n",
              "3    anita_c        True 2025-12-10 04:03:31.822817  \n",
              "7    anita_c        True 2025-12-10 04:03:31.822817  \n",
              "13  magesh_t        True 2025-12-10 04:03:31.822817  \n",
              "17  magesh_t        True 2025-12-10 04:03:31.822817  \n",
              "19  magesh_t       False 2025-12-10 04:03:31.822817  \n",
              "22  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "23  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "24  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "25  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "26  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "27  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "28  raymon_s        True 2025-12-10 04:03:31.822817  \n",
              "29  raymon_s        True 2025-12-10 04:03:31.822817  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c802d1ca-5252-46fa-b59b-1f2f13e4186a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>href</th>\n",
              "      <th>client</th>\n",
              "      <th>allocation</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>carsales.com.au/cars/details/2016-hyundai-i30-active-x-auto-my17/SSE-AD-18649338/</td>\n",
              "      <td>anita_c</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>facebook.com/marketplace/item/780119465046681/</td>\n",
              "      <td>anita_c</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>carsales.com.au/cars/details/2016-hyundai-i30-active-x-auto-my17/SSE-AD-18649338/</td>\n",
              "      <td>magesh_t</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>facebook.com/marketplace/item/780119465046681/</td>\n",
              "      <td>magesh_t</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>carsales.com.au/cars/details/2012-hyundai-i30-sx-auto-my11/OAG-AD-25309900/</td>\n",
              "      <td>magesh_t</td>\n",
              "      <td>False</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>carsales.com.au/cars/details/2010-honda-civic-limited-edition-auto-my10/SSE-AD-19803853/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>facebook.com/marketplace/item/2185278138969437/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>carsales.com.au/cars/details/2008-honda-civic-vti-auto-my08/SSE-AD-19792838/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>facebook.com/marketplace/item/1997631307682530/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>facebook.com/marketplace/item/857037513744343/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>facebook.com/marketplace/item/1199747862066900/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>facebook.com/marketplace/item/1189520169224476/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>carsales.com.au/cars/details/2009-honda-civic-vti-auto-my09/SSE-AD-19568430/</td>\n",
              "      <td>raymon_s</td>\n",
              "      <td>True</td>\n",
              "      <td>2025-12-10 04:03:31.822817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c802d1ca-5252-46fa-b59b-1f2f13e4186a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c802d1ca-5252-46fa-b59b-1f2f13e4186a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c802d1ca-5252-46fa-b59b-1f2f13e4186a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-413fff96-d090-4ca0-a1e9-609c69ad0ce5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-413fff96-d090-4ca0-a1e9-609c69ad0ce5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-413fff96-d090-4ca0-a1e9-609c69ad0ce5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d83ee447-8c16-405e-af5f-b686a1bd61f7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('new_allocations')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d83ee447-8c16-405e-af5f-b686a1bd61f7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('new_allocations');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_allocations",
              "summary": "{\n  \"name\": \"new_allocations\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"href\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"carsales.com.au/cars/details/2008-honda-civic-vti-auto-my08/SSE-AD-19792838/\",\n          \"carsales.com.au/cars/details/2016-hyundai-i30-active-x-auto-my17/SSE-AD-18649338/\",\n          \"facebook.com/marketplace/item/1189520169224476/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"client\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"anita_c\",\n          \"magesh_t\",\n          \"raymon_s\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"allocation\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-12-10 04:03:31.822817\",\n        \"max\": \"2025-12-10 04:03:31.822817\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-12-10 04:03:31.822817\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}