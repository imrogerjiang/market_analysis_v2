{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "1DYdMWKeEnLW"
      },
      "id": "1DYdMWKeEnLW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compare_df"
      ],
      "metadata": {
        "id": "Ke3ctgS_-wt8"
      },
      "id": "Ke3ctgS_-wt8"
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_df(df_name):\n",
        "    # Load the 'old' DataFrame from the drive\n",
        "    old_df = pd.read_csv(f\"/content/drive/Shareddrives/market_analysis_v2/{df_name}.csv\")\n",
        "\n",
        "    # Determine the current_df based on df_name\n",
        "    if df_name == 'listings':\n",
        "        current_df = listings\n",
        "    elif df_name == 'allocations':\n",
        "        current_df = allocations\n",
        "    elif df_name == 'notes':\n",
        "        current_df = notes\n",
        "    else:\n",
        "        raise ValueError(f\"df_name '{df_name}' not supported for comparison.\")\n",
        "\n",
        "    # Set 'href' as the index for both DataFrames\n",
        "    old_df_indexed = old_df.set_index(\"href\")\n",
        "    current_df_indexed = current_df.set_index(\"href\")\n",
        "\n",
        "    # Identify common 'href' values\n",
        "    common_idx = old_df_indexed.index.intersection(current_df_indexed.index)\n",
        "\n",
        "    # Create common DataFrames based on common hrefs\n",
        "    old_common = old_df_indexed.loc[common_idx]\n",
        "    current_common = current_df_indexed.loc[common_idx]\n",
        "\n",
        "    # Calculate new and removed rows\n",
        "    n_new_rows = len(current_df_indexed.index.difference(old_df_indexed.index))\n",
        "    n_removed_rows = len(old_df_indexed.index.difference(current_df_indexed.index))\n",
        "\n",
        "    # Get the intersection of columns for a clean comparison\n",
        "    cols_to_compare = list(set(old_common.columns) & set(current_common.columns))\n",
        "\n",
        "    old_comp = old_common[cols_to_compare]\n",
        "    current_comp = current_common[cols_to_compare]\n",
        "\n",
        "    # Convert 'date_scraped' to datetime for both before comparison to avoid dtype issues\n",
        "    if 'date_scraped' in cols_to_compare:\n",
        "        old_comp['date_scraped'] = pd.to_datetime(old_comp['date_scraped'], errors='coerce')\n",
        "        current_comp['date_scraped'] = pd.to_datetime(current_comp['date_scraped'], errors='coerce')\n",
        "\n",
        "    # Define change mask (NaN-safe):\n",
        "    # A change is identified if values are different, OR if one is NaN and the other is not.\n",
        "    # NaNs in the same position in both dataframes are NOT considered a change.\n",
        "    change_mask = (old_comp.ne(current_comp) & ~(old_comp.isna() & current_comp.isna()))\n",
        "\n",
        "    # Calculate changed rows (any column)\n",
        "    n_changed_rows = change_mask.any(axis=1).sum()\n",
        "\n",
        "    # Calculate row changes per column\n",
        "    row_changes_per_column = change_mask.sum()\n",
        "\n",
        "    print(f\"'{df_name}' on Drive has {len(old_df)} rows.\")\n",
        "    print(f\"'{df_name}' current has {len(current_df)} rows.\")\n",
        "    print(f\"Number of new rows: {n_new_rows}\")\n",
        "    print(f\"Number of removed rows: {n_removed_rows}\")\n",
        "    print(f\"Number of changed rows: {n_changed_rows}\")\n",
        "    print(f\"Row changes per column:\\n{row_changes_per_column}\")"
      ],
      "metadata": {
        "id": "GGC4hNzSWb71"
      },
      "id": "GGC4hNzSWb71",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_df"
      ],
      "metadata": {
        "id": "Icb9MU9nNtv-"
      },
      "id": "Icb9MU9nNtv-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afb574cc"
      },
      "source": [
        "def save_df(df: pd.DataFrame, filename: str):\n",
        "    \"\"\"\n",
        "    Archives current file on Drive\n",
        "    Overwrites a file in the base_path\n",
        "    If the archive file for the current day already exists, it will not be overwritten.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        base_path (str): The base directory where the file will be saved and archived.\n",
        "        filename (str): The name of the file (e.g., 'gen_lookup.csv').\n",
        "    \"\"\"\n",
        "    include_index_map = {\n",
        "      \"listings.csv\": False,\n",
        "      \"gen_lookup.csv\": False,\n",
        "      \"notes.csv\": True,\n",
        "      \"allocations.csv\": True,\n",
        "    }\n",
        "    base_path = \"/content/drive/Shareddrives/market_analysis_v2/\"\n",
        "    # Determine if index should be included for the current filename\n",
        "    include_idx = include_index_map.get(filename, False)\n",
        "\n",
        "    # Construct the full path for the original file\n",
        "    original_filepath = os.path.join(base_path, filename)\n",
        "\n",
        "    prev_df = pd.read_csv(original_filepath, index=include_idx)\n",
        "\n",
        "    # Create the archive directory path\n",
        "    archive_dir = os.path.join(base_path, 'archive')\n",
        "    os.makedirs(archive_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the archive filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d')\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    file_n = 0\n",
        "    archive_filename_base = f\"{name}_{timestamp}\"\n",
        "    archive_filepath = os.path.join(archive_dir, f\"{archive_filename_base}{ext}\")\n",
        "\n",
        "    # Find a unique filename by incrementing file_n\n",
        "    while os.path.exists(archive_filepath):\n",
        "        file_n += 1\n",
        "        archive_filepath = os.path.join(archive_dir, f\"{archive_filename_base}_{file_n}{ext}\")\n",
        "\n",
        "    # Archive current file in Drive\n",
        "    prev_df.to_csv(archive_filepath, index=include_idx)\n",
        "    print(f\"Archived to: {archive_filepath}\")\n",
        "\n",
        "    # Overwrite file on drive\n",
        "    df.to_csv(original_filepath, index=include_idx)"
      ],
      "id": "afb574cc",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## const and helpers"
      ],
      "metadata": {
        "id": "3l2VZo-bNrKq"
      },
      "id": "3l2VZo-bNrKq"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "# --- Carsales/General Scrapes (CS) Constants ---\n",
        "YEAR_MIN, YEAR_MAX = 1980, 2035\n",
        "ORDER: List[str] = ['href', 'year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']\n",
        "\n",
        "YEAR_RE  = r'\\b(19[89]\\d|20[0-3]\\d)\\b'\n",
        "PRICE_RE = r'^(?:AU\\$|\\$)\\s*[\\d,]+(?:\\.\\d{2})?\\b' # Made currency symbol mandatory\n",
        "ODOM_RE  = r'^\\s*\\d+(?:,?\\d{3})*K?\\s*km\\s*$' # Added optional 'K' for Facebook odometer format\n",
        "URL_RE   = r'^(?:https?://|www\\.)'\n",
        "TX, SELLER = {'automatic', 'manual'}, {'private', 'dealer used'}\n",
        "\n",
        "THRESH: Dict[str, float] = {\n",
        "    'year_make_model': 0.50,\n",
        "    \"listed_price\":           0.60,\n",
        "    'transmission':    0.80,\n",
        "    'odometer':        0.60,\n",
        "    'seller_type':     0.70,\n",
        "}\n",
        "\n",
        "# --- Facebook Marketplace (FB) Constants ---\n",
        "FB_ORDER: List[str] = ['href', 'year_make_model', 'listed_price', 'odometer', 'location']\n",
        "THRESH_FB: Dict[str, float] = {\n",
        "    'href':            0.80,\n",
        "    'year_make_model': 0.50,\n",
        "    'listed_price':    0.60,\n",
        "    'odometer':        0.60,\n",
        "    'location':        0.40,\n",
        "}\n",
        "\n",
        "# --- Predicates (Validation Rules) ---\n",
        "def _ratio(mask: pd.Series) -> float:\n",
        "    return float(mask.mean()) if len(mask) else 0.0\n",
        "\n",
        "def _yr_ok(s: pd.Series) -> pd.Series:\n",
        "    years = pd.to_numeric(s.astype(str).str.extract(YEAR_RE, expand=False), errors='coerce')\n",
        "    return years.between(YEAR_MIN, YEAR_MAX)\n",
        "\n",
        "PRED = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    \"listed_price\":           lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'transmission':    lambda s: s.astype(str).str.strip().str.lower().isin(TX),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "    'seller_type':     lambda s: s.astype(str).str.strip().str.lower().isin(SELLER),\n",
        "}\n",
        "\n",
        "PRED_FB = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    'listed_price':    lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "}\n",
        "\n",
        "# --- Core Identification Functions ---\n",
        "def identify_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Carsales/General columns.\"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in ORDER}\n",
        "\n",
        "    href_col = cols[0]\n",
        "\n",
        "    # Exclude URL-like columns from other detection logic\n",
        "    url_ratio = {c: _ratio(df[c].astype(str).str.contains(URL_RE, case=False, na=False)) for c in cols}\n",
        "    urlish = {c for c, r in url_ratio.items() if r >= 0.50}\n",
        "    blocked = {href_col} | urlish\n",
        "\n",
        "    remaining = [c for c in cols if c not in blocked]\n",
        "    picks = {t: None for t in PRED}\n",
        "\n",
        "    for t in PRED:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED[t](df[c])) for c in remaining}\n",
        "        best_col, best_score = max(scores.items(), key=lambda kv: kv[1])\n",
        "        if best_score >= THRESH[t]:\n",
        "            picks[t] = best_col\n",
        "            remaining.remove(best_col)\n",
        "\n",
        "    trim_col = None\n",
        "    ymm = picks.get('year_make_model')\n",
        "    if ymm in cols:\n",
        "        i = cols.index(ymm)\n",
        "        if i + 1 < len(cols):\n",
        "            trim_col = cols[i + 1]\n",
        "\n",
        "    return {'href': href_col, **picks, 'trim': trim_col}\n",
        "\n",
        "def identify_fb_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Facebook Marketplace columns.\n",
        "    Note: 'href' is assumed to be the first column and is handled by clean_fb directly.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in FB_ORDER}\n",
        "\n",
        "    picks = {t: None for t in FB_ORDER}\n",
        "    remaining = set(cols)\n",
        "\n",
        "    # 'href' is now handled externally by clean_fb and is assumed to be the first column\n",
        "    # So we set it to None here or simply don't try to identify it.\n",
        "    # We explicitly remove the first column from 'remaining' as it's the href\n",
        "    if cols and cols[0] in remaining:\n",
        "        remaining.remove(cols[0])\n",
        "    picks['href'] = None # No longer identified by this function\n",
        "\n",
        "    # Identify 'year_make_model', 'listed_price', 'odometer'\n",
        "    for t in ['year_make_model', 'listed_price', 'odometer']:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED_FB[t](df[c])) for c in remaining}\n",
        "        if scores:\n",
        "            best_col, score = max(scores.items(), key=lambda kv: kv[1])\n",
        "            if score >= THRESH_FB[t]:\n",
        "                picks[t] = best_col\n",
        "                remaining.remove(best_col)\n",
        "\n",
        "    # Assign 'location', often found in column 'c' or as the last remaining column\n",
        "    if picks['location'] is None:\n",
        "        if 'c' in remaining:\n",
        "            picks['location'] = 'c'\n",
        "            remaining.remove('c')\n",
        "        elif len(remaining) == 1:\n",
        "            picks['location'] = remaining.pop()\n",
        "\n",
        "    return picks"
      ],
      "metadata": {
        "id": "gECV1vdedUm0"
      },
      "id": "gECV1vdedUm0",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_cs"
      ],
      "metadata": {
        "id": "AUnP9GT2Nn7C"
      },
      "id": "AUnP9GT2Nn7C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5c70d3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_cs(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_cs function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Carsales/General web scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', etc.) using `identify_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Transforms 'odometer' values from 'km' to '000 km' (e.g., 180,000 km -> 180).\n",
        "    4.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_carsales_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_columns(df)\n",
        "    for col in ['year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']:\n",
        "        src = mapping.get(col)\n",
        "        if src is not None and src != out['href'].name:\n",
        "            out[col] = df[src]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_cols = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_cols.columns:\n",
        "            out['year'] = pd.to_numeric(\n",
        "                split_cols[0].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_cols[1] if 1 in split_cols.columns else pd.NA\n",
        "        out['model'] = split_cols[2] if 2 in split_cols.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        # Revert: Remove .str.lower(), keep http(s)://www. prefixes removal\n",
        "        out['href'] = out['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True).str.split('?').str[0]\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    if 'odometer' in out.columns:\n",
        "        out['odometer'] = out['odometer'] // 1000\n",
        "\n",
        "    final_cols = ['href', 'year', 'make', 'model', \"listed_price\", 'trim', 'odometer', 'seller_type']\n",
        "    if save_raw:\n",
        "        final_cols.insert(0, 'raw')\n",
        "    return out[[c for c in final_cols if c in out.columns]]"
      ],
      "execution_count": 39,
      "outputs": [],
      "id": "aa5c70d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_fb"
      ],
      "metadata": {
        "id": "hfCSFOAWNlyg"
      },
      "id": "hfCSFOAWNlyg"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_fb(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_fb function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Facebook Marketplace scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', 'location') using `identify_fb_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Filters out listings with 'listed_price' explicitly marked as \"free\".\n",
        "    4.  **Data Quality Filtering:** Drops rows with missing (`pd.NA`) values in critical columns\n",
        "        ('listed_price', 'odometer', 'year') to ensure data integrity. Also removes listings\n",
        "        with a placeholder 'listed_price' of 12345.\n",
        "    5.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_facebook_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_fb_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_fb_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_fb_columns(df)\n",
        "    for canonical_col, src_col in mapping.items():\n",
        "        if canonical_col != 'href' and src_col is not None and src_col in df.columns:\n",
        "            out[canonical_col] = df[src_col]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_df = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_df.columns:\n",
        "            out['year'] = split_df[0].astype(str).str.replace(r'[^0-9]', '', regex=True).replace('', pd.NA).astype(float).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_df[1] if 1 in split_df.columns else pd.NA\n",
        "        out['model'] = split_df[2] if 2 in split_df.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        # Revert: Remove .str.lower(), keep http(s)://www. prefixes removal\n",
        "        out['href'] = out['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True).str.split('?').str[0]\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            if col == 'listed_price':\n",
        "                out = out[out[col].astype(str).str.lower() != \"free\"]\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^0-9]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    cols_to_check_for_na = []\n",
        "    if 'listed_price' in out.columns: cols_to_check_for_na.append('listed_price')\n",
        "    if 'odometer' in out.columns: cols_to_check_for_na.append('odometer')\n",
        "    if 'year' in out.columns: cols_to_check_for_na.append('year')\n",
        "\n",
        "    if cols_to_check_for_na:\n",
        "        out = out.dropna(subset=cols_to_check_for_na)\n",
        "\n",
        "    final_columns = ['href', 'year', 'make', 'model', \"listed_price\", 'odometer', 'location']\n",
        "    if save_raw:\n",
        "        final_columns.insert(0, 'raw')\n",
        "    return out[[c for c in final_columns if c in out.columns]]"
      ],
      "metadata": {
        "id": "N5MMcCRVEbDl"
      },
      "id": "N5MMcCRVEbDl",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enrich_df"
      ],
      "metadata": {
        "id": "XXQnhOXJNj7S"
      },
      "id": "XXQnhOXJNj7S"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def enrich_df(df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Final clean after clean_cs or clean_fb, including generation assignment.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to enrich.\n",
        "        gen_lookup (pd.DataFrame): A lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The enriched DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Add/Update date_scraped ---\n",
        "    current_timestamp = pd.Timestamp.now().normalize()\n",
        "    if 'date_scraped' not in df.columns:\n",
        "        # Initialize as datetime type with NaT values if column doesn't exist\n",
        "        df['date_scraped'] = pd.Series(pd.NaT, index=df.index, dtype='datetime64[ns]')\n",
        "    else:\n",
        "        # Ensure it's datetime type, coercing errors if it exists but isn't datetime\n",
        "        df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "\n",
        "    # Now fill NaT values with current_timestamp\n",
        "    df['date_scraped'] = df['date_scraped'].fillna(current_timestamp)\n",
        "\n",
        "    # --- 2. Normalise make & model ---\n",
        "    for col in [\"make\", \"model\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .astype(str)\n",
        "                .str.lower()\n",
        "                .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
        "            )\n",
        "\n",
        "    # --- Remove 'https://' or 'http://' and 'www.' from href ---\n",
        "    if 'href' in df.columns:\n",
        "        df['href'] = df['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True)\n",
        "\n",
        "    # --- 3. Ensure year is numeric ---\n",
        "    if \"year\" in df.columns:\n",
        "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # --- 4. Calculate age ---\n",
        "    if 'year' in df.columns:\n",
        "        df['age'] = 2026 - df['year']\n",
        "\n",
        "    # --- 5. Assign generation manually (no merge, no year_start/year_end contamination) ---\n",
        "    df[\"gen\"] = pd.NA\n",
        "\n",
        "    for idx, row in gen_lookup.iterrows():\n",
        "        mask = (\n",
        "            (df[\"make\"] == row[\"make\"]) &\n",
        "            (df[\"model\"] == row[\"model\"]) &\n",
        "            (df[\"year\"].between(row[\"year_start\"], row[\"year_end\"], inclusive=\"both\"))\n",
        "        )\n",
        "        df.loc[mask, \"gen\"] = row[\"gen\"]\n",
        "\n",
        "    df[\"gen\"] = df[\"gen\"].astype(\"Int64\")\n",
        "\n",
        "    # --- 6. Create model_gen ---\n",
        "    df[\"model_gen\"] = df.apply(\n",
        "        lambda r: f\"{r['model']}_{r['gen']}\" if pd.notna(r[\"gen\"]) else None,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "9EoYNNjuIakO"
      },
      "id": "9EoYNNjuIakO",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_bad_listings"
      ],
      "metadata": {
        "id": "ifMZe8Y4NhWe"
      },
      "id": "ifMZe8Y4NhWe"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_bad_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies filters to remove bad or undesirable listings from the DataFrame.\n",
        "    This function is intended to be called after initial cleaning and data type conversions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to filter, expected to have 'year', 'listed_price', and 'odometer' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Price filters as specified by the user\n",
        "    if 'listed_price' in df_filtered.columns:\n",
        "        # Ensure listed_price is numeric for comparison\n",
        "        df_filtered['listed_price'] = pd.to_numeric(df_filtered['listed_price'], errors='coerce')\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] != 12345]\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] > 3000]\n",
        "\n",
        "    # Calculate age temporarily for the odometer filter if 'year' is available\n",
        "    # Assuming 2026 is the reference year for age calculation based on other parts of the notebook\n",
        "    if 'year' in df_filtered.columns:\n",
        "        df_filtered['year'] = pd.to_numeric(df_filtered['year'], errors='coerce') # Ensure year is numeric\n",
        "        temp_age = 2026 - df_filtered['year']\n",
        "    else:\n",
        "        temp_age = pd.Series(pd.NA, index=df_filtered.index) # Create a Series of NA for consistent operations\n",
        "\n",
        "    # Odometer filter: odometer > 2 * age\n",
        "    if 'odometer' in df_filtered.columns:\n",
        "        # Ensure odometer is numeric\n",
        "        df_filtered['odometer'] = pd.to_numeric(df_filtered['odometer'], errors='coerce')\n",
        "\n",
        "        # Create a mask for rows where both odometer and temp_age are valid for comparison\n",
        "        mask_valid_comparison = df_filtered['odometer'].notna() & temp_age.notna()\n",
        "\n",
        "        # Filter out rows where (odometer is NOT > 2 * age) AND (the comparison is valid)\n",
        "        # We keep rows where (odometer > 2 * age) OR (the comparison cannot be made due to NA values)\n",
        "        df_filtered = df_filtered[~((df_filtered['odometer'] <= 2 * temp_age) & mask_valid_comparison)]\n",
        "\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "PuyPlWYcDWcc"
      },
      "id": "PuyPlWYcDWcc",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare_new_listings"
      ],
      "metadata": {
        "id": "xIoIjpiuNfDH"
      },
      "id": "xIoIjpiuNfDH"
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_new_listings(listings: pd.DataFrame, gen_lookup: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Processes new listing files, cleans, enriches, and compares them against existing listings.\n",
        "\n",
        "    Args:\n",
        "        listings (pd.DataFrame): Existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): Lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, int, int, int, int]: A tuple containing:\n",
        "            - enriched_new_listings (pd.DataFrame): DataFrame of newly processed and enriched listings.\n",
        "            - unq_new (int): Total count of truly new unique listings.\n",
        "            - unq_updated (int): Total count of updated unique listings.\n",
        "            - unq_unchanged (int): Total count of unchanged unique listings.\n",
        "            - unq_tot (int): Total count of all unique listings processed from new files.\n",
        "    \"\"\"\n",
        "    # Sets to track unique hrefs across all processed files\n",
        "    unique_new_hrefs = set()\n",
        "    unique_updated_hrefs = set()\n",
        "    unique_unchanged_hrefs = set()\n",
        "    unique_total_hrefs = set()\n",
        "\n",
        "    enriched_new_listings = pd.DataFrame()\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "\n",
        "    for file_path in cs_files + fb_files:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None and not df_cleaned.empty:\n",
        "            unique_total_hrefs.update(df_cleaned['href'].tolist())\n",
        "\n",
        "        # Checking how many new, updated, unchanged listings\n",
        "        df_comparison = pd.merge(\n",
        "            df_cleaned,\n",
        "            listings,\n",
        "            on='href',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_existing')\n",
        "        )\n",
        "\n",
        "        # Identify new listings\n",
        "        new_listings_df = df_comparison[df_comparison['listed_price_existing'].isnull()]\n",
        "        n_new = len(new_listings_df)\n",
        "        if not new_listings_df.empty:\n",
        "            unique_new_hrefs.update(new_listings_df['href'].tolist())\n",
        "\n",
        "        # Identify matched listings\n",
        "        matched_listings_df = df_comparison[df_comparison['listed_price_existing'].notnull()]\n",
        "\n",
        "        # From matched_listings, identify updated listings\n",
        "        updated_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] != matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_updated = len(updated_listings_df)\n",
        "        if not updated_listings_df.empty:\n",
        "            unique_updated_hrefs.update(updated_listings_df['href'].tolist())\n",
        "\n",
        "        # From matched_listings, identify unchanged listings\n",
        "        unchanged_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] == matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_unchanged = len(unchanged_listings_df)\n",
        "        if not unchanged_listings_df.empty:\n",
        "            unique_unchanged_hrefs.update(unchanged_listings_df['href'].tolist())\n",
        "\n",
        "        # Calculate total listings for the current file\n",
        "        n_total_listings = len(df_cleaned)\n",
        "\n",
        "        # Print the comparison result for the current file\n",
        "        print(f\"{file_path}    \\t {n_new=}   \\t {n_updated=} \\t {n_unchanged=} \\t Tot {n_total_listings}\")\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            enriched_new_listings = pd.concat([enriched_new_listings, df_enriched], ignore_index=True)\n",
        "\n",
        "    # Calculate unique total counts at the end\n",
        "    unq_new = len(unique_new_hrefs)\n",
        "    unq_updated = len(unique_updated_hrefs)\n",
        "    unq_unchanged = len(unique_unchanged_hrefs)\n",
        "    unq_tot = len(unique_total_hrefs)\n",
        "\n",
        "    print(f\"\\t \\t \\t \\t {unq_new=} \\t {unq_updated=}\\t {unq_unchanged=} {unq_tot=}\")\n",
        "\n",
        "    # Check for missing values in enriched_new_listings after concatenation\n",
        "    if not enriched_new_listings.empty:\n",
        "        for col in ['model_gen', 'age', 'odometer']:\n",
        "            if col in enriched_new_listings.columns and enriched_new_listings[col].isna().any():\n",
        "                missing_count = enriched_new_listings[col].isna().sum()\n",
        "                print(f\"WARNING: Column '{col}' in enriched_new_listings has {missing_count} missing values.\")\n",
        "\n",
        "\n",
        "    return enriched_new_listings"
      ],
      "metadata": {
        "id": "elwfHrt9xAZ9"
      },
      "id": "elwfHrt9xAZ9",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## integrate_listings"
      ],
      "metadata": {
        "id": "DNtbnWgdNbxT"
      },
      "id": "DNtbnWgdNbxT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23a71e21"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "import glob # Import glob for file pattern matching\n",
        "\n",
        "def integrate_listings(listings_df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Integrates new car listings from '/content/carsales*.csv' and '/content/facebook*.csv' files into an existing listings DataFrame.\n",
        "\n",
        "    Args:\n",
        "        listings_df (pd.DataFrame): The existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): The lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame (`listings_1`) with integrated, cleaned, and enriched listings,\n",
        "                      with existing listings handled by keeping the most recent entry.\n",
        "    \"\"\"\n",
        "    processed_dfs = []\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "    new_file_paths = cs_files + fb_files\n",
        "\n",
        "    for file_path in new_file_paths:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            processed_dfs.append(df_enriched)\n",
        "\n",
        "    if processed_dfs:\n",
        "        new_listings_df = pd.concat(processed_dfs, ignore_index=True)\n",
        "\n",
        "        # Define all possible columns that might exist in either DataFrame\n",
        "        # Get columns from existing listings and new listings, handling potential differences\n",
        "        all_cols = list(set(listings_df.columns) | set(new_listings_df.columns))\n",
        "\n",
        "        # Reindex both DataFrames to ensure they have the same columns\n",
        "        listings_aligned = listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "        new_listings_aligned = new_listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "\n",
        "        # Ensure 'date_scraped' is in datetime format for proper sorting\n",
        "        listings_aligned['date_scraped'] = pd.to_datetime(listings_aligned['date_scraped'], errors='coerce')\n",
        "        new_listings_aligned['date_scraped'] = pd.to_datetime(new_listings_aligned['date_scraped'], errors='coerce')\n",
        "\n",
        "        # Explicitly cast dtypes of new_listings_aligned to match listings_aligned for common columns\n",
        "        # This helps prevent FutureWarning and ensures consistent types across the concatenated DataFrame\n",
        "        for col in all_cols:\n",
        "            if col in listings_aligned.columns and col in new_listings_aligned.columns:\n",
        "                if listings_aligned[col].dtype != new_listings_aligned[col].dtype:\n",
        "                    try:\n",
        "                        if pd.api.types.is_numeric_dtype(listings_aligned[col]):\n",
        "                            if str(listings_aligned[col].dtype) == 'Int64':\n",
        "                                new_listings_aligned[col] = new_listings_aligned[col].astype('Int64')\n",
        "                            else:\n",
        "                                new_listings_aligned[col] = pd.to_numeric(new_listings_aligned[col], errors='coerce').astype(listings_aligned[col].dtype)\n",
        "                        else:\n",
        "                            new_listings_aligned[col] = new_listings_aligned[col].astype(listings_aligned[col].dtype)\n",
        "                    except (TypeError, ValueError):\n",
        "                        pass # Keep original dtype if casting causes error\n",
        "\n",
        "        # Concatenate the aligned Dataframes\n",
        "        listings_1 = pd.concat([listings_aligned, new_listings_aligned], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No new listings\")\n",
        "        return listings_df # Return the original listings_df if no new listings were processed\n",
        "\n",
        "\n",
        "    # Sort by href, then listed_price (lowest first), then date_scraped (most recent first), then drop duplicates keeping the first\n",
        "    listings_1 = listings_1.sort_values(by=['href', 'listed_price', 'date_scraped'], ascending=[True, True, True])\n",
        "    listings_1 = listings_1.drop_duplicates(subset=['href'], keep='first')\n",
        "    listings_1 = remove_bad_listings(listings_1)\n",
        "\n",
        "    # Ensure 'gen' column is Int64 after all operations\n",
        "    listings_1['gen'] = listings_1['gen'].astype('Int64')\n",
        "\n",
        "    print(f\"Final DataFrame has {len(listings_1)} unique listings after merging and de-duplication.\")\n",
        "    return listings_1"
      ],
      "id": "23a71e21",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## allocate_listings"
      ],
      "metadata": {
        "id": "SUj2yn4nNTDB"
      },
      "id": "SUj2yn4nNTDB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import date\n",
        "from typing import Optional, List\n",
        "\n",
        "def allocate_listings(listings_lr: pd.DataFrame, notes: pd.DataFrame, allocations: pd.DataFrame, clients_to_process: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Allocates car listings based on universal standards, client-specific criteria, and notes/allocation history.\n",
        "\n",
        "    Business Logic for allocate_listings function:\n",
        "\n",
        "    This function identifies and allocates suitable car listings to clients based on a multi-step filtering process:\n",
        "\n",
        "    1.  **Client Identification:** Determines which clients to process based on `clients_to_process` or processes all global clients if none are specified.\n",
        "    2.  **Universal Filters:** Applies initial filters to `listings_lr`:\n",
        "        *   `odometer` must be greater than 4 times the `age` of the car.\n",
        "        *   `listed_price` must be less than 95% of the `market_value`.\n",
        "    3.  **Client-Specific Allocation:** Iterates through each active client and applies their specific criteria:\n",
        "        *   `max_listing_price`: Listing price must be at or below the client's maximum price.\n",
        "        *   `max_odometer`: Odometer reading must be at or below the client's maximum odometer.\n",
        "        *   `model_gens_allowed`: The `model_gen` of the listing must match one of the client's allowed model generations (allowing for partial matches).\n",
        "    4.  **Duplicate Allocation Prevention:** New allocation records are created. Before adding them to the `allocations` DataFrame, the function checks if an (href, client) pair already exists in the historical `allocations` to prevent re-allocating an already processed listing.\n",
        "    5.  **Output:** Returns an updated `allocations` DataFrame containing all previous allocations plus any newly proposed and unique allocations.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): The DataFrame of car listings with regression results (market_value, excess_value).\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        clients_to_process (Optional[List[str]]): List of client names to process. If None, all global clients are processed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An updated allocations DataFrame containing newly proposed allocations.\n",
        "    \"\"\"\n",
        "\n",
        "    global clients # Access the global list of client configuration dictionaries\n",
        "\n",
        "    # Determine which clients to actually process\n",
        "    effective_clients_info = []\n",
        "    if clients_to_process is None:\n",
        "        effective_clients_info = clients # Process all clients\n",
        "    else:\n",
        "        # Filter global clients to get the dictionaries for specified client names\n",
        "        effective_clients_info = [c_info for c_info in clients if c_info['client'] in clients_to_process]\n",
        "\n",
        "    if not effective_clients_info:\n",
        "        print(\"No clients specified or found to process for allocations.\")\n",
        "        return allocations\n",
        "\n",
        "    # Make copies to avoid modifying original DataFrames\n",
        "    listings_filtered = listings_lr.copy()\n",
        "    notes_filtered = notes.copy() # notes_filtered is still used to filter out listings, but will not be changed here\n",
        "    current_allocations = allocations.copy()\n",
        "\n",
        "    # 1. Apply Universal Filters\n",
        "    listings_filtered = listings_filtered[\n",
        "        (listings_filtered['odometer'] > 4 * listings_filtered['age']) &\n",
        "        (listings_filtered['listed_price'] < 0.95 * listings_filtered['market_value'])\n",
        "    ]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after universal filters.\")\n",
        "        return allocations\n",
        "\n",
        "    # Ensure 'excess_value' is present for sorting\n",
        "    if 'excess_value' not in listings_filtered.columns:\n",
        "        print(\"Error: 'excess_value' column is missing for sorting.\")\n",
        "        return allocations\n",
        "\n",
        "    new_allocation_records = []\n",
        "    current_timestamp = pd.Timestamp.now()\n",
        "\n",
        "    # 2. Iterate through each specified client for allocations\n",
        "    for client_info in effective_clients_info:\n",
        "        current_client_name = client_info['client']\n",
        "        max_price = client_info['max_listing_price']\n",
        "        max_odometer = client_info['max_odometer']\n",
        "        model_gens_allowed = client_info['model_gens']\n",
        "\n",
        "        # Client-specific criteria\n",
        "        price_cond = listings_filtered['listed_price'] <= max_price\n",
        "        odometer_cond = listings_filtered['odometer'] <= max_odometer\n",
        "\n",
        "        # Model generation condition (using str.startswith for broader matching)\n",
        "        model_gen_cond = pd.Series(False, index=listings_filtered.index)\n",
        "        if 'model_gen' in listings_filtered.columns and model_gens_allowed:\n",
        "            for allowed_gen_pattern in model_gens_allowed:\n",
        "                model_gen_cond = model_gen_cond | (\n",
        "                    listings_filtered['model_gen'].astype(str).str.startswith(allowed_gen_pattern)\n",
        "                )\n",
        "\n",
        "        client_eligible_listings = listings_filtered[\n",
        "            price_cond & odometer_cond & model_gen_cond\n",
        "        ].copy()\n",
        "\n",
        "        if not client_eligible_listings.empty:\n",
        "            for _, listing_row in client_eligible_listings.iterrows():\n",
        "                href = listing_row['href']\n",
        "                new_allocation_records.append({\n",
        "                    'href': href,\n",
        "                    'client': current_client_name,\n",
        "                    'allocation': True,\n",
        "                    'timestamp': current_timestamp\n",
        "                })\n",
        "\n",
        "    if new_allocation_records:\n",
        "        new_allocations_df = pd.DataFrame(new_allocation_records)\n",
        "        new_allocations_df['timestamp'] = pd.to_datetime(new_allocations_df['timestamp'])\n",
        "        new_allocations_df['allocation'] = new_allocations_df['allocation'].astype('boolean')\n",
        "\n",
        "\n",
        "        # Filter out new allocations that are already present in the existing 'allocations' DataFrame\n",
        "        existing_allocation_keys = allocations[['href', 'client']].drop_duplicates()\n",
        "        merged_df = pd.merge(\n",
        "            new_allocations_df,\n",
        "            existing_allocation_keys,\n",
        "            on=['href', 'client'],\n",
        "            how='left',\n",
        "            indicator=True\n",
        "        )\n",
        "        truly_new_allocations = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "\n",
        "        # 3. Concatenate the truly new allocation records with the existing allocations DataFrame\n",
        "        allocations = pd.concat([allocations, truly_new_allocations], ignore_index=True)\n",
        "        allocations['allocation'] = allocations['allocation'].astype('boolean')\n",
        "        print(f\"Added {len(truly_new_allocations)} new allocation entries.\")\n",
        "    else:\n",
        "        print(\"No new allocations found based on current criteria.\")\n",
        "\n",
        "    return allocations"
      ],
      "metadata": {
        "id": "HmF0gPBJzvkW"
      },
      "id": "HmF0gPBJzvkW",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get_best_listings"
      ],
      "metadata": {
        "id": "6JCPqZfk9Sy6"
      },
      "id": "6JCPqZfk9Sy6"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_listings(listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes: pd.DataFrame, clients: List[Dict], newer_than_date: Optional[str] = None, n_top: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies the best car listings for each client based on allocation, status, and excess value.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results (e.g., market_value, excess_value).\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        clients (List[Dict]): List of client configuration dictionaries.\n",
        "        newer_than_date (Optional[str]): If provided, filter listings scraped on or after this date. Format 'YYYY-MM-DD'.\n",
        "        n_top (int): Number of top listings to select for each client.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of unique hrefs representing the best listings across all clients.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Make copies of the input DataFrames\n",
        "    df = listings_lr.copy()\n",
        "    allocations_copy = allocations.copy()\n",
        "    notes_copy = notes.copy()\n",
        "\n",
        "    # 2. Convert date columns to datetime objects\n",
        "    df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "    notes_copy['timestamp'] = pd.to_datetime(notes_copy['timestamp'], errors='coerce')\n",
        "\n",
        "    # 3. Process notes_copy to determine the latest status for each unique href\n",
        "    # Sort notes by href and timestamp (descending) to get the latest status per href\n",
        "    notes_copy_sorted = notes_copy.sort_values(by=['href', 'timestamp'], ascending=[True, False])\n",
        "    # Drop duplicates, keeping the first (which will be the latest status for each href)\n",
        "    latest_notes_status = notes_copy_sorted.drop_duplicates(subset=['href'], keep='first')\n",
        "    latest_notes_status = latest_notes_status[['href', 'status']].rename(columns={'status': 'last_status'})\n",
        "\n",
        "    # 4. Merge df with this last_status information\n",
        "    df = pd.merge(df, latest_notes_status, on='href', how='left')\n",
        "\n",
        "    # Create client allocation flags\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        alloc_mask = (allocations_copy['client'] == client_name) & (allocations_copy['allocation'] == True)\n",
        "        allocated_hrefs = allocations_copy[alloc_mask]['href'].unique()\n",
        "        df[f'client_{client_name}'] = df['href'].isin(allocated_hrefs)\n",
        "\n",
        "    # 5. Implement date filtering\n",
        "    if newer_than_date:\n",
        "        newer_than_date_dt = pd.to_datetime(newer_than_date)\n",
        "        df = df[df['date_scraped'] >= newer_than_date_dt]\n",
        "    else:\n",
        "        # If no specific date is given, filter for the latest date_scraped\n",
        "        if not df.empty:\n",
        "            latest_scraped_date = df['date_scraped'].max()\n",
        "            df = df[df['date_scraped'] == latest_scraped_date]\n",
        "\n",
        "    # 6. Further filter df to keep only listings where last_status is either None (missing) or 'seen'\n",
        "    # Convert 'last_status' to string to handle both NaN and actual string values consistently\n",
        "    df = df[df['last_status'].isna() | (df['last_status'] == 'seen')]\n",
        "\n",
        "    # 7. Ensure the excess_value column in df is numeric and drop any rows where it is missing\n",
        "    df['excess_value'] = pd.to_numeric(df['excess_value'], errors='coerce')\n",
        "    df.dropna(subset=['excess_value'], inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No listings remain after initial filtering.\")\n",
        "        return []\n",
        "\n",
        "    all_best_hrefs = []\n",
        "\n",
        "    # 8. Iterate through each client and select top listings\n",
        "    for client_info in clients:\n",
        "        client_name = client_info['client']\n",
        "        client_col = f'client_{client_name}'\n",
        "\n",
        "        # Filter for listings allocated to the current client\n",
        "        client_df = df[df[client_col] == True].copy()\n",
        "\n",
        "        if not client_df.empty:\n",
        "            # Sort by excess_value in descending order and get the top n_top hrefs\n",
        "            top_listings_for_client = client_df.sort_values(by='excess_value', ascending=False).head(n_top)\n",
        "            all_best_hrefs.extend(top_listings_for_client['href'].tolist())\n",
        "\n",
        "    # 9. Convert all_best_hrefs to a Series, remove duplicate hrefs, and return as a list\n",
        "    unique_best_hrefs = pd.Series(all_best_hrefs).drop_duplicates().tolist()\n",
        "\n",
        "    return unique_best_hrefs"
      ],
      "metadata": {
        "id": "wFQzH6L-LQ8B"
      },
      "id": "wFQzH6L-LQ8B",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## write_yaml"
      ],
      "metadata": {
        "id": "rTmF-kcqNN7P"
      },
      "id": "rTmF-kcqNN7P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e025f4f7"
      },
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "from datetime import datetime, date\n",
        "import numpy as np\n",
        "from google.colab import files # Import files for download functionality\n",
        "from typing import List, Tuple\n",
        "\n",
        "def write_yaml(listings_to_print: List[str], listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes_df: pd.DataFrame, out_file: Optional[str] = None, download: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Consolidates listing data from shortlist and notes DataFrames and saves it to a YAML file.\n",
        "    Dynamically adds client eligibility flags from shortlist columns.\n",
        "    If a listing has no status, it adds a 'seen' status with author 'beep_boop'.\n",
        "\n",
        "    Args:\n",
        "        listings_to_print (List[str]): List of hrefs to include in the YAML output.\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes_df (pd.DataFrame): DataFrame containing notes associated with listings.\n",
        "        out_file (Optional[str]): The filename to save the YAML to. If None, defaults to 'shortlist.yaml'.\n",
        "        download (bool): If True, the generated YAML file will be prompted for download.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame, potentially with new 'seen' entries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to convert pandas-specific types to standard Python equivalents\n",
        "    def to_python_type(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "        if isinstance(value, pd.Timestamp):\n",
        "            return value.to_pydatetime() # Convert pandas Timestamp to datetime object\n",
        "        if isinstance(value, (pd.Int64Dtype, np.int64)):\n",
        "            return int(value)\n",
        "        if isinstance(value, (pd.Float64Dtype, np.float64)):\n",
        "            return float(value)\n",
        "        if isinstance(value, (date, datetime)): # Use datetime.date and datetime\n",
        "            return value\n",
        "        return value\n",
        "\n",
        "    # Prepare notes_df (make a copy to ensure any internal modifications are to this copy)\n",
        "    current_notes_df = notes_df.copy()\n",
        "    current_notes_df['timestamp'] = pd.to_datetime(current_notes_df['timestamp'], errors='coerce')\n",
        "    current_notes_df.dropna(subset=['timestamp'], inplace=True)\n",
        "\n",
        "    # Filter listings_lr to include only the listings specified in listings_to_print\n",
        "    # Then sort by 'excess_value' in descending order\n",
        "    current_shortlist = listings_lr[listings_lr['href'].isin(listings_to_print)].copy()\n",
        "    if 'excess_value' in current_shortlist.columns:\n",
        "        current_shortlist = current_shortlist.sort_values(by='excess_value', ascending=False)\n",
        "    else:\n",
        "        print(\"Warning: 'excess_value' column not found, cannot sort by it.\")\n",
        "\n",
        "    all_listings_data = []\n",
        "\n",
        "    for idx, row in current_shortlist.iterrows():\n",
        "        href = row['href']\n",
        "        initial_status_for_listing = None # This will hold the status *before* any 'seen' logic\n",
        "        current_notes_for_listing = [] # Notes associated *before* any 'seen' logic\n",
        "\n",
        "        matching_notes = current_notes_df[current_notes_df['href'] == href]\n",
        "\n",
        "        if not matching_notes.empty:\n",
        "            matching_notes_sorted = matching_notes.sort_values(by='timestamp', ascending=False)\n",
        "            initial_status_for_listing = to_python_type(matching_notes_sorted.iloc[0]['status'])\n",
        "            current_notes_for_listing = [to_python_type(n) for n in matching_notes_sorted['note'].tolist() if pd.notna(n)]\n",
        "\n",
        "        # Create a dictionary named listing_data with the specified order and format\n",
        "        listing_data = {\n",
        "            'title': f\"{to_python_type(row['year'])}, {to_python_type(row['model_gen'])}, {int(to_python_type(row['odometer']))}k\",\n",
        "            'seller': to_python_type(row['seller']), # Modified to use seller\n",
        "            'listed_price': to_python_type(row['listed_price']),\n",
        "            'excess_value': int(to_python_type(row['excess_value'])), # Convert to int here\n",
        "            'href': to_python_type(row['href'])\n",
        "        }\n",
        "\n",
        "        # Dynamically add client eligibility from the allocations DataFrame\n",
        "        eligible_clients = allocations[\n",
        "            (allocations['href'] == href) & (allocations['allocation'] == True)\n",
        "        ]['client'].unique().tolist()\n",
        "        listing_data['clients'] = eligible_clients\n",
        "\n",
        "        # Add status and notes based on initial values\n",
        "        listing_data['status'] = initial_status_for_listing\n",
        "        listing_data['notes'] = current_notes_for_listing\n",
        "\n",
        "        all_listings_data.append(listing_data)\n",
        "\n",
        "        # Now, if the listing had no status, add 'seen' to current_notes_df for the *next* iteration\n",
        "        if initial_status_for_listing is None:\n",
        "            current_notes_df = add_note(current_notes_df, 'beep_boop', href, status='seen')\n",
        "\n",
        "    output_filename = out_file if out_file is not None else 'shortlist.yaml'\n",
        "    # Initialize yaml_content list\n",
        "    yaml_content = []\n",
        "    for listing in all_listings_data:\n",
        "        yaml_content.append('---\\n') # Add separator before each listing\n",
        "        yaml_content.append(yaml.dump(listing, allow_unicode=True, sort_keys=False))\n",
        "        yaml_content.append('\\n') # Add an extra newline after each dumped listing for readability\n",
        "\n",
        "    yaml_content_str = \"\".join(yaml_content)\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.write(yaml_content_str)\n",
        "\n",
        "    if download:\n",
        "        files.download(output_filename)\n",
        "        print(f\"The YAML file '{output_filename}' has been generated and prompted for download with {len(all_listings_data)} listings.\")\n",
        "    else:\n",
        "        print(f\"The YAML file '{output_filename}' has been generated with {len(all_listings_data)} listings (download skipped).\")\n",
        "\n",
        "    return current_notes_df # Return the potentially updated notes_df\n"
      ],
      "id": "e025f4f7",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apply_regression"
      ],
      "metadata": {
        "id": "1aAGDDUkOKsL"
      },
      "id": "1aAGDDUkOKsL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23f91c4e"
      },
      "source": [
        "def apply_regression(df: pd.DataFrame) -> (pd.DataFrame, pd.Series):\n",
        "    \"\"\"\n",
        "    Applies Huber regression to the input DataFrame to predict car prices.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing car listings.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame, pd.Series): A tuple containing:\n",
        "            - The DataFrame with 'market_value' and 'excess_value' columns added.\n",
        "            - A Series of unscaled regression coefficients.\n",
        "    \"\"\"\n",
        "    listings_lr = df.copy()\n",
        "\n",
        "    # 1) Coerce numeric types\n",
        "    listings_lr['year'] = pd.to_numeric(listings_lr['year'], errors='coerce')\n",
        "    listings_lr['odometer'] = pd.to_numeric(listings_lr['odometer'], errors='coerce')\n",
        "    listings_lr[\"listed_price\"] = pd.to_numeric(listings_lr[\"listed_price\"], errors='coerce')\n",
        "\n",
        "    # 2) One-hot encode model_gen\n",
        "    listings_lr[\"model_gen\"] = listings_lr[\"model_gen\"].astype(str)\n",
        "    dummies = pd.get_dummies(listings_lr[\"model_gen\"], prefix=\"mg_\", prefix_sep=\"\")\n",
        "\n",
        "    # remove base category \"civic_9\" if it exists\n",
        "    base_col = \"mg_civic_9\" # Corrected base column name to match dummy format\n",
        "    if base_col in dummies.columns:\n",
        "        dummies = dummies.drop(columns=[base_col])\n",
        "\n",
        "    listings_lr = pd.concat([listings_lr, dummies], axis=1)\n",
        "\n",
        "    # 3) Build X, y & keep mask\n",
        "    predictor_cols = ['age', 'odometer'] + list(dummies.columns)\n",
        "    X = listings_lr[predictor_cols].astype(float)\n",
        "    y = listings_lr[\"listed_price\"].astype(float)\n",
        "\n",
        "    keep = X.notna().all(axis=1) & y.notna()\n",
        "\n",
        "    X_keep = X.loc[keep]\n",
        "    y_keep = y.loc[keep]\n",
        "\n",
        "    # 4) Scale predictors\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_keep)\n",
        "\n",
        "    # 5) Fit Huber Regression\n",
        "    huber = HuberRegressor(max_iter=1000, epsilon=1.5)\n",
        "    huber.fit(X_scaled, y_keep)\n",
        "\n",
        "    # 6) Predict & store results\n",
        "    pred = huber.predict(X_scaled)\n",
        "    listings_lr.loc[keep, \"market_value\"] = pred\n",
        "    listings_lr.loc[keep, \"excess_value\"] = pred - listings_lr.loc[keep, \"listed_price\"]\n",
        "\n",
        "    # 7) Recover coefficients on the original (unscaled) feature scale\n",
        "    coef_scaled = huber.coef_\n",
        "    mu = scaler.mean_\n",
        "    sigma = scaler.scale_\n",
        "\n",
        "    original_intercept = huber.intercept_ - np.sum(coef_scaled * (mu / sigma))\n",
        "    original_coefs = coef_scaled / sigma\n",
        "\n",
        "    coef_unscaled = pd.Series(\n",
        "        np.concatenate([[original_intercept], original_coefs]),\n",
        "        index=[\"intercept\"] + predictor_cols\n",
        "    )\n",
        "\n",
        "    listings_lr = listings_lr.loc[:, ~listings_lr.columns.str.startswith(\"mg_\")]\n",
        "\n",
        "    return listings_lr, coef_unscaled"
      ],
      "id": "23f91c4e",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##add_note"
      ],
      "metadata": {
        "id": "h6aKUgtSfCr3"
      },
      "id": "h6aKUgtSfCr3"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def add_note(notes_df: pd.DataFrame, author: str, href: str, status: str = None, note: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a new note or updates a status for a specific href in the `notes_df` DataFrame.\n",
        "    Checks if the status/note already exists and adds it only if new.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The DataFrame containing historical notes and statuses.\n",
        "        author (str): The author of the note/status update.\n",
        "        href (str): The href of the listing to update.\n",
        "        status (str, optional): The new status. Defaults to None.\n",
        "        note (str, optional): The new note. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    current_notes = notes_df.copy()\n",
        "\n",
        "    # Ensure 'timestamp' column is datetime for comparison\n",
        "    current_notes['timestamp'] = pd.to_datetime(current_notes['timestamp'], errors='coerce')\n",
        "\n",
        "    current_timestamp = pd.Timestamp.now(tz='UTC')\n",
        "    new_entries = []\n",
        "    href = href.replace('https://', '').replace('http://', '').replace('www.', '') # Clean href\n",
        "\n",
        "    # Filter existing records for the current href\n",
        "    existing_notes_for_href = current_notes[current_notes['href'] == href].copy()\n",
        "    existing_notes_for_href.sort_values(by='timestamp', ascending=False, inplace=True)\n",
        "\n",
        "    latest_status_in_notes = None\n",
        "    if not existing_notes_for_href.empty:\n",
        "        latest_status_in_notes = existing_notes_for_href.iloc[0]['status']\n",
        "\n",
        "    existing_note_texts = set(existing_notes_for_href['note'].dropna().tolist())\n",
        "\n",
        "    # Check and add new status if provided and different\n",
        "    if status is not None and status != latest_status_in_notes:\n",
        "        new_entries.append({\n",
        "            'href': href,\n",
        "            'timestamp': current_timestamp,\n",
        "            'author': author,\n",
        "            'status': status,\n",
        "            'note': pd.NA\n",
        "        })\n",
        "\n",
        "    # Check and add new note if provided and not already existing\n",
        "    if note is not None and note not in existing_note_texts:\n",
        "        new_entries.append({\n",
        "            'href': href,\n",
        "            'timestamp': current_timestamp,\n",
        "            'author': author,\n",
        "            'status': pd.NA, # Status is not changing, just adding a note\n",
        "            'note': note\n",
        "        })\n",
        "\n",
        "    if new_entries:\n",
        "        new_notes_df = pd.DataFrame(new_entries)\n",
        "        new_notes_df['timestamp'] = pd.to_datetime(new_notes_df['timestamp'])\n",
        "        # Ensure consistent column order and dtypes. If current_notes is empty, ensure new_notes_df has the correct columns.\n",
        "        if current_notes.empty:\n",
        "            # Define columns if current_notes is empty, assuming standard notes DataFrame columns\n",
        "            # This is a fallback and might need adjustment if notes columns vary significantly.\n",
        "            new_notes_df = new_notes_df.reindex(columns=['href', 'timestamp', 'author', 'status', 'note'])\n",
        "        else:\n",
        "            new_notes_df = new_notes_df.reindex(columns=current_notes.columns)\n",
        "\n",
        "        current_notes = pd.concat([current_notes, new_notes_df], ignore_index=True)\n",
        "\n",
        "        # --- User's requested logic for YAML export ---\n",
        "        if status in [\"message_left\", \"follow_up\"]:\n",
        "\n",
        "            # Get the latest status for each href in the *updated* notes DataFrame\n",
        "            latest_notes_status = current_notes.sort_values(by='timestamp', ascending=False).drop_duplicates(subset=['href'], keep='first')\n",
        "\n",
        "            # Filter for 'message_left' or 'follow_up' statuses\n",
        "            hrefs_for_yaml = latest_notes_status[\n",
        "                latest_notes_status['status'].isin([\"message_left\", \"follow_up\"])\n",
        "            ]['href'].unique().tolist()\n",
        "\n",
        "            if hrefs_for_yaml:\n",
        "                print(f\"Found {len(hrefs_for_yaml)} listings with current status 'message_left' or 'follow_up'.\")\n",
        "                # Access global variables for write_yaml\n",
        "                # These are assumed to be defined in the global scope of the notebook.\n",
        "                from __main__ import listings_lr, allocations, write_yaml\n",
        "\n",
        "                output_path = \"/content/drive/Shareddrives/market_analysis_v2/message_left.yaml\"\n",
        "                write_yaml(\n",
        "                    listings_to_print=hrefs_for_yaml,\n",
        "                    listings_lr=listings_lr,\n",
        "                    allocations=allocations,\n",
        "                    notes_df=current_notes, # Pass the updated notes DataFrame\n",
        "                    out_file=output_path,\n",
        "                    download=False # Do not prompt for download in a background task\n",
        "                )\n",
        "        # --- End of user's requested logic ---\n",
        "\n",
        "    return current_notes"
      ],
      "metadata": {
        "id": "3AqI6jYKe9GJ"
      },
      "id": "3AqI6jYKe9GJ",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_notes"
      ],
      "metadata": {
        "id": "rylENVkd9dWD"
      },
      "id": "rylENVkd9dWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def update_notes(notes_df: pd.DataFrame, update_yaml: list, author: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes shortlist data and updates the `notes_df` DataFrame\n",
        "    with new status changes and notes using the `add_note` function.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The DataFrame containing historical notes and statuses.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "        author (str): The author of the notes.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame.\n",
        "    \"\"\"\n",
        "    updated_notes_df = notes_df.copy()\n",
        "    initial_notes_count = len(updated_notes_df)\n",
        "\n",
        "    for listing in update_yaml:\n",
        "        current_href = listing['href']\n",
        "        current_status_from_yaml = listing['status']\n",
        "        notes_list_from_yaml = listing['notes'] if listing['notes'] is not None else []\n",
        "\n",
        "        # Update status using add_note\n",
        "        updated_notes_df = add_note(updated_notes_df, author, current_href, status=current_status_from_yaml)\n",
        "\n",
        "        # Update individual notes using add_note\n",
        "        for note_text in notes_list_from_yaml:\n",
        "            if pd.notna(note_text): # Ensure note_text is not NaN before passing\n",
        "                updated_notes_df = add_note(updated_notes_df, author, current_href, note=note_text)\n",
        "\n",
        "    final_notes_count = len(updated_notes_df)\n",
        "    added_entries_count = final_notes_count - initial_notes_count\n",
        "\n",
        "    if added_entries_count > 0:\n",
        "        print(f\"Total {added_entries_count} new entries added to notes DataFrame through update_notes.\")\n",
        "    else:\n",
        "        print(\"No new notes or status updates to add via update_notes.\")\n",
        "\n",
        "    return updated_notes_df"
      ],
      "metadata": {
        "id": "2dhYD0Mv9dy4"
      },
      "id": "2dhYD0Mv9dy4",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_seller"
      ],
      "metadata": {
        "id": "0sQh5O7bDJyK"
      },
      "id": "0sQh5O7bDJyK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def update_seller(listings_df: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the 'seller' column in listings_df based on information from update_yaml.\n",
        "\n",
        "    Args:\n",
        "        update_yaml (list): A list of dictionaries parsed from a YAML file, potentially containing\n",
        "                                   'href' and 'seller' information.\n",
        "        listings_df (pd.DataFrame): The DataFrame of car listings to be updated.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified listings_df with updated 'seller' information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    updated_listings_df = listings_df.copy()\n",
        "\n",
        "    # Ensure 'seller' column is of object (string) type to accommodate string assignments\n",
        "    # This prevents FutureWarning when assigning strings to a float64 column that might contain NaNs\n",
        "    if 'seller' in updated_listings_df.columns and updated_listings_df['seller'].dtype != object:\n",
        "        updated_listings_df['seller'] = updated_listings_df['seller'].astype(\"string\")\n",
        "\n",
        "    for item in update_yaml:\n",
        "        seller = item.get('seller')\n",
        "        href = item.get('href')\n",
        "\n",
        "        if seller is not None and href is not None:\n",
        "            # Clean the href string using re.sub for regex replacement\n",
        "            cleaned_href = re.sub(r'^(https?://)?(www\\.)?', '', str(href))\n",
        "\n",
        "            # Update the 'seller' column for matching 'href' entries\n",
        "            updated_listings_df.loc[updated_listings_df['href'] == cleaned_href, 'seller'] = seller\n",
        "\n",
        "    return updated_listings_df\n",
        "\n",
        "print(\"Defined `update_seller` function.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ogNU9_UDJPo",
        "outputId": "05397d2d-0d41-44ad-dd39-e9d0fe42193f"
      },
      "id": "4ogNU9_UDJPo",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined `update_seller` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_allocations"
      ],
      "metadata": {
        "id": "c3Gz3FZoSO0o"
      },
      "id": "c3Gz3FZoSO0o"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def update_allocations(allocations: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the allocations DataFrame based on information from the update_yaml.\n",
        "\n",
        "    Business Logic for update_allocations function:\n",
        "\n",
        "    This function processes updates from a YAML file (typically `shortlist-edited.yaml`) to modify\n",
        "    the `allocations` DataFrame. The key business rules applied are:\n",
        "\n",
        "    1.  **De-allocation of Unspecified Clients:** For listings present in the `update_yaml`,\n",
        "        if a client was previously allocated to that listing but is *not* present\n",
        "        in the `clients` list for that listing in the `update_yaml`, their `allocation` status in the\n",
        "        `allocations` DataFrame will be set to `False`. This ensures that clients no longer interested\n",
        "        in a particular listing (as indicated by the YAML) are de-allocated.\n",
        "    2.  **Preservation of Other Allocations:** Allocations for listings not mentioned in the `update_yaml`,\n",
        "        or allocations for clients still specified in the `update_yaml`, remain unchanged.\n",
        "\n",
        "    Args:\n",
        "        allocations (pd.DataFrame): The existing DataFrame of allocations.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified allocations DataFrame.\n",
        "    \"\"\"\n",
        "    # 1. Create a copy of the input allocations DataFrame named allocations_copy.\n",
        "    allocations_copy = allocations.copy()\n",
        "\n",
        "    # 2. Iterate through update_yaml to create a dictionary update_lookup where keys are href values\n",
        "    # and values are dictionaries containing the status and a list of clients for that href from the YAML.\n",
        "    update_lookup = {}\n",
        "    for item in update_yaml:\n",
        "        href = item.get('href')\n",
        "        status = item.get('status')\n",
        "        clients_from_yaml = item.get('clients', [])\n",
        "        # Fix: Ensure clients_from_yaml is always a list, even if 'clients' key has a None value\n",
        "        if clients_from_yaml is None:\n",
        "            clients_from_yaml = []\n",
        "        if href:\n",
        "            # Clean the href string, consistent with enrich_df and update_seller\n",
        "            cleaned_href = href.replace('https://', '').replace('http://', '').replace('www.', '')\n",
        "            update_lookup[cleaned_href] = {\n",
        "                'status': status,\n",
        "                'clients': clients_from_yaml\n",
        "            }\n",
        "\n",
        "    # All hrefs from update_lookup are considered for de-allocation logic, regardless of status\n",
        "    hrefs_in_yaml = set(update_lookup.keys())\n",
        "\n",
        "    # 3. Create a set current_active_allocations containing (href, client) tuples for all entries\n",
        "    # in the modified allocations_copy where allocation is True.\n",
        "    current_active_allocations = set(\n",
        "        allocations_copy[allocations_copy['allocation'] == True]\n",
        "        [['href', 'client']].apply(tuple, axis=1)\n",
        "    )\n",
        "\n",
        "    # 4. Create a set yaml_should_be_active containing (href, client) tuples for all clients\n",
        "    # associated with hrefs present in update_lookup.\n",
        "    yaml_should_be_active = set()\n",
        "    for href in hrefs_in_yaml:\n",
        "        data = update_lookup[href]\n",
        "        for client in data['clients']:\n",
        "            yaml_should_be_active.add((href, client))\n",
        "\n",
        "    # Filter current_active_allocations to only include hrefs that are in the update_yaml scope\n",
        "    active_allocations_in_yaml_scope = {\n",
        "        (h, c) for h, c in current_active_allocations if h in hrefs_in_yaml\n",
        "    }\n",
        "\n",
        "    # 5. Determine the set of (href, client) pairs that need to be de-allocated.\n",
        "    # These are allocations that were active within the scope of hrefs mentioned in YAML,\n",
        "    # but are not present in the 'clients' list for those hrefs in the YAML.\n",
        "    to_deallocate = active_allocations_in_yaml_scope - yaml_should_be_active\n",
        "\n",
        "    # 6. Iterate through the identified (href, client) pairs and set the allocation column to\n",
        "    # False for those specific entries in allocations_copy.\n",
        "    for href, client in to_deallocate:\n",
        "        allocations_copy.loc[\n",
        "            (allocations_copy['href'] == href) & (allocations_copy['client'] == client),\n",
        "            'allocation'\n",
        "        ] = False\n",
        "\n",
        "    # Ensure the 'allocation' column is boolean type\n",
        "    allocations_copy['allocation'] = allocations_copy['allocation'].astype('boolean')\n",
        "\n",
        "    # 7. Return the modified allocations_copy DataFrame.\n",
        "    print(f\"De-allocated {len(to_deallocate)} entries based on YAML updates.\")\n",
        "    return allocations_copy"
      ],
      "metadata": {
        "id": "apPs89oFSOgu"
      },
      "id": "apPs89oFSOgu",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##email_client"
      ],
      "metadata": {
        "id": "-j6txk8TrYiJ"
      },
      "id": "-j6txk8TrYiJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# def email_client(hrefs, listings_lr, coefficients, notes):\n",
        "\n",
        "# # print rows\n",
        "# for _, row in best_n.iterrows():\n",
        "#     print(f\"{model_name} ({row['rank']})\")\n",
        "#     print(f\"Link: {row['href']}\")\n",
        "#     print(f\"Market Value: ${row['predicted_price']:,.0f}\")\n",
        "#     print(f\"Listed Price: ${row[\"listed_price\"]:,}\")\n",
        "#     try:\n",
        "#         print(f\"Negotiated Price: ${row['nego_price']:,.0f}\")\n",
        "#     except KeyError as e:\n",
        "#         pass\n",
        "#     print(f\"Year: {row['year']:.0f}\")\n",
        "#     print(f\"Odometer: {row['odometer']:,.0f},000km\")\n",
        "#     print(f\"Notes:\\n\")\n",
        "\n",
        "\n",
        "# # Produce scatterplot\n",
        "# # Function to format price axis\n",
        "# def price_format(x, _):\n",
        "#     return f'${int(x):,}'\n",
        "\n",
        "# # Plotting\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# # Compute age\n",
        "# other_listings['age'] = 2026 - other_listings['year']\n",
        "# best_n['age'] = 2026 - best_n['year']\n",
        "\n",
        "# # Scatter (Year vs Price)\n",
        "# ax1.scatter(other_listings['year'], other_listings[\"listed_price\"],\n",
        "#             label='Data', color='lightsteelblue', s=20)\n",
        "\n",
        "# for _, row in best_n.iterrows():\n",
        "#     ax1.scatter(row['year'], row[\"listed_price\"], s=70, facecolors='none', linewidths=1.2)\n",
        "#     ax1.text(row['year'], row[\"listed_price\"], str(int(row['rank'])),\n",
        "#              ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#              color='red', alpha=0.7)\n",
        "#     if not pd.isna(row['nego_price']):\n",
        "#         ax1.text(row['year'], row['nego_price'], str(int(row['rank'])),\n",
        "#                  ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#                  color='green', alpha=0.7)\n",
        "\n",
        "# # Regression line (fix odometer at mean)\n",
        "# year_range = np.linspace(other_listings['year'].min(),\n",
        "#                          other_listings['year'].max(), 100)\n",
        "\n",
        "# age_range = 2026 - year_range  # convert back to age for model input\n",
        "# mean_odometer = other_listings['odometer'].mean()\n",
        "\n",
        "# X_line = pd.DataFrame({\n",
        "#     'const': 1,\n",
        "#     'age': age_range,\n",
        "#     'odometer': [mean_odometer] * 100\n",
        "# })\n",
        "\n",
        "# y_line = model.predict(X_line)\n",
        "\n",
        "# ax1.plot(year_range, y_line, label='Regression line')\n",
        "\n",
        "# ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# ax1.yaxis.set_major_formatter(FuncFormatter(price_format))\n",
        "# ax1.set_xlabel('Model Year')\n",
        "# ax1.set_ylabel(\"listed_price\")\n",
        "# ax1.set_title(f\"{model_name} Price vs Year\")\n",
        "\n",
        "# # Scatter (Odometer vs Price)\n",
        "# ax2.scatter(other_listings['odometer'], other_listings[\"listed_price\"],\n",
        "#             label='Data', color='lightsteelblue', s=20)\n",
        "\n",
        "# for _, row in best_n.iterrows():\n",
        "#     ax2.scatter(row['odometer'], row[\"listed_price\"], s=70, facecolors='none', linewidths=1.2)\n",
        "#     ax2.text(row['odometer'], row[\"listed_price\"], str(int(row['rank'])),\n",
        "#              ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#              color='red', alpha=0.7)\n",
        "#     if not pd.isna(row['nego_price']):\n",
        "#         ax2.text(row['odometer'], row['nego_price'], str(int(row['rank'])),\n",
        "#                  ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#                  color='green', alpha=0.7)\n",
        "\n",
        "# # Regression line (fix age at mean)\n",
        "# odometer_range = np.linspace(other_listings['odometer'].min(),\n",
        "#                              other_listings['odometer'].max(), 100)\n",
        "\n",
        "# mean_age = other_listings['age'].mean()\n",
        "\n",
        "# X_line2 = pd.DataFrame({\n",
        "#     'const': 1,\n",
        "#     'age': [mean_age] * 100,\n",
        "#     'odometer': odometer_range\n",
        "# })\n",
        "\n",
        "# y_line2 = model.predict(X_line2)\n",
        "# ax2.plot(odometer_range, y_line2, label='Regression line')\n",
        "\n",
        "# ax2.yaxis.set_major_formatter(FuncFormatter(price_format))\n",
        "# ax2.set_xlabel('Odometer (kms)')\n",
        "# ax2.set_ylabel(\"listed_price\")\n",
        "# ax2.set_title(f\"{model_name} Price vs Mileage\")\n",
        "\n",
        "\n",
        "# # Legend handles\n",
        "# live_listing_handle = Line2D([], [], marker='o', color='lightsteelblue', linestyle='None', markersize=6, label=f'Listing as of {df1.iloc[0][\"date_scraped\"]}')\n",
        "# listed_price_handle = Line2D([], [], marker='o', color='red', linestyle='None',\n",
        "#                                  markersize=8, label='Listed Price')\n",
        "# negotiated_price_handle = Line2D([], [], marker='o', color='green', linestyle='None',\n",
        "#                                  markersize=8, label='Negotiated Price')\n",
        "\n",
        "# # Apply legend to both subplots\n",
        "# ax1.legend(handles=[live_listing_handle, listed_price_handle, negotiated_price_handle])\n",
        "# ax2.legend(handles=[live_listing_handle, listed_price_handle, negotiated_price_handle])\n",
        "\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "udv17eUKrcwl"
      },
      "id": "udv17eUKrcwl",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ZAdTyLbASdqQ"
      },
      "id": "ZAdTyLbASdqQ"
    },
    {
      "cell_type": "code",
      "source": [
        "clients=[\n",
        "    {\n",
        "        \"client\":\"anita_c\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"jazz_3\",\n",
        "            \"i30_2\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"magesh_t\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"i30_2\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"raymon_s\",\n",
        "        \"max_listing_price\":11000,\n",
        "        \"max_odometer\":210,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"civic_8\",\n",
        "            \"i30_2\",\n",
        "            \"city_1\",\n",
        "            \"city_2\",\n",
        "            \"corolla_10\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "Eb8E8j5Fsaxy"
      },
      "id": "Eb8E8j5Fsaxy",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2ea91a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2ea91a",
        "outputId": "40459b06-fed1-4e2a-8756-9aa3ca7514ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, Optional, List\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataframes\n",
        "gen_lookup = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/gen_lookup.csv\")\n",
        "listings = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/listings.csv\")\n",
        "notes = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/notes.csv\", index_col=0)\n",
        "allocations = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/allocations.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "YJKBICNE_iDu"
      },
      "id": "YJKBICNE_iDu",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statuses = {\n",
        "    None:\"No status saved\",\n",
        "    \"seen\": \"listing has been printed to YAML at least once\",\n",
        "    \"rejected\": \"listing not suitable for any buyer\",\n",
        "    \"sold\": \"sold or on hold\",\n",
        "    \"shortlisted\": \"VA checked listing and looks good\",\n",
        "    \"contacted\": \"Roger has contacted the seller\",\n",
        "    \"message_left\": \"self explanatory\",\n",
        "    \"follow_up\": \"Roger to call seller\",\n",
        "    \"inspection\": \"Inspection booked\",\n",
        "    \"deposit\": \"Deposit left with seller\",\n",
        "    \"purchased\": \"Self explainatory\",\n",
        "    \"bad_inspection\": \"Not recommended after inspection (Roger/Andrew)\",\n",
        "}"
      ],
      "metadata": {
        "id": "whpy5rTojg7Y"
      },
      "id": "whpy5rTojg7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working"
      ],
      "metadata": {
        "id": "d3osOXG0ExJz"
      },
      "id": "d3osOXG0ExJz"
    },
    {
      "cell_type": "code",
      "source": [
        "z = compare_new_listings(listings, gen_lookup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiTdqt2VxNCE",
        "outputId": "a6850289-12eb-437f-c21d-667fc42fe08d"
      },
      "id": "BiTdqt2VxNCE",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/carsales (1).csv    \t n_new=2   \t n_updated=1 \t n_unchanged=5 \t Tot 8\n",
            "/content/carsales (5).csv    \t n_new=9   \t n_updated=3 \t n_unchanged=10 \t Tot 22\n",
            "/content/carsales (7).csv    \t n_new=3   \t n_updated=1 \t n_unchanged=18 \t Tot 22\n",
            "/content/carsales (4).csv    \t n_new=10   \t n_updated=3 \t n_unchanged=9 \t Tot 22\n",
            "/content/carsales (10).csv    \t n_new=5   \t n_updated=0 \t n_unchanged=17 \t Tot 22\n",
            "/content/carsales (9).csv    \t n_new=5   \t n_updated=0 \t n_unchanged=17 \t Tot 22\n",
            "/content/carsales (2).csv    \t n_new=6   \t n_updated=7 \t n_unchanged=9 \t Tot 22\n",
            "/content/carsales (3).csv    \t n_new=10   \t n_updated=1 \t n_unchanged=11 \t Tot 22\n",
            "/content/carsales (6).csv    \t n_new=9   \t n_updated=2 \t n_unchanged=11 \t Tot 22\n",
            "/content/carsales (8).csv    \t n_new=8   \t n_updated=0 \t n_unchanged=14 \t Tot 22\n",
            "/content/carsales.csv    \t n_new=6   \t n_updated=2 \t n_unchanged=6 \t Tot 14\n",
            "/content/facebook (4).csv    \t n_new=32   \t n_updated=1 \t n_unchanged=11 \t Tot 44\n",
            "/content/facebook.csv    \t n_new=33   \t n_updated=2 \t n_unchanged=17 \t Tot 52\n",
            "/content/facebook (3).csv    \t n_new=43   \t n_updated=1 \t n_unchanged=8 \t Tot 52\n",
            "/content/facebook (1).csv    \t n_new=27   \t n_updated=5 \t n_unchanged=20 \t Tot 52\n",
            "/content/facebook (2).csv    \t n_new=42   \t n_updated=1 \t n_unchanged=9 \t Tot 52\n",
            "\t \t \t \t unq_new=248 \t unq_updated=30\t unq_unchanged=190 unq_tot=468\n",
            "WARNING: Column 'model_gen' in enriched_new_listings has 9 missing values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d173a02",
        "outputId": "389e9663-dc48-4f9d-867c-dd903cd2ed84"
      },
      "source": [
        "# Add new listings to listings dataframe\n",
        "listings = integrate_listings(listings, gen_lookup)"
      ],
      "id": "6d173a02",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final DataFrame has 1097 unique listings after merging and de-duplication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29cd174a"
      },
      "source": [
        "listings_lr, coefficients = apply_regression(listings)"
      ],
      "id": "29cd174a",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = allocate_listings(listings_lr, notes, allocations)"
      ],
      "metadata": {
        "id": "xHaWD0sWsCbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40461d9e-dfb9-429a-d095-76bd5f1680c1"
      },
      "id": "xHaWD0sWsCbT",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 0 new allocation entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_listings = get_best_listings(listings_lr, allocations, notes, clients)"
      ],
      "metadata": {
        "id": "6j5GN7w_NIZ4"
      },
      "id": "6j5GN7w_NIZ4",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "new_cell",
        "outputId": "daceaa5f-386c-4c74-8d6a-8a13b7c0e1f0"
      },
      "source": [
        "# Call the updated output_shortlist function\n",
        "notes = write_yaml(best_listings, listings_lr, allocations, notes)\n"
      ],
      "id": "new_cell",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d405921c-2474-471b-8dac-a04199ec8bec\", \"shortlist.yaml\", 3935)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The YAML file 'shortlist.yaml' has been generated and prompted for download with 20 listings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efded001",
        "outputId": "d5805a42-c7e0-4721-f0c1-5f91e45e3b99"
      },
      "source": [
        "with open('/content/shortlist-edited.yaml', 'r') as file:\n",
        "    update_yaml = list(yaml.safe_load_all(file))\n",
        "\n",
        "print(\"YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\")"
      ],
      "id": "efded001",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "author = \"roger\"\n",
        "notes = update_notes(notes, update_yaml, author)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOr7wbC_8yU",
        "outputId": "2e928f8d-5ddf-41d2-e06e-9582b946dc41"
      },
      "id": "4rOr7wbC_8yU",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 12 new entries added to notes DataFrame through update_notes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listings = update_seller(listings, update_yaml)"
      ],
      "metadata": {
        "id": "14AXxgkRA5TY"
      },
      "id": "14AXxgkRA5TY",
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = update_allocations(allocations, update_yaml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izxraNyocXHW",
        "outputId": "2d3f7e51-c23e-4238-8c7b-537170161da8"
      },
      "id": "izxraNyocXHW",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De-allocated 5 entries based on YAML updates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_df(\"listings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HlE0uDbW_2t",
        "outputId": "5a1c1fca-b361-4f7b-85ca-efb41ac9f734"
      },
      "id": "8HlE0uDbW_2t",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'listings' on Drive has 868 rows.\n",
            "'listings' current has 1097 rows.\n",
            "Number of new rows: 230\n",
            "Number of removed rows: 1\n",
            "Number of changed rows: 27\n",
            "Row changes per column:\n",
            "gen              0\n",
            "model_gen        1\n",
            "seller_type      0\n",
            "odometer         1\n",
            "location         1\n",
            "date_scraped    27\n",
            "age              1\n",
            "model            1\n",
            "make             1\n",
            "year             1\n",
            "seller           0\n",
            "trim             1\n",
            "listed_price    27\n",
            "dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't compare allocations/notes as it compares on \"href\"\n",
        "# \"href\" is not unique key\n",
        "# compare_df(\"allocations\")\n",
        "# compare_df(\"notes\")"
      ],
      "metadata": {
        "id": "MQwBcAViXFIc"
      },
      "id": "MQwBcAViXFIc",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_df(listings, \"listings\")\n",
        "save_df(allocations, \"allocations\")\n",
        "save_df(notes, \"notes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BKFOmSKYr35",
        "outputId": "d56fd4c4-81ae-431e-f5fe-9d18f5cb23ef"
      },
      "id": "5BKFOmSKYr35",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote: /content/drive/Shareddrives/market_analysis_v2/listings\n",
            "Archived to: /content/drive/Shareddrives/market_analysis_v2/archive/listings_20251213\n",
            "Overwrote: /content/drive/Shareddrives/market_analysis_v2/allocations\n",
            "Archived to: /content/drive/Shareddrives/market_analysis_v2/archive/allocations_20251213\n",
            "Overwrote: /content/drive/Shareddrives/market_analysis_v2/notes\n",
            "Archived to: /content/drive/Shareddrives/market_analysis_v2/archive/notes_20251213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imVGZN-qX3Q0"
      },
      "id": "imVGZN-qX3Q0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roger"
      ],
      "metadata": {
        "id": "VfqlksLNmY4W"
      },
      "id": "VfqlksLNmY4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# After\n",
        "\n",
        "new_notes = add_note(\n",
        "    new_notes,\n",
        "    \"roger\",\n",
        "    \"\", #href\n",
        "    status=\"message_left\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "1NTRYmb5mbH_",
        "outputId": "b96cf1ee-d9dd-405e-a6ae-01b063cb1f19"
      },
      "id": "1NTRYmb5mbH_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'new_notes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2952586502.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m new_notes = add_note(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnew_notes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"roger\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#href\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contacted\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_notes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H4KefG6nb8O"
      },
      "id": "0H4KefG6nb8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65d5df39"
      },
      "source": [
        "# Task\n",
        "Define a Python function named `compare_df` that takes `df_name` as an argument. This function should load an 'old' DataFrame from `/content/drive/Shareddrives/market_analysis_v2/{df_name}.csv` and compare it with the 'current' `listings` DataFrame. The comparison should be based on 'href' as the unique identifier and should identify new rows, removed rows, and changed rows (including changes per column). Finally, print the comparison results, including the number of new rows, removed rows, changed rows, and a series of row changes per column."
      ],
      "id": "65d5df39"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2027e97b"
      },
      "source": [
        "## Define compare_df Function Structure\n",
        "\n",
        "### Subtask:\n",
        "Create the basic structure of the `compare_df` function, accepting `df_name` as an argument. This step will also include loading the 'old' DataFrame from the drive based on `df_name`.\n"
      ],
      "id": "2027e97b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1d2007"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `compare_df` function as requested by the user, taking `df_name` as an argument and loading the 'old' DataFrame from the specified path.\n",
        "\n"
      ],
      "id": "0e1d2007"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b537384e"
      },
      "source": [
        "def compare_df(df_name):\n",
        "    # Load the 'old' DataFrame from the drive\n",
        "    a = pd.read_csv(f\"/content/drive/Shareddrives/market_analysis_v2/{df_name}.csv\")\n",
        "\n",
        "    # The global 'listings' DataFrame is implicitly accessible within this function\n",
        "    # No explicit 'global listings' declaration is strictly needed for reading, but good to acknowledge.\n",
        "    # For comparison, 'listings' would be used directly as it's a global variable.\n",
        "\n",
        "    print(f\"Loaded old '{df_name}' DataFrame with {len(a)} rows.\")\n",
        "    print(f\"The global 'listings' DataFrame (current state) has {len(listings)} rows.\")"
      ],
      "id": "b537384e",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e09eb3a6"
      },
      "source": [
        "## Implement DataFrame Indexing and Alignment\n",
        "\n",
        "### Subtask:\n",
        "Modify the `compare_df` function to index both the loaded 'old' DataFrame and the corresponding 'current' DataFrame (e.g., `listings`, `allocations`, or `notes`) by 'href'. Then, align them to find common rows and identify new/removed rows. Variables will be renamed for clarity (e.g., `old_df`, `current_df`, `old_df_indexed`, `current_df_indexed`).\n"
      ],
      "id": "e09eb3a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91dc52c1"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `compare_df` function to implement DataFrame indexing, alignment, and calculation of new/removed rows as per the subtask instructions. This involves renaming variables, dynamically selecting the current DataFrame, setting 'href' as the index, finding common elements, and calculating row differences.\n",
        "\n"
      ],
      "id": "91dc52c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5218778a"
      },
      "source": [
        "def compare_df(df_name):\n",
        "    # Load the 'old' DataFrame from the drive\n",
        "    old_df = pd.read_csv(f\"/content/drive/Shareddrives/market_analysis_v2/{df_name}.csv\")\n",
        "\n",
        "    # Determine the current_df based on df_name\n",
        "    if df_name == 'listings':\n",
        "        current_df = listings\n",
        "    elif df_name == 'allocations':\n",
        "        current_df = allocations\n",
        "    elif df_name == 'notes':\n",
        "        current_df = notes\n",
        "    else:\n",
        "        raise ValueError(f\"df_name '{df_name}' not supported for comparison.\")\n",
        "\n",
        "    # Set 'href' as the index for both DataFrames\n",
        "    old_df_indexed = old_df.set_index(\"href\")\n",
        "    current_df_indexed = current_df.set_index(\"href\")\n",
        "\n",
        "    # Identify common 'href' values\n",
        "    common_idx = old_df_indexed.index.intersection(current_df_indexed.index)\n",
        "\n",
        "    # Create common DataFrames\n",
        "    old_common = old_df_indexed.loc[common_idx]\n",
        "    current_common = current_df_indexed.loc[common_idx]\n",
        "\n",
        "    # Calculate new and removed rows\n",
        "    n_new_rows = len(current_df_indexed.index.difference(old_df_indexed.index))\n",
        "    n_removed_rows = len(old_df_indexed.index.difference(current_df_indexed.index))\n",
        "\n",
        "    print(f\"Loaded old '{df_name}' DataFrame with {len(old_df)} rows.\")\n",
        "    print(f\"The global '{df_name}' DataFrame (current state) has {len(current_df)} rows.\")\n",
        "    print(f\"Number of new rows: {n_new_rows}\")\n",
        "    print(f\"Number of removed rows: {n_removed_rows}\")\n",
        "\n",
        "    # Placeholder for change_mask and further calculations, will be implemented in next step\n",
        "    # change_mask = (\n",
        "    #     old_common.ne(current_common)\n",
        "    #     & ~(old_common.isna() & current_common.isna())\n",
        "    # )\n",
        "    # n_changed_rows = change_mask.any(axis=1).sum()\n",
        "    # row_changes_per_column = change_mask.sum()\n",
        "\n",
        "    # print(f\"Number of changed rows: {n_changed_rows}\")\n",
        "    # print(f\"Row changes per column:\\n{row_changes_per_column}\")"
      ],
      "id": "5218778a",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e32d82b9"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to complete the `compare_df` function by implementing the logic for identifying changed rows and changes per column, as outlined in the overall task description and implicitly suggested by the commented code in the previous step. This involves creating a `change_mask`, calculating `n_changed_rows`, and `row_changes_per_column`.\n",
        "\n"
      ],
      "id": "e32d82b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d14d605d"
      },
      "source": [
        "def compare_df(df_name):\n",
        "    # Load the 'old' DataFrame from the drive\n",
        "    old_df = pd.read_csv(f\"/content/drive/Shareddrives/market_analysis_v2/{df_name}.csv\")\n",
        "\n",
        "    # Determine the current_df based on df_name\n",
        "    if df_name == 'listings':\n",
        "        current_df = listings\n",
        "    elif df_name == 'allocations':\n",
        "        current_df = allocations\n",
        "    elif df_name == 'notes':\n",
        "        current_df = notes\n",
        "    else:\n",
        "        raise ValueError(f\"df_name '{df_name}' not supported for comparison.\")\n",
        "\n",
        "    # Set 'href' as the index for both DataFrames\n",
        "    old_df_indexed = old_df.set_index(\"href\")\n",
        "    current_df_indexed = current_df.set_index(\"href\")\n",
        "\n",
        "    # Identify common 'href' values\n",
        "    common_idx = old_df_indexed.index.intersection(current_df_indexed.index)\n",
        "\n",
        "    # Create common DataFrames\n",
        "    old_common = old_df_indexed.loc[common_idx]\n",
        "    current_common = current_df_indexed.loc[common_idx]\n",
        "\n",
        "    # Calculate new and removed rows\n",
        "    n_new_rows = len(current_df_indexed.index.difference(old_df_indexed.index))\n",
        "    n_removed_rows = len(old_df_indexed.index.difference(current_df_indexed.index))\n",
        "\n",
        "    # Define change mask ONCE (NaN-safe)\n",
        "    # Align columns before comparison to avoid errors if columns differ\n",
        "    aligned_old_common, aligned_current_common = old_common.align(current_common, join='outer', axis=1)\n",
        "\n",
        "    # Fill NaN from alignment with a distinct placeholder to prevent false positives in .ne()\n",
        "    # Using a string placeholder for object/mixed types, and a numeric one for numeric types\n",
        "    # This ensures NaNs introduced by align are treated as different\n",
        "    aligned_old_common_filled = aligned_old_common.fillna('__NAN_OLD__')\n",
        "    aligned_current_common_filled = aligned_current_common.fillna('__NAN_CURRENT__')\n",
        "\n",
        "    change_mask = (aligned_old_common_filled != aligned_current_common_filled)\n",
        "\n",
        "    # Exclude columns that exist in one but not the other after alignment\n",
        "    # For accurate change detection, we only care about columns common to both original dataframes\n",
        "    original_common_cols = list(set(old_common.columns) & set(current_common.columns))\n",
        "    change_mask = change_mask[original_common_cols]\n",
        "\n",
        "    # Calculate changed rows (any column)\n",
        "    n_changed_rows = change_mask.any(axis=1).sum()\n",
        "\n",
        "    # Calculate row changes per column\n",
        "    row_changes_per_column = change_mask.sum()\n",
        "\n",
        "    print(f\"Loaded old '{df_name}' DataFrame with {len(old_df)} rows.\")\n",
        "    print(f\"The global '{df_name}' DataFrame (current state) has {len(current_df)} rows.\")\n",
        "    print(f\"Number of new rows: {n_new_rows}\")\n",
        "    print(f\"Number of removed rows: {n_removed_rows}\")\n",
        "    print(f\"Number of changed rows: {n_changed_rows}\")\n",
        "    print(f\"Row changes per column:\\n{row_changes_per_column}\")\n"
      ],
      "id": "d14d605d",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bf2b71c"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `compare_df` function is fully defined, I will call it with `df_name='listings'` to demonstrate its complete functionality and verify the comparison results, including new rows, removed rows, changed rows, and changes per column for the 'listings' DataFrame.\n",
        "\n"
      ],
      "id": "4bf2b71c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "3e9377de",
        "outputId": "1319745a-64c4-45f5-aa27-a539177c457a"
      },
      "source": [
        "compare_df('listings')"
      ],
      "id": "3e9377de",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Invalid value '__NAN_CURRENT__' for dtype Int64",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m                 new_values = self.values.fillna(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_setitem_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36m_validate_setitem_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m#  py38 builds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid value '{str(value)}' for dtype {self.dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid value '__NAN_CURRENT__' for dtype Int64",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3362172601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'listings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1051611766.py\u001b[0m in \u001b[0;36mcompare_df\u001b[0;34m(df_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# This ensures NaNs introduced by align are treated as different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0maligned_old_common_filled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maligned_old_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__NAN_OLD__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0maligned_current_common_filled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maligned_current_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__NAN_CURRENT__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mchange_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maligned_old_common_filled\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maligned_current_common_filled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   7432\u001b[0m                     \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7433\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7434\u001b[0;31m                     new_data = self._mgr.fillna(\n\u001b[0m\u001b[1;32m   7435\u001b[0m                         \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7436\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         return self.apply_with_block(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;34m\"fillna\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   2332\u001b[0m                 \u001b[0;31m# 3rd party EA that has not implemented copy keyword yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m                 \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2334\u001b[0;31m                 \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2335\u001b[0m                 \u001b[0;31m# issue the warning *after* retrying, in case the TypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m                 \u001b[0;31m#  was caused by an invalid fill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_setitem_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36m_validate_setitem_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Note: without the \"str\" here, the f-string rendering raises in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m#  py38 builds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid value '{str(value)}' for dtype {self.dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid value '__NAN_CURRENT__' for dtype Int64"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917fdec4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because it attempted to fill `NaN` values in nullable integer columns (`Int64`) with string placeholders, causing a `TypeError`. To fix this, I will modify the `compare_df` function to use a NaN-safe comparison method that does not involve filling numeric columns with non-numeric values. Specifically, I will align the common columns of both DataFrames and then use a combination of `ne()` (not equal) and `isna()` (is NaN) to accurately identify changes, treating `NaN` values as equal if they appear in the same position in both DataFrames.\n",
        "\n"
      ],
      "id": "917fdec4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81f9c6b3"
      },
      "source": [
        "def compare_df(df_name):\n",
        "    # Load the 'old' DataFrame from the drive\n",
        "    old_df = pd.read_csv(f\"/content/drive/Shareddrives/market_analysis_v2/{df_name}.csv\")\n",
        "\n",
        "    # Determine the current_df based on df_name\n",
        "    if df_name == 'listings':\n",
        "        current_df = listings\n",
        "    elif df_name == 'allocations':\n",
        "        current_df = allocations\n",
        "    elif df_name == 'notes':\n",
        "        current_df = notes\n",
        "    else:\n",
        "        raise ValueError(f\"df_name '{df_name}' not supported for comparison.\")\n",
        "\n",
        "    # Set 'href' as the index for both DataFrames\n",
        "    old_df_indexed = old_df.set_index(\"href\")\n",
        "    current_df_indexed = current_df.set_index(\"href\")\n",
        "\n",
        "    # Identify common 'href' values\n",
        "    common_idx = old_df_indexed.index.intersection(current_df_indexed.index)\n",
        "\n",
        "    # Create common DataFrames based on common hrefs\n",
        "    old_common = old_df_indexed.loc[common_idx]\n",
        "    current_common = current_df_indexed.loc[common_idx]\n",
        "\n",
        "    # Calculate new and removed rows\n",
        "    n_new_rows = len(current_df_indexed.index.difference(old_df_indexed.index))\n",
        "    n_removed_rows = len(old_df_indexed.index.difference(current_df_indexed.index))\n",
        "\n",
        "    # Get the intersection of columns for a clean comparison\n",
        "    cols_to_compare = list(set(old_common.columns) & set(current_common.columns))\n",
        "\n",
        "    old_comp = old_common[cols_to_compare]\n",
        "    current_comp = current_common[cols_to_compare]\n",
        "\n",
        "    # Convert 'date_scraped' to datetime for both before comparison to avoid dtype issues\n",
        "    if 'date_scraped' in cols_to_compare:\n",
        "        old_comp['date_scraped'] = pd.to_datetime(old_comp['date_scraped'], errors='coerce')\n",
        "        current_comp['date_scraped'] = pd.to_datetime(current_comp['date_scraped'], errors='coerce')\n",
        "\n",
        "    # Define change mask (NaN-safe):\n",
        "    # A change is identified if values are different, OR if one is NaN and the other is not.\n",
        "    # NaNs in the same position in both dataframes are NOT considered a change.\n",
        "    change_mask = (old_comp.ne(current_comp) & ~(old_comp.isna() & current_comp.isna()))\n",
        "\n",
        "    # Calculate changed rows (any column)\n",
        "    n_changed_rows = change_mask.any(axis=1).sum()\n",
        "\n",
        "    # Calculate row changes per column\n",
        "    row_changes_per_column = change_mask.sum()\n",
        "\n",
        "    print(f\"Loaded old '{df_name}' DataFrame with {len(old_df)} rows.\")\n",
        "    print(f\"The global '{df_name}' DataFrame (current state) has {len(current_df)} rows.\")\n",
        "    print(f\"Number of new rows: {n_new_rows}\")\n",
        "    print(f\"Number of removed rows: {n_removed_rows}\")\n",
        "    print(f\"Number of changed rows: {n_changed_rows}\")\n",
        "    print(f\"Row changes per column:\\n{row_changes_per_column}\")"
      ],
      "id": "81f9c6b3",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1874aa99"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `compare_df` function has been fixed to handle NaN values safely, I will call it with `df_name='listings'` to demonstrate its complete functionality and verify the comparison results, including new rows, removed rows, changed rows, and changes per column for the 'listings' DataFrame.\n",
        "\n"
      ],
      "id": "1874aa99"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ad46bc1",
        "outputId": "c18237c7-ee5e-42fe-f6b5-eb8fe71ab90f"
      },
      "source": [
        "compare_df('listings')"
      ],
      "id": "2ad46bc1",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded old 'listings' DataFrame with 868 rows.\n",
            "The global 'listings' DataFrame (current state) has 1097 rows.\n",
            "Number of new rows: 230\n",
            "Number of removed rows: 1\n",
            "Number of changed rows: 27\n",
            "Row changes per column:\n",
            "gen              0\n",
            "model_gen        1\n",
            "seller_type      0\n",
            "odometer         1\n",
            "location         1\n",
            "date_scraped    27\n",
            "age              1\n",
            "model            1\n",
            "make             1\n",
            "year             1\n",
            "seller           0\n",
            "trim             1\n",
            "listed_price    27\n",
            "dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f111643"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the `compare_df` function is correctly defined and ready for use, summarizing its capabilities.\n"
      ],
      "id": "2f111643"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5feb2187"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The `compare_df` function is correctly defined and ready for use. Its capabilities include:\n",
        "1.  Loading an 'old' DataFrame from a specified CSV file.\n",
        "2.  Dynamically selecting a 'current' global DataFrame (e.g., `listings`, `allocations`, `notes`) based on the input `df_name`.\n",
        "3.  Comparing the 'old' and 'current' DataFrames using 'href' as the unique identifier.\n",
        "4.  Identifying and counting new rows, removed rows, and changed rows.\n",
        "5.  Providing a detailed breakdown of the number of changes per column for rows that exist in both DataFrames.\n",
        "6.  Handling potential `NaN` values robustly during comparison.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `compare_df` function successfully identifies data changes between two versions of a DataFrame, exemplified by the comparison of 'listings' DataFrames.\n",
        "*   For the 'listings' DataFrame comparison:\n",
        "    *   230 new rows were identified.\n",
        "    *   1 row was removed.\n",
        "    *   27 rows had changes in at least one column.\n",
        "*   Column-level change analysis revealed that `date_scraped` and `listed_price` each accounted for 27 changes, while columns like `model_gen`, `odometer`, `location`, `age`, `model`, `make`, `year`, and `trim` each had 1 change. Some columns (`gen`, `seller_type`, `seller`) showed no changes.\n",
        "*   A `TypeError` (Invalid value '__NAN\\_CURRENT__' for dtype Int64) was encountered and successfully resolved by refining the NaN-safe comparison logic and ensuring consistent data types (e.g., converting `date_scraped` to datetime) across DataFrames before comparison.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The `compare_df` function is a versatile tool for monitoring data evolution and can be crucial for data quality checks or tracking changes in scraped data over time.\n",
        "*   To enhance usability, consider modifying the function to return the actual changed, new, or removed rows/columns as DataFrames, rather than just printing summary statistics, for deeper investigation or integration into automated workflows.\n"
      ],
      "id": "5feb2187"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}