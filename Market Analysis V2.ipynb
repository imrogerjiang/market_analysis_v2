{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "1DYdMWKeEnLW"
      },
      "id": "1DYdMWKeEnLW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compare_dfs"
      ],
      "metadata": {
        "id": "Ke3ctgS_-wt8"
      },
      "id": "Ke3ctgS_-wt8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_df"
      ],
      "metadata": {
        "id": "Icb9MU9nNtv-"
      },
      "id": "Icb9MU9nNtv-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afb574cc"
      },
      "source": [
        "def save_df(df: pd.DataFrame, base_path: str, filename: str):\n",
        "    \"\"\"\n",
        "    Overwrites a file in the base_path and creates a timestamped copy in an 'archive' subdirectory.\n",
        "    If the archive file for the current day already exists, it will not be overwritten.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        base_path (str): The base directory where the file will be saved and archived.\n",
        "        filename (str): The name of the file (e.g., 'gen_lookup.csv').\n",
        "    \"\"\"\n",
        "    include_index_map = {\n",
        "      \"listings.csv\": False,\n",
        "      \"gen_lookup.csv\": False,\n",
        "      \"notes.csv\": True,\n",
        "      \"allocations.csv\": True,\n",
        "    }\n",
        "    # Determine if index should be included for the current filename\n",
        "    include_idx = include_index_map.get(filename, False)\n",
        "\n",
        "    # Construct the full path for the original file\n",
        "    original_filepath = os.path.join(base_path, filename)\n",
        "\n",
        "    # Save (overwrite) the original file\n",
        "    df.to_csv(original_filepath, index=include_idx)\n",
        "    print(f\"Overwrote: {original_filepath}\")\n",
        "\n",
        "    # Create the archive directory path\n",
        "    archive_dir = os.path.join(base_path, 'archive')\n",
        "    os.makedirs(archive_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the archive filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d')\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    archive_filename = f\"{name}_{timestamp}{ext}\"\n",
        "    archive_filepath = os.path.join(archive_dir, archive_filename)\n",
        "\n",
        "    # Check if the archive file already exists before saving\n",
        "    if not os.path.exists(archive_filepath):\n",
        "        # Save the archived file\n",
        "        df.to_csv(archive_filepath, index=include_idx)\n",
        "        print(f\"Archived to: {archive_filepath}\")\n",
        "    else:\n",
        "        print(f\"Archive file already exists for today: {archive_filepath}. Skipping archive save.\")\n",
        "\n",
        "# Example usage:\n",
        "# base_directory = \"/content/drive/Shareddrives/market_analysis_v2/\"\n",
        "# save_df(gen_lookup, base_directory, \"gen_lookup.csv\")"
      ],
      "id": "afb574cc",
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## const and helpers"
      ],
      "metadata": {
        "id": "3l2VZo-bNrKq"
      },
      "id": "3l2VZo-bNrKq"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "# --- Carsales/General Scrapes (CS) Constants ---\n",
        "YEAR_MIN, YEAR_MAX = 1980, 2035\n",
        "ORDER: List[str] = ['href', 'year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']\n",
        "\n",
        "YEAR_RE  = r'\\b(19[89]\\d|20[0-3]\\d)\\b'\n",
        "PRICE_RE = r'^(?:AU\\$|\\$)\\s*[\\d,]+(?:\\.\\d{2})?\\b' # Made currency symbol mandatory\n",
        "ODOM_RE  = r'^\\s*\\d+(?:,?\\d{3})*K?\\s*km\\s*$' # Added optional 'K' for Facebook odometer format\n",
        "URL_RE   = r'^(?:https?://|www\\.)'\n",
        "TX, SELLER = {'automatic', 'manual'}, {'private', 'dealer used'}\n",
        "\n",
        "THRESH: Dict[str, float] = {\n",
        "    'year_make_model': 0.50,\n",
        "    \"listed_price\":           0.60,\n",
        "    'transmission':    0.80,\n",
        "    'odometer':        0.60,\n",
        "    'seller_type':     0.70,\n",
        "}\n",
        "\n",
        "# --- Facebook Marketplace (FB) Constants ---\n",
        "FB_ORDER: List[str] = ['href', 'year_make_model', 'listed_price', 'odometer', 'location']\n",
        "THRESH_FB: Dict[str, float] = {\n",
        "    'href':            0.80,\n",
        "    'year_make_model': 0.50,\n",
        "    'listed_price':    0.60,\n",
        "    'odometer':        0.60,\n",
        "    'location':        0.40,\n",
        "}\n",
        "\n",
        "# --- Predicates (Validation Rules) ---\n",
        "def _ratio(mask: pd.Series) -> float:\n",
        "    return float(mask.mean()) if len(mask) else 0.0\n",
        "\n",
        "def _yr_ok(s: pd.Series) -> pd.Series:\n",
        "    years = pd.to_numeric(s.astype(str).str.extract(YEAR_RE, expand=False), errors='coerce')\n",
        "    return years.between(YEAR_MIN, YEAR_MAX)\n",
        "\n",
        "PRED = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    \"listed_price\":           lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'transmission':    lambda s: s.astype(str).str.strip().str.lower().isin(TX),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "    'seller_type':     lambda s: s.astype(str).str.strip().str.lower().isin(SELLER),\n",
        "}\n",
        "\n",
        "PRED_FB = {\n",
        "    'year_make_model': lambda s: s.astype(str).pipe(_yr_ok) & s.astype(str).str.contains(r'[A-Za-z]', na=False),\n",
        "    'listed_price':    lambda s: s.astype(str).str.match(PRICE_RE, na=False),\n",
        "    'odometer':        lambda s: s.astype(str).str.match(ODOM_RE, flags=re.I, na=False),\n",
        "}\n",
        "\n",
        "# --- Core Identification Functions ---\n",
        "def identify_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Carsales/General columns.\"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in ORDER}\n",
        "\n",
        "    href_col = cols[0]\n",
        "\n",
        "    # Exclude URL-like columns from other detection logic\n",
        "    url_ratio = {c: _ratio(df[c].astype(str).str.contains(URL_RE, case=False, na=False)) for c in cols}\n",
        "    urlish = {c for c, r in url_ratio.items() if r >= 0.50}\n",
        "    blocked = {href_col} | urlish\n",
        "\n",
        "    remaining = [c for c in cols if c not in blocked]\n",
        "    picks = {t: None for t in PRED}\n",
        "\n",
        "    for t in PRED:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED[t](df[c])) for c in remaining}\n",
        "        best_col, best_score = max(scores.items(), key=lambda kv: kv[1])\n",
        "        if best_score >= THRESH[t]:\n",
        "            picks[t] = best_col\n",
        "            remaining.remove(best_col)\n",
        "\n",
        "    trim_col = None\n",
        "    ymm = picks.get('year_make_model')\n",
        "    if ymm in cols:\n",
        "        i = cols.index(ymm)\n",
        "        if i + 1 < len(cols):\n",
        "            trim_col = cols[i + 1]\n",
        "\n",
        "    return {'href': href_col, **picks, 'trim': trim_col}\n",
        "\n",
        "def identify_fb_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"Identifies and maps raw DataFrame columns to canonical Facebook Marketplace columns.\n",
        "    Note: 'href' is assumed to be the first column and is handled by clean_fb directly.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    if not cols:\n",
        "        return {k: None for k in FB_ORDER}\n",
        "\n",
        "    picks = {t: None for t in FB_ORDER}\n",
        "    remaining = set(cols)\n",
        "\n",
        "    # 'href' is now handled externally by clean_fb and is assumed to be the first column\n",
        "    # So we set it to None here or simply don't try to identify it.\n",
        "    # We explicitly remove the first column from 'remaining' as it's the href\n",
        "    if cols and cols[0] in remaining:\n",
        "        remaining.remove(cols[0])\n",
        "    picks['href'] = None # No longer identified by this function\n",
        "\n",
        "    # Identify 'year_make_model', 'listed_price', 'odometer'\n",
        "    for t in ['year_make_model', 'listed_price', 'odometer']:\n",
        "        if not remaining:\n",
        "            break\n",
        "        scores = {c: _ratio(PRED_FB[t](df[c])) for c in remaining}\n",
        "        if scores:\n",
        "            best_col, score = max(scores.items(), key=lambda kv: kv[1])\n",
        "            if score >= THRESH_FB[t]:\n",
        "                picks[t] = best_col\n",
        "                remaining.remove(best_col)\n",
        "\n",
        "    # Assign 'location', often found in column 'c' or as the last remaining column\n",
        "    if picks['location'] is None:\n",
        "        if 'c' in remaining:\n",
        "            picks['location'] = 'c'\n",
        "            remaining.remove('c')\n",
        "        elif len(remaining) == 1:\n",
        "            picks['location'] = remaining.pop()\n",
        "\n",
        "    return picks"
      ],
      "metadata": {
        "id": "gECV1vdedUm0"
      },
      "id": "gECV1vdedUm0",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_cs"
      ],
      "metadata": {
        "id": "AUnP9GT2Nn7C"
      },
      "id": "AUnP9GT2Nn7C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5c70d3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_cs(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_cs function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Carsales/General web scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', etc.) using `identify_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Transforms 'odometer' values from 'km' to '000 km' (e.g., 180,000 km -> 180).\n",
        "    4.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_carsales_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_columns(df)\n",
        "    for col in ['year_make_model', 'trim', \"listed_price\", 'transmission', 'odometer', 'seller_type']:\n",
        "        src = mapping.get(col)\n",
        "        if src is not None and src != out['href'].name:\n",
        "            out[col] = df[src]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_cols = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_cols.columns:\n",
        "            out['year'] = pd.to_numeric(\n",
        "                split_cols[0].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_cols[1] if 1 in split_cols.columns else pd.NA\n",
        "        out['model'] = split_cols[2] if 2 in split_cols.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        # Revert: Remove .str.lower(), keep http(s)://www. prefixes removal\n",
        "        out['href'] = out['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True).str.split('?').str[0]\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^\\d]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    if 'odometer' in out.columns:\n",
        "        out['odometer'] = out['odometer'] // 1000\n",
        "\n",
        "    final_cols = ['href', 'year', 'make', 'model', \"listed_price\", 'trim', 'odometer', 'seller_type']\n",
        "    if save_raw:\n",
        "        final_cols.insert(0, 'raw')\n",
        "    return out[[c for c in final_cols if c in out.columns]]"
      ],
      "execution_count": 130,
      "outputs": [],
      "id": "aa5c70d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean_fb"
      ],
      "metadata": {
        "id": "hfCSFOAWNlyg"
      },
      "id": "hfCSFOAWNlyg"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def clean_fb(df: pd.DataFrame, save_raw: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Business Logic for clean_fb function:\n",
        "\n",
        "    This function processes raw DataFrame outputs from Facebook Marketplace scrapes to standardize\n",
        "    and clean vehicle listing data into a consistent format for analysis.\n",
        "\n",
        "    Key steps and business rules:\n",
        "    1.  **Raw Data Preservation (Optional):** If `save_raw` is True, the original DataFrame\n",
        "        is saved to a timestamped CSV, and a 'raw' column (filename) is added to the output.\n",
        "    2.  **Column Identification:** Dynamically maps raw DataFrame columns to canonical names\n",
        "        ('href', 'year_make_model', 'listed_price', 'odometer', 'location') using `identify_fb_columns`.\n",
        "    3.  **Data Extraction & Standardization:**\n",
        "        *   Cleans 'href' by removing query parameters and `http(s)://www.` prefix.\n",
        "        *   Splits 'year_make_model' into 'year', 'make', and 'model'; converts 'year' to integer.\n",
        "        *   Converts 'listed_price' and 'odometer' to integer, removing non-numeric characters.\n",
        "        *   Filters out listings with 'listed_price' explicitly marked as \"free\".\n",
        "    4.  **Data Quality Filtering:** Drops rows with missing (`pd.NA`) values in critical columns\n",
        "        ('listed_price', 'odometer', 'year') to ensure data integrity. Also removes listings\n",
        "        with a placeholder 'listed_price' of 12345.\n",
        "    5.  **Output Structure:** Returns a DataFrame with a standardized set of columns for consistency.\n",
        "    \"\"\"\n",
        "    raw_col_value = None\n",
        "    if save_raw:\n",
        "        raw_data_dir = 'data/raws'\n",
        "        os.makedirs(raw_data_dir, exist_ok=True)\n",
        "        timestamp = datetime.now()\n",
        "        raw_filename = ''\n",
        "        while True:\n",
        "            raw_filename = os.path.join(raw_data_dir, f\"raw_facebook_data_{timestamp.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "            if not os.path.exists(raw_filename):\n",
        "                break\n",
        "            timestamp += timedelta(seconds=1)\n",
        "        df.to_csv(raw_filename, index=False)\n",
        "        raw_col_value = os.path.basename(raw_filename)\n",
        "\n",
        "    if 'identify_fb_columns' not in globals():\n",
        "        raise NameError(\"Function 'identify_fb_columns' not found. Please ensure 'constants_and_helpers.py' or cell 'gECV1vdedUm0' has been executed.\")\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    if not df.empty and len(df.columns) > 0:\n",
        "        out['href'] = df.iloc[:, 0]\n",
        "\n",
        "    mapping = identify_fb_columns(df)\n",
        "    for canonical_col, src_col in mapping.items():\n",
        "        if canonical_col != 'href' and src_col is not None and src_col in df.columns:\n",
        "            out[canonical_col] = df[src_col]\n",
        "\n",
        "    if save_raw and raw_col_value:\n",
        "        out['raw'] = raw_col_value\n",
        "\n",
        "    if 'year_make_model' in out.columns:\n",
        "        split_df = out['year_make_model'].astype(str).str.split(expand=True, n=2)\n",
        "        if 0 in split_df.columns:\n",
        "            out['year'] = split_df[0].astype(str).str.replace(r'[^0-9]', '', regex=True).replace('', pd.NA).astype(float).astype('Int64')\n",
        "        else:\n",
        "            out['year'] = pd.NA\n",
        "        out['make'] = split_df[1] if 1 in split_df.columns else pd.NA\n",
        "        out['model'] = split_df[2] if 2 in split_df.columns else pd.NA\n",
        "    else:\n",
        "        out[['year', 'make', 'model']] = pd.NA\n",
        "\n",
        "    if 'href' in out.columns:\n",
        "        # Revert: Remove .str.lower(), keep http(s)://www. prefixes removal\n",
        "        out['href'] = out['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True).str.split('?').str[0]\n",
        "\n",
        "    for col in [\"listed_price\", 'odometer']:\n",
        "        if col in out.columns:\n",
        "            if col == 'listed_price':\n",
        "                out = out[out[col].astype(str).str.lower() != \"free\"]\n",
        "            out[col] = pd.to_numeric(\n",
        "                out[col].astype(str).str.replace(r'[^0-9]', '', regex=True),\n",
        "                errors='coerce'\n",
        "            ).astype('Int64')\n",
        "\n",
        "    cols_to_check_for_na = []\n",
        "    if 'listed_price' in out.columns: cols_to_check_for_na.append('listed_price')\n",
        "    if 'odometer' in out.columns: cols_to_check_for_na.append('odometer')\n",
        "    if 'year' in out.columns: cols_to_check_for_na.append('year')\n",
        "\n",
        "    if cols_to_check_for_na:\n",
        "        out = out.dropna(subset=cols_to_check_for_na)\n",
        "\n",
        "    final_columns = ['href', 'year', 'make', 'model', \"listed_price\", 'odometer', 'location']\n",
        "    if save_raw:\n",
        "        final_columns.insert(0, 'raw')\n",
        "    return out[[c for c in final_columns if c in out.columns]]"
      ],
      "metadata": {
        "id": "N5MMcCRVEbDl"
      },
      "id": "N5MMcCRVEbDl",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enrich_df"
      ],
      "metadata": {
        "id": "XXQnhOXJNj7S"
      },
      "id": "XXQnhOXJNj7S"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "def enrich_df(df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Final clean after clean_cs or clean_fb, including generation assignment.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to enrich.\n",
        "        gen_lookup (pd.DataFrame): A lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The enriched DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Add/Update date_scraped ---\n",
        "    current_timestamp = pd.Timestamp.now().normalize()\n",
        "    if 'date_scraped' not in df.columns:\n",
        "        # Initialize as datetime type with NaT values if column doesn't exist\n",
        "        df['date_scraped'] = pd.Series(pd.NaT, index=df.index, dtype='datetime64[ns]')\n",
        "    else:\n",
        "        # Ensure it's datetime type, coercing errors if it exists but isn't datetime\n",
        "        df['date_scraped'] = pd.to_datetime(df['date_scraped'], errors='coerce')\n",
        "\n",
        "    # Now fill NaT values with current_timestamp\n",
        "    df['date_scraped'] = df['date_scraped'].fillna(current_timestamp)\n",
        "\n",
        "    # --- 2. Normalise make & model ---\n",
        "    for col in [\"make\", \"model\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .astype(str)\n",
        "                .str.lower()\n",
        "                .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
        "            )\n",
        "\n",
        "    # --- Remove 'https://' or 'http://' and 'www.' from href ---\n",
        "    if 'href' in df.columns:\n",
        "        df['href'] = df['href'].astype(str).str.replace(r'^(https?://)?(www\\.)?', '', regex=True)\n",
        "\n",
        "    # --- 3. Ensure year is numeric ---\n",
        "    if \"year\" in df.columns:\n",
        "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # --- 4. Calculate age ---\n",
        "    if 'year' in df.columns:\n",
        "        df['age'] = 2026 - df['year']\n",
        "\n",
        "    # --- 5. Assign generation manually (no merge, no year_start/year_end contamination) ---\n",
        "    df[\"gen\"] = pd.NA\n",
        "\n",
        "    for idx, row in gen_lookup.iterrows():\n",
        "        mask = (\n",
        "            (df[\"make\"] == row[\"make\"]) &\n",
        "            (df[\"model\"] == row[\"model\"]) &\n",
        "            (df[\"year\"].between(row[\"year_start\"], row[\"year_end\"], inclusive=\"both\"))\n",
        "        )\n",
        "        df.loc[mask, \"gen\"] = row[\"gen\"]\n",
        "\n",
        "    df[\"gen\"] = df[\"gen\"].astype(\"Int64\")\n",
        "\n",
        "    # --- 6. Create model_gen ---\n",
        "    df[\"model_gen\"] = df.apply(\n",
        "        lambda r: f\"{r['model']}_{r['gen']}\" if pd.notna(r[\"gen\"]) else None,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "9EoYNNjuIakO"
      },
      "id": "9EoYNNjuIakO",
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_bad_listings"
      ],
      "metadata": {
        "id": "ifMZe8Y4NhWe"
      },
      "id": "ifMZe8Y4NhWe"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_bad_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies filters to remove bad or undesirable listings from the DataFrame.\n",
        "    This function is intended to be called after initial cleaning and data type conversions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to filter, expected to have 'year', 'listed_price', and 'odometer' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Price filters as specified by the user\n",
        "    if 'listed_price' in df_filtered.columns:\n",
        "        # Ensure listed_price is numeric for comparison\n",
        "        df_filtered['listed_price'] = pd.to_numeric(df_filtered['listed_price'], errors='coerce')\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] != 12345]\n",
        "        df_filtered = df_filtered[df_filtered[\"listed_price\"] > 3000]\n",
        "\n",
        "    # Calculate age temporarily for the odometer filter if 'year' is available\n",
        "    # Assuming 2026 is the reference year for age calculation based on other parts of the notebook\n",
        "    if 'year' in df_filtered.columns:\n",
        "        df_filtered['year'] = pd.to_numeric(df_filtered['year'], errors='coerce') # Ensure year is numeric\n",
        "        temp_age = 2026 - df_filtered['year']\n",
        "    else:\n",
        "        temp_age = pd.Series(pd.NA, index=df_filtered.index) # Create a Series of NA for consistent operations\n",
        "\n",
        "    # Odometer filter: odometer > 2 * age\n",
        "    if 'odometer' in df_filtered.columns:\n",
        "        # Ensure odometer is numeric\n",
        "        df_filtered['odometer'] = pd.to_numeric(df_filtered['odometer'], errors='coerce')\n",
        "\n",
        "        # Create a mask for rows where both odometer and temp_age are valid for comparison\n",
        "        mask_valid_comparison = df_filtered['odometer'].notna() & temp_age.notna()\n",
        "\n",
        "        # Filter out rows where (odometer is NOT > 2 * age) AND (the comparison is valid)\n",
        "        # We keep rows where (odometer > 2 * age) OR (the comparison cannot be made due to NA values)\n",
        "        df_filtered = df_filtered[~((df_filtered['odometer'] <= 2 * temp_age) & mask_valid_comparison)]\n",
        "\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "PuyPlWYcDWcc"
      },
      "id": "PuyPlWYcDWcc",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare_new_listings"
      ],
      "metadata": {
        "id": "xIoIjpiuNfDH"
      },
      "id": "xIoIjpiuNfDH"
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_new_listings(listings: pd.DataFrame, gen_lookup: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Processes new listing files, cleans, enriches, and compares them against existing listings.\n",
        "\n",
        "    Args:\n",
        "        listings (pd.DataFrame): Existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): Lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, int, int, int, int]: A tuple containing:\n",
        "            - enriched_new_listings (pd.DataFrame): DataFrame of newly processed and enriched listings.\n",
        "            - unq_new (int): Total count of truly new unique listings.\n",
        "            - unq_updated (int): Total count of updated unique listings.\n",
        "            - unq_unchanged (int): Total count of unchanged unique listings.\n",
        "            - unq_tot (int): Total count of all unique listings processed from new files.\n",
        "    \"\"\"\n",
        "    # Sets to track unique hrefs across all processed files\n",
        "    unique_new_hrefs = set()\n",
        "    unique_updated_hrefs = set()\n",
        "    unique_unchanged_hrefs = set()\n",
        "    unique_total_hrefs = set()\n",
        "\n",
        "    enriched_new_listings = pd.DataFrame()\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "\n",
        "    for file_path in cs_files + fb_files:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None and not df_cleaned.empty:\n",
        "            unique_total_hrefs.update(df_cleaned['href'].tolist())\n",
        "\n",
        "        # Checking how many new, updated, unchanged listings\n",
        "        df_comparison = pd.merge(\n",
        "            df_cleaned,\n",
        "            listings,\n",
        "            on='href',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_existing')\n",
        "        )\n",
        "\n",
        "        # Identify new listings\n",
        "        new_listings_df = df_comparison[df_comparison['listed_price_existing'].isnull()]\n",
        "        n_new = len(new_listings_df)\n",
        "        if not new_listings_df.empty:\n",
        "            unique_new_hrefs.update(new_listings_df['href'].tolist())\n",
        "\n",
        "        # Identify matched listings\n",
        "        matched_listings_df = df_comparison[df_comparison['listed_price_existing'].notnull()]\n",
        "\n",
        "        # From matched_listings, identify updated listings\n",
        "        updated_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] != matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_updated = len(updated_listings_df)\n",
        "        if not updated_listings_df.empty:\n",
        "            unique_updated_hrefs.update(updated_listings_df['href'].tolist())\n",
        "\n",
        "        # From matched_listings, identify unchanged listings\n",
        "        unchanged_listings_df = matched_listings_df[\n",
        "            matched_listings_df['listed_price_new'] == matched_listings_df['listed_price_existing']\n",
        "        ]\n",
        "        n_unchanged = len(unchanged_listings_df)\n",
        "        if not unchanged_listings_df.empty:\n",
        "            unique_unchanged_hrefs.update(unchanged_listings_df['href'].tolist())\n",
        "\n",
        "        # Calculate total listings for the current file\n",
        "        n_total_listings = len(df_cleaned)\n",
        "\n",
        "        # Print the comparison result for the current file\n",
        "        print(f\"{file_path}    \\t {n_new=}   \\t {n_updated=} \\t {n_unchanged=} \\t Tot {n_total_listings}\")\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            enriched_new_listings = pd.concat([enriched_new_listings, df_enriched], ignore_index=True)\n",
        "\n",
        "    # Calculate unique total counts at the end\n",
        "    unq_new = len(unique_new_hrefs)\n",
        "    unq_updated = len(unique_updated_hrefs)\n",
        "    unq_unchanged = len(unique_unchanged_hrefs)\n",
        "    unq_tot = len(unique_total_hrefs)\n",
        "\n",
        "    print(f\"\\t \\t \\t \\t {unq_new=} \\t {unq_updated=}\\t {unq_unchanged=} {unq_tot=}\")\n",
        "\n",
        "    # Check for missing values in enriched_new_listings after concatenation\n",
        "    if not enriched_new_listings.empty:\n",
        "        for col in ['model_gen', 'age', 'odometer']:\n",
        "            if col in enriched_new_listings.columns and enriched_new_listings[col].isna().any():\n",
        "                missing_count = enriched_new_listings[col].isna().sum()\n",
        "                print(f\"WARNING: Column '{col}' in enriched_new_listings has {missing_count} missing values.\")\n",
        "\n",
        "\n",
        "    return enriched_new_listings"
      ],
      "metadata": {
        "id": "elwfHrt9xAZ9"
      },
      "id": "elwfHrt9xAZ9",
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## integrate_listings"
      ],
      "metadata": {
        "id": "DNtbnWgdNbxT"
      },
      "id": "DNtbnWgdNbxT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23a71e21"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Optional, List\n",
        "import glob # Import glob for file pattern matching\n",
        "\n",
        "def integrate_listings(listings_df: pd.DataFrame, gen_lookup: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Integrates new car listings from '/content/carsales*.csv' and '/content/facebook*.csv' files into an existing listings DataFrame.\n",
        "\n",
        "    Args:\n",
        "        listings_df (pd.DataFrame): The existing DataFrame of car listings.\n",
        "        gen_lookup (pd.DataFrame): The lookup table for car generations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame (`listings_1`) with integrated, cleaned, and enriched listings,\n",
        "                      with existing listings handled by keeping the most recent entry.\n",
        "    \"\"\"\n",
        "    processed_dfs = []\n",
        "\n",
        "    # Dynamically find new CSV files\n",
        "    cs_files = glob.glob('/content/carsales*.csv')\n",
        "    fb_files = glob.glob('/content/facebook*.csv')\n",
        "    new_file_paths = cs_files + fb_files\n",
        "\n",
        "    for file_path in new_file_paths:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        df_cleaned = None\n",
        "\n",
        "        if 'carsales' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_cs(df_raw, save_raw=False)\n",
        "        elif 'facebook' in os.path.basename(file_path):\n",
        "            df_cleaned = clean_fb(df_raw, save_raw=False)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        if df_cleaned is not None:\n",
        "            df_enriched = enrich_df(df_cleaned, gen_lookup)\n",
        "            processed_dfs.append(df_enriched)\n",
        "\n",
        "    if processed_dfs:\n",
        "        new_listings_df = pd.concat(processed_dfs, ignore_index=True)\n",
        "\n",
        "        # Define all possible columns that might exist in either DataFrame\n",
        "        # Get columns from existing listings and new listings, handling potential differences\n",
        "        all_cols = list(set(listings_df.columns) | set(new_listings_df.columns))\n",
        "\n",
        "        # Reindex both DataFrames to ensure they have the same columns\n",
        "        listings_aligned = listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "        new_listings_aligned = new_listings_df.reindex(columns=all_cols, fill_value=pd.NA)\n",
        "\n",
        "        # Ensure 'date_scraped' is in datetime format for proper sorting\n",
        "        listings_aligned['date_scraped'] = pd.to_datetime(listings_aligned['date_scraped'], errors='coerce')\n",
        "        new_listings_aligned['date_scraped'] = pd.to_datetime(new_listings_aligned['date_scraped'], errors='coerce')\n",
        "\n",
        "        # Explicitly cast dtypes of new_listings_aligned to match listings_aligned for common columns\n",
        "        # This helps prevent FutureWarning and ensures consistent types across the concatenated DataFrame\n",
        "        for col in all_cols:\n",
        "            if col in listings_aligned.columns and col in new_listings_aligned.columns:\n",
        "                if listings_aligned[col].dtype != new_listings_aligned[col].dtype:\n",
        "                    try:\n",
        "                        if pd.api.types.is_numeric_dtype(listings_aligned[col]):\n",
        "                            if str(listings_aligned[col].dtype) == 'Int64':\n",
        "                                new_listings_aligned[col] = new_listings_aligned[col].astype('Int64')\n",
        "                            else:\n",
        "                                new_listings_aligned[col] = pd.to_numeric(new_listings_aligned[col], errors='coerce').astype(listings_aligned[col].dtype)\n",
        "                        else:\n",
        "                            new_listings_aligned[col] = new_listings_aligned[col].astype(listings_aligned[col].dtype)\n",
        "                    except (TypeError, ValueError):\n",
        "                        pass # Keep original dtype if casting causes error\n",
        "\n",
        "        # Concatenate the aligned Dataframes\n",
        "        listings_1 = pd.concat([listings_aligned, new_listings_aligned], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No new listings\")\n",
        "        return listings_df # Return the original listings_df if no new listings were processed\n",
        "\n",
        "\n",
        "    # Sort by href, then listed_price (lowest first), then date_scraped (most recent first), then drop duplicates keeping the first\n",
        "    listings_1 = listings_1.sort_values(by=['href', 'listed_price', 'date_scraped'], ascending=[True, True, True])\n",
        "    listings_1 = listings_1.drop_duplicates(subset=['href'], keep='first')\n",
        "    listings_1 = remove_bad_listings(listings_1)\n",
        "\n",
        "    # Ensure 'gen' column is Int64 after all operations\n",
        "    listings_1['gen'] = listings_1['gen'].astype('Int64')\n",
        "\n",
        "    print(f\"Final DataFrame has {len(listings_1)} unique listings after merging and de-duplication.\")\n",
        "    return listings_1"
      ],
      "id": "23a71e21",
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## allocate_listings"
      ],
      "metadata": {
        "id": "SUj2yn4nNTDB"
      },
      "id": "SUj2yn4nNTDB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import date\n",
        "from typing import Optional, List\n",
        "\n",
        "def allocate_listings(listings_lr: pd.DataFrame, notes: pd.DataFrame, allocations: pd.DataFrame, clients_to_process: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Allocates car listings based on universal standards, client-specific criteria, and notes/allocation history.\n",
        "\n",
        "    Args:\n",
        "        listings_lr (pd.DataFrame): The DataFrame of car listings with regression results (market_value, excess_value).\n",
        "        notes (pd.DataFrame): DataFrame containing historical notes and statuses for listings.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        clients_to_process (Optional[List[str]]): List of client names to process. If None, all global clients are processed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An updated allocations DataFrame containing newly proposed allocations.\n",
        "    \"\"\"\n",
        "\n",
        "    global clients # Access the global list of client configuration dictionaries\n",
        "\n",
        "    # Determine which clients to actually process\n",
        "    effective_clients_info = []\n",
        "    if clients_to_process is None:\n",
        "        effective_clients_info = clients # Process all clients\n",
        "    else:\n",
        "        # Filter global clients to get the dictionaries for specified client names\n",
        "        effective_clients_info = [c_info for c_info in clients if c_info['client'] in clients_to_process]\n",
        "\n",
        "    if not effective_clients_info:\n",
        "        print(\"No clients specified or found to process for allocations.\")\n",
        "        return allocations\n",
        "\n",
        "    # Make copies to avoid modifying original DataFrames\n",
        "    listings_filtered = listings_lr.copy()\n",
        "    notes_filtered = notes.copy()\n",
        "    current_allocations = allocations.copy()\n",
        "\n",
        "    # 1. Apply Universal Filters\n",
        "    listings_filtered = listings_filtered[\n",
        "        (listings_filtered['odometer'] > 4 * listings_filtered['age']) &\n",
        "        (listings_filtered['listed_price'] < 0.95 * listings_filtered['market_value'])\n",
        "    ]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after universal filters.\")\n",
        "        return allocations\n",
        "\n",
        "    # 2. Apply Date Filter (most recent listings)\n",
        "    # Ensure 'date_scraped' is datetime for comparison\n",
        "    listings_filtered['date_scraped'] = pd.to_datetime(listings_filtered['date_scraped'], errors='coerce')\n",
        "    most_recent_date = listings_filtered['date_scraped'].max()\n",
        "    listings_filtered = listings_filtered[listings_filtered['date_scraped'].dt.date == most_recent_date.date()]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after date filtering.\")\n",
        "        return allocations\n",
        "\n",
        "    # 3. Filter out listings based on 'notes' status\n",
        "    # Convert notes timestamp to datetime for proper sorting\n",
        "    notes_filtered['timestamp'] = pd.to_datetime(notes_filtered['timestamp'], errors='coerce')\n",
        "\n",
        "    # Get the most recent status for each href\n",
        "    latest_notes = notes_filtered.sort_values(by='timestamp', ascending=False).drop_duplicates(subset=['href'], keep='first')\n",
        "\n",
        "    # Identify hrefs that are 'sold', 'rejected', or 'allocated'\n",
        "    excluded_hrefs_from_notes = latest_notes[\n",
        "        latest_notes['status'].isin(['sold', 'rejected', 'allocated'])\n",
        "    ]['href'].unique()\n",
        "\n",
        "    # Filter listings_filtered to remove these excluded hrefs\n",
        "    listings_filtered = listings_filtered[~listings_filtered['href'].isin(excluded_hrefs_from_notes)]\n",
        "\n",
        "    if listings_filtered.empty:\n",
        "        print(\"No listings remain after notes status filtering.\")\n",
        "        return allocations\n",
        "\n",
        "    # Ensure 'excess_value' is present for sorting\n",
        "    if 'excess_value' not in listings_filtered.columns:\n",
        "        print(\"Error: 'excess_value' column is missing for sorting.\")\n",
        "        return allocations\n",
        "\n",
        "    new_allocation_records = []\n",
        "    current_timestamp = pd.Timestamp.now()\n",
        "\n",
        "    # 4. Iterate through each specified client for allocations\n",
        "    for client_info in effective_clients_info:\n",
        "        current_client_name = client_info['client']\n",
        "        max_price = client_info['max_listing_price']\n",
        "        max_odometer = client_info['max_odometer']\n",
        "        model_gens_allowed = client_info['model_gens']\n",
        "\n",
        "        # Client-specific criteria\n",
        "        price_cond = listings_filtered['listed_price'] <= max_price\n",
        "        odometer_cond = listings_filtered['odometer'] <= max_odometer\n",
        "\n",
        "        # Model generation condition (using str.startswith for broader matching)\n",
        "        model_gen_cond = pd.Series(False, index=listings_filtered.index)\n",
        "        if 'model_gen' in listings_filtered.columns and model_gens_allowed:\n",
        "            for allowed_gen_pattern in model_gens_allowed:\n",
        "                model_gen_cond = model_gen_cond | (\n",
        "                    listings_filtered['model_gen'].astype(str).str.startswith(allowed_gen_pattern)\n",
        "                )\n",
        "\n",
        "        client_eligible_listings = listings_filtered[\n",
        "            price_cond & odometer_cond & model_gen_cond\n",
        "        ].copy()\n",
        "\n",
        "        if not client_eligible_listings.empty:\n",
        "            # 5. Sort by 'excess_value' descending and select top 10 (or all available)\n",
        "            top_listings_for_client = client_eligible_listings.sort_values(by='excess_value', ascending=False).head(10)\n",
        "\n",
        "            for _, listing_row in top_listings_for_client.iterrows():\n",
        "                href = listing_row['href']\n",
        "                # 6. Check if (href, client_name) pair already exists in the existing 'allocations' DataFrame\n",
        "                # This ensures we don't re-allocate already allocated items for this client.\n",
        "                # The final deduplication step also handles this more broadly.\n",
        "                new_allocation_records.append({\n",
        "                    'href': href,\n",
        "                    'client': current_client_name,\n",
        "                    'allocation': True,\n",
        "                    'timestamp': current_timestamp\n",
        "                })\n",
        "\n",
        "    if new_allocation_records:\n",
        "        new_allocations_df = pd.DataFrame(new_allocation_records)\n",
        "        new_allocations_df['timestamp'] = pd.to_datetime(new_allocations_df['timestamp'])\n",
        "        new_allocations_df['allocation'] = new_allocations_df['allocation'].astype('boolean')\n",
        "\n",
        "\n",
        "        # Filter out new allocations that are already present in the existing 'allocations' DataFrame\n",
        "        existing_allocation_keys = allocations[['href', 'client']].drop_duplicates()\n",
        "        merged_df = pd.merge(\n",
        "            new_allocations_df,\n",
        "            existing_allocation_keys,\n",
        "            on=['href', 'client'],\n",
        "            how='left',\n",
        "            indicator=True\n",
        "        )\n",
        "        truly_new_allocations = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "\n",
        "        # 7. Concatenate the truly new allocation records with the existing allocations DataFrame\n",
        "        allocations = pd.concat([allocations, truly_new_allocations], ignore_index=True)\n",
        "        allocations['allocation'] = allocations['allocation'].astype('boolean')\n",
        "        print(f\"Added {len(truly_new_allocations)} new allocation entries.\")\n",
        "    else:\n",
        "        print(\"No new allocations found based on current criteria.\")\n",
        "\n",
        "    return allocations"
      ],
      "metadata": {
        "id": "HmF0gPBJzvkW"
      },
      "id": "HmF0gPBJzvkW",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## write_yaml"
      ],
      "metadata": {
        "id": "rTmF-kcqNN7P"
      },
      "id": "rTmF-kcqNN7P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e025f4f7"
      },
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "from datetime import datetime, date\n",
        "import numpy as np\n",
        "from google.colab import files # Import files for download functionality\n",
        "from typing import List, Tuple\n",
        "\n",
        "def write_yaml(listings_to_print: List[str], listings_lr: pd.DataFrame, allocations: pd.DataFrame, notes_df: pd.DataFrame, out_file: Optional[str] = None, download: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Consolidates listing data from shortlist and notes DataFrames and saves it to a YAML file.\n",
        "    Dynamically adds client eligibility flags from shortlist columns.\n",
        "    If a listing has no status, it adds a 'seen' status with author 'beep_boop'.\n",
        "\n",
        "    Args:\n",
        "        listings_to_print (List[str]): List of hrefs to include in the YAML output.\n",
        "        listings_lr (pd.DataFrame): DataFrame of car listings with regression results.\n",
        "        allocations (pd.DataFrame): DataFrame containing historical allocation decisions.\n",
        "        notes_df (pd.DataFrame): DataFrame containing notes associated with listings.\n",
        "        out_file (Optional[str]): The filename to save the YAML to. If None, defaults to 'shortlist.yaml'.\n",
        "        download (bool): If True, the generated YAML file will be prompted for download.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame, potentially with new 'seen' entries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to convert pandas-specific types to standard Python equivalents\n",
        "    def to_python_type(value):\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "        if isinstance(value, pd.Timestamp):\n",
        "            return value.to_pydatetime() # Convert pandas Timestamp to datetime object\n",
        "        if isinstance(value, (pd.Int64Dtype, np.int64)):\n",
        "            return int(value)\n",
        "        if isinstance(value, (pd.Float64Dtype, np.float64)):\n",
        "            return float(value)\n",
        "        if isinstance(value, (date, datetime)): # Use datetime.date and datetime\n",
        "            return value\n",
        "        return value\n",
        "\n",
        "    # Prepare notes_df (make a copy to ensure any internal modifications are to this copy)\n",
        "    current_notes_df = notes_df.copy()\n",
        "    current_notes_df['timestamp'] = pd.to_datetime(current_notes_df['timestamp'], errors='coerce')\n",
        "    current_notes_df.dropna(subset=['timestamp'], inplace=True)\n",
        "\n",
        "    # Filter listings_lr to include only the listings specified in listings_to_print\n",
        "    # Then sort by 'excess_value' in descending order\n",
        "    current_shortlist = listings_lr[listings_lr['href'].isin(listings_to_print)].copy()\n",
        "    if 'excess_value' in current_shortlist.columns:\n",
        "        current_shortlist = current_shortlist.sort_values(by='excess_value', ascending=False)\n",
        "    else:\n",
        "        print(\"Warning: 'excess_value' column not found, cannot sort by it.\")\n",
        "\n",
        "    all_listings_data = []\n",
        "\n",
        "    for idx, row in current_shortlist.iterrows():\n",
        "        href = row['href']\n",
        "        initial_status_for_listing = None # This will hold the status *before* any 'seen' logic\n",
        "        current_notes_for_listing = [] # Notes associated *before* any 'seen' logic\n",
        "\n",
        "        matching_notes = current_notes_df[current_notes_df['href'] == href]\n",
        "\n",
        "        if not matching_notes.empty:\n",
        "            matching_notes_sorted = matching_notes.sort_values(by='timestamp', ascending=False)\n",
        "            initial_status_for_listing = to_python_type(matching_notes_sorted.iloc[0]['status'])\n",
        "            current_notes_for_listing = [to_python_type(n) for n in matching_notes_sorted['note'].tolist() if pd.notna(n)]\n",
        "\n",
        "        # Create a dictionary named listing_data with the specified order and format\n",
        "        listing_data = {\n",
        "            'title': f\"{to_python_type(row['year'])}, {to_python_type(row['model_gen'])}, {int(to_python_type(row['odometer']))}k\",\n",
        "            'seller': to_python_type(row['seller']), # Modified to use seller\n",
        "            'listed_price': to_python_type(row['listed_price']),\n",
        "            'excess_value': int(to_python_type(row['excess_value'])), # Convert to int here\n",
        "            'href': to_python_type(row['href'])\n",
        "        }\n",
        "\n",
        "        # Dynamically add client eligibility from the allocations DataFrame\n",
        "        eligible_clients = allocations[\n",
        "            (allocations['href'] == href) & (allocations['allocation'] == True)\n",
        "        ]['client'].unique().tolist()\n",
        "        listing_data['clients'] = eligible_clients\n",
        "\n",
        "        # Add status and notes based on initial values\n",
        "        listing_data['status'] = initial_status_for_listing\n",
        "        listing_data['notes'] = current_notes_for_listing\n",
        "\n",
        "        all_listings_data.append(listing_data)\n",
        "\n",
        "        # Now, if the listing had no status, add 'seen' to current_notes_df for the *next* iteration\n",
        "        if initial_status_for_listing is None:\n",
        "            current_notes_df = add_note(current_notes_df, 'beep_boop', href, status='seen')\n",
        "\n",
        "    output_filename = out_file if out_file is not None else 'shortlist.yaml'\n",
        "    # Initialize yaml_content list\n",
        "    yaml_content = []\n",
        "    for listing in all_listings_data:\n",
        "        yaml_content.append('---\\n') # Add separator before each listing\n",
        "        yaml_content.append(yaml.dump(listing, allow_unicode=True, sort_keys=False))\n",
        "        yaml_content.append('\\n') # Add an extra newline after each dumped listing for readability\n",
        "\n",
        "    yaml_content_str = \"\".join(yaml_content)\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.write(yaml_content_str)\n",
        "\n",
        "    if download:\n",
        "        files.download(output_filename)\n",
        "        print(f\"The YAML file '{output_filename}' has been generated and prompted for download with {len(all_listings_data)} listings.\")\n",
        "    else:\n",
        "        print(f\"The YAML file '{output_filename}' has been generated with {len(all_listings_data)} listings (download skipped).\")\n",
        "\n",
        "    return current_notes_df # Return the potentially updated notes_df\n"
      ],
      "id": "e025f4f7",
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apply_regression"
      ],
      "metadata": {
        "id": "1aAGDDUkOKsL"
      },
      "id": "1aAGDDUkOKsL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23f91c4e"
      },
      "source": [
        "def apply_regression(df: pd.DataFrame) -> (pd.DataFrame, pd.Series):\n",
        "    \"\"\"\n",
        "    Applies Huber regression to the input DataFrame to predict car prices.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing car listings.\n",
        "\n",
        "    Returns:\n",
        "        (pd.DataFrame, pd.Series): A tuple containing:\n",
        "            - The DataFrame with 'market_value' and 'excess_value' columns added.\n",
        "            - A Series of unscaled regression coefficients.\n",
        "    \"\"\"\n",
        "    listings_lr = df.copy()\n",
        "\n",
        "    # 1) Coerce numeric types\n",
        "    listings_lr['year'] = pd.to_numeric(listings_lr['year'], errors='coerce')\n",
        "    listings_lr['odometer'] = pd.to_numeric(listings_lr['odometer'], errors='coerce')\n",
        "    listings_lr[\"listed_price\"] = pd.to_numeric(listings_lr[\"listed_price\"], errors='coerce')\n",
        "\n",
        "    # 2) One-hot encode model_gen\n",
        "    listings_lr[\"model_gen\"] = listings_lr[\"model_gen\"].astype(str)\n",
        "    dummies = pd.get_dummies(listings_lr[\"model_gen\"], prefix=\"mg_\", prefix_sep=\"\")\n",
        "\n",
        "    # remove base category \"civic_9\" if it exists\n",
        "    base_col = \"mg_civic_9\" # Corrected base column name to match dummy format\n",
        "    if base_col in dummies.columns:\n",
        "        dummies = dummies.drop(columns=[base_col])\n",
        "\n",
        "    listings_lr = pd.concat([listings_lr, dummies], axis=1)\n",
        "\n",
        "    # 3) Build X, y & keep mask\n",
        "    predictor_cols = ['age', 'odometer'] + list(dummies.columns)\n",
        "    X = listings_lr[predictor_cols].astype(float)\n",
        "    y = listings_lr[\"listed_price\"].astype(float)\n",
        "\n",
        "    keep = X.notna().all(axis=1) & y.notna()\n",
        "\n",
        "    X_keep = X.loc[keep]\n",
        "    y_keep = y.loc[keep]\n",
        "\n",
        "    # 4) Scale predictors\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_keep)\n",
        "\n",
        "    # 5) Fit Huber Regression\n",
        "    huber = HuberRegressor(max_iter=1000, epsilon=1.5)\n",
        "    huber.fit(X_scaled, y_keep)\n",
        "\n",
        "    # 6) Predict & store results\n",
        "    pred = huber.predict(X_scaled)\n",
        "    listings_lr.loc[keep, \"market_value\"] = pred\n",
        "    listings_lr.loc[keep, \"excess_value\"] = pred - listings_lr.loc[keep, \"listed_price\"]\n",
        "\n",
        "    # 7) Recover coefficients on the original (unscaled) feature scale\n",
        "    coef_scaled = huber.coef_\n",
        "    mu = scaler.mean_\n",
        "    sigma = scaler.scale_\n",
        "\n",
        "    original_intercept = huber.intercept_ - np.sum(coef_scaled * (mu / sigma))\n",
        "    original_coefs = coef_scaled / sigma\n",
        "\n",
        "    coef_unscaled = pd.Series(\n",
        "        np.concatenate([[original_intercept], original_coefs]),\n",
        "        index=[\"intercept\"] + predictor_cols\n",
        "    )\n",
        "\n",
        "    listings_lr = listings_lr.loc[:, ~listings_lr.columns.str.startswith(\"mg_\")]\n",
        "\n",
        "    return listings_lr, coef_unscaled"
      ],
      "id": "23f91c4e",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##add_note"
      ],
      "metadata": {
        "id": "h6aKUgtSfCr3"
      },
      "id": "h6aKUgtSfCr3"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def add_note(notes_df: pd.DataFrame, author: str, href: str, status: str = None, note: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a new note or updates a status for a specific href in the `notes_df` DataFrame.\n",
        "    Checks if the status/note already exists and adds it only if new.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The DataFrame containing historical notes and statuses.\n",
        "        author (str): The author of the note/status update.\n",
        "        href (str): The href of the listing to update.\n",
        "        status (str, optional): The new status. Defaults to None.\n",
        "        note (str, optional): The new note. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    current_notes = notes_df.copy()\n",
        "\n",
        "    # Ensure 'timestamp' column is datetime for comparison\n",
        "    current_notes['timestamp'] = pd.to_datetime(current_notes['timestamp'], errors='coerce')\n",
        "\n",
        "    current_timestamp = pd.Timestamp.now(tz='UTC')\n",
        "    new_entries = []\n",
        "    href = href.replace('https://', '').replace('http://', '').replace('www.', '') # Clean href\n",
        "\n",
        "    # Filter existing records for the current href\n",
        "    existing_notes_for_href = current_notes[current_notes['href'] == href].copy()\n",
        "    existing_notes_for_href.sort_values(by='timestamp', ascending=False, inplace=True)\n",
        "\n",
        "    latest_status_in_notes = None\n",
        "    if not existing_notes_for_href.empty:\n",
        "        latest_status_in_notes = existing_notes_for_href.iloc[0]['status']\n",
        "\n",
        "    existing_note_texts = set(existing_notes_for_href['note'].dropna().tolist())\n",
        "\n",
        "    # Check and add new status if provided and different\n",
        "    if status is not None and status != latest_status_in_notes:\n",
        "        new_entries.append({\n",
        "            'href': href,\n",
        "            'timestamp': current_timestamp,\n",
        "            'author': author,\n",
        "            'status': status,\n",
        "            'note': pd.NA\n",
        "        })\n",
        "\n",
        "    # Check and add new note if provided and not already existing\n",
        "    if note is not None and note not in existing_note_texts:\n",
        "        new_entries.append({\n",
        "            'href': href,\n",
        "            'timestamp': current_timestamp,\n",
        "            'author': author,\n",
        "            'status': pd.NA, # Status is not changing, just adding a note\n",
        "            'note': note\n",
        "        })\n",
        "\n",
        "    if new_entries:\n",
        "        new_notes_df = pd.DataFrame(new_entries)\n",
        "        new_notes_df['timestamp'] = pd.to_datetime(new_notes_df['timestamp'])\n",
        "        # Ensure consistent column order and dtypes. If current_notes is empty, ensure new_notes_df has the correct columns.\n",
        "        if current_notes.empty:\n",
        "            # Define columns if current_notes is empty, assuming standard notes DataFrame columns\n",
        "            # This is a fallback and might need adjustment if notes columns vary significantly.\n",
        "            new_notes_df = new_notes_df.reindex(columns=['href', 'timestamp', 'author', 'status', 'note'])\n",
        "        else:\n",
        "            new_notes_df = new_notes_df.reindex(columns=current_notes.columns)\n",
        "\n",
        "        current_notes = pd.concat([current_notes, new_notes_df], ignore_index=True)\n",
        "\n",
        "        # --- User's requested logic for YAML export ---\n",
        "        if status in [\"message_left\", \"follow_up\"]:\n",
        "\n",
        "            # Get the latest status for each href in the *updated* notes DataFrame\n",
        "            latest_notes_status = current_notes.sort_values(by='timestamp', ascending=False).drop_duplicates(subset=['href'], keep='first')\n",
        "\n",
        "            # Filter for 'message_left' or 'follow_up' statuses\n",
        "            hrefs_for_yaml = latest_notes_status[\n",
        "                latest_notes_status['status'].isin([\"message_left\", \"follow_up\"])\n",
        "            ]['href'].unique().tolist()\n",
        "\n",
        "            if hrefs_for_yaml:\n",
        "                print(f\"Found {len(hrefs_for_yaml)} listings with current status 'message_left' or 'follow_up'.\")\n",
        "                # Access global variables for write_yaml\n",
        "                # These are assumed to be defined in the global scope of the notebook.\n",
        "                from __main__ import listings_lr, allocations, write_yaml\n",
        "\n",
        "                output_path = \"/content/drive/Shareddrives/market_analysis_v2/message_left.yaml\"\n",
        "                write_yaml(\n",
        "                    listings_to_print=hrefs_for_yaml,\n",
        "                    listings_lr=listings_lr,\n",
        "                    allocations=allocations,\n",
        "                    notes_df=current_notes, # Pass the updated notes DataFrame\n",
        "                    out_file=output_path,\n",
        "                    download=False # Do not prompt for download in a background task\n",
        "                )\n",
        "        # --- End of user's requested logic ---\n",
        "\n",
        "    return current_notes"
      ],
      "metadata": {
        "id": "3AqI6jYKe9GJ"
      },
      "id": "3AqI6jYKe9GJ",
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_notes"
      ],
      "metadata": {
        "id": "rylENVkd9dWD"
      },
      "id": "rylENVkd9dWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def update_notes(notes_df: pd.DataFrame, update_yaml: list, author: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes shortlist data and updates the `notes_df` DataFrame\n",
        "    with new status changes and notes using the `add_note` function.\n",
        "\n",
        "    Args:\n",
        "        notes_df (pd.DataFrame): The DataFrame containing historical notes and statuses.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "        author (str): The author of the notes.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated `notes_df` DataFrame.\n",
        "    \"\"\"\n",
        "    updated_notes_df = notes_df.copy()\n",
        "    initial_notes_count = len(updated_notes_df)\n",
        "\n",
        "    for listing in update_yaml:\n",
        "        current_href = listing['href']\n",
        "        current_status_from_yaml = listing['status']\n",
        "        notes_list_from_yaml = listing['notes'] if listing['notes'] is not None else []\n",
        "\n",
        "        # Update status using add_note\n",
        "        updated_notes_df = add_note(updated_notes_df, author, current_href, status=current_status_from_yaml)\n",
        "\n",
        "        # Update individual notes using add_note\n",
        "        for note_text in notes_list_from_yaml:\n",
        "            if pd.notna(note_text): # Ensure note_text is not NaN before passing\n",
        "                updated_notes_df = add_note(updated_notes_df, author, current_href, note=note_text)\n",
        "\n",
        "    final_notes_count = len(updated_notes_df)\n",
        "    added_entries_count = final_notes_count - initial_notes_count\n",
        "\n",
        "    if added_entries_count > 0:\n",
        "        print(f\"Total {added_entries_count} new entries added to notes DataFrame through update_notes.\")\n",
        "    else:\n",
        "        print(\"No new notes or status updates to add via update_notes.\")\n",
        "\n",
        "    return updated_notes_df"
      ],
      "metadata": {
        "id": "2dhYD0Mv9dy4"
      },
      "id": "2dhYD0Mv9dy4",
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_seller"
      ],
      "metadata": {
        "id": "0sQh5O7bDJyK"
      },
      "id": "0sQh5O7bDJyK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def update_seller(listings_df: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the 'seller' column in listings_df based on information from update_yaml.\n",
        "\n",
        "    Args:\n",
        "        update_yaml (list): A list of dictionaries parsed from a YAML file, potentially containing\n",
        "                                   'href' and 'seller' information.\n",
        "        listings_df (pd.DataFrame): The DataFrame of car listings to be updated.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified listings_df with updated 'seller' information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a copy to avoid modifying the original DataFrame directly\n",
        "    updated_listings_df = listings_df.copy()\n",
        "\n",
        "    # Ensure 'seller' column is of object (string) type to accommodate string assignments\n",
        "    # This prevents FutureWarning when assigning strings to a float64 column that might contain NaNs\n",
        "    if 'seller' in updated_listings_df.columns and updated_listings_df['seller'].dtype != object:\n",
        "        updated_listings_df['seller'] = updated_listings_df['seller'].astype(\"string\")\n",
        "\n",
        "    for item in update_yaml:\n",
        "        seller = item.get('seller')\n",
        "        href = item.get('href')\n",
        "\n",
        "        if seller is not None and href is not None:\n",
        "            # Clean the href string using re.sub for regex replacement\n",
        "            cleaned_href = re.sub(r'^(https?://)?(www\\.)?', '', str(href))\n",
        "\n",
        "            # Update the 'seller' column for matching 'href' entries\n",
        "            updated_listings_df.loc[updated_listings_df['href'] == cleaned_href, 'seller'] = seller\n",
        "\n",
        "    return updated_listings_df\n",
        "\n",
        "print(\"Defined `update_seller` function.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ogNU9_UDJPo",
        "outputId": "1860310d-1d69-47b6-de57-da7c9de33813"
      },
      "id": "4ogNU9_UDJPo",
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined `update_seller` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## update_allocations"
      ],
      "metadata": {
        "id": "c3Gz3FZoSO0o"
      },
      "id": "c3Gz3FZoSO0o"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def update_allocations(allocations: pd.DataFrame, update_yaml: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Updates the allocations DataFrame based on information from the update_yaml.\n",
        "\n",
        "    Args:\n",
        "        allocations (pd.DataFrame): The existing DataFrame of allocations.\n",
        "        update_yaml (list): A list of dictionaries parsed from shortlist-edited.yaml.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified allocations DataFrame.\n",
        "    \"\"\"\n",
        "    # 2. Create a copy of the input allocations DataFrame named allocations_copy.\n",
        "    allocations_copy = allocations.copy()\n",
        "\n",
        "    # 3. Iterate through update_yaml to create a dictionary update_lookup where keys are href values\n",
        "    # and values are dictionaries containing the status and a list of clients for that href from the YAML.\n",
        "    update_lookup = {}\n",
        "    for item in update_yaml:\n",
        "        href = item.get('href')\n",
        "        status = item.get('status')\n",
        "        clients_from_yaml = item.get('clients', [])\n",
        "        # Fix: Ensure clients_from_yaml is always a list, even if 'clients' key has a None value\n",
        "        if clients_from_yaml is None:\n",
        "            clients_from_yaml = []\n",
        "        if href:\n",
        "            # Clean the href string, consistent with enrich_df and update_seller\n",
        "            cleaned_href = href.replace('https://', '').replace('http://', '').replace('www.', '')\n",
        "            update_lookup[cleaned_href] = {\n",
        "                'status': status,\n",
        "                'clients': clients_from_yaml\n",
        "            }\n",
        "\n",
        "    # 4. Identify all hrefs from update_lookup that have a 'sold' or 'rejected' status\n",
        "    # and store them in a set called sold_rejected_hrefs.\n",
        "    sold_rejected_hrefs = set()\n",
        "    for href, data in update_lookup.items():\n",
        "        if data['status'] in ['sold', 'rejected']:\n",
        "            sold_rejected_hrefs.add(href)\n",
        "\n",
        "    # 5. Remove rows from allocations_copy where the href is present in sold_rejected_hrefs.\n",
        "    allocations_copy = allocations_copy[~allocations_copy['href'].isin(sold_rejected_hrefs)].copy()\n",
        "\n",
        "    # 6. Create a set current_active_allocations containing (href, client) tuples for all entries\n",
        "    # in the modified allocations_copy where allocation is True.\n",
        "    current_active_allocations = set(\n",
        "        allocations_copy[allocations_copy['allocation'] == True]\n",
        "        [['href', 'client']].apply(tuple, axis=1)\n",
        "    )\n",
        "\n",
        "    # 7. Create a set yaml_should_be_active containing (href, client) tuples for all clients\n",
        "    # associated with hrefs in update_lookup that are *not* in sold_rejected_hrefs.\n",
        "    yaml_should_be_active = set()\n",
        "    for href, data in update_lookup.items():\n",
        "        if href not in sold_rejected_hrefs:\n",
        "            for client in data['clients']:\n",
        "                yaml_should_be_active.add((href, client))\n",
        "\n",
        "    # Identify hrefs that are present in the update_yaml\n",
        "    hrefs_in_yaml = set(update_lookup.keys())\n",
        "\n",
        "    # Filter current_active_allocations to only include hrefs that are in the update_yaml\n",
        "    active_allocations_in_yaml_scope = {\n",
        "        (h, c) for h, c in current_active_allocations if h in hrefs_in_yaml\n",
        "    }\n",
        "\n",
        "    # 8. Determine the set of (href, client) pairs that need to be de-allocated.\n",
        "    # These are allocations that were active within the scope of hrefs mentioned in YAML,\n",
        "    # but are not present in the 'clients' list for those hrefs in the YAML.\n",
        "    to_deallocate = active_allocations_in_yaml_scope - yaml_should_be_active\n",
        "\n",
        "    # 9. Iterate through the identified (href, client) pairs and set the allocation column to\n",
        "    # False for those specific entries in allocations_copy.\n",
        "    for href, client in to_deallocate:\n",
        "        allocations_copy.loc[\n",
        "            (allocations_copy['href'] == href) & (allocations_copy['client'] == client),\n",
        "            'allocation'\n",
        "        ] = False\n",
        "\n",
        "    # Ensure the 'allocation' column is boolean type\n",
        "    allocations_copy['allocation'] = allocations_copy['allocation'].astype('boolean')\n",
        "\n",
        "    # 10. Return the modified allocations_copy DataFrame.\n",
        "    print(f\"De-allocated {len(to_deallocate)} entries based on YAML updates.\")\n",
        "    return allocations_copy"
      ],
      "metadata": {
        "id": "apPs89oFSOgu"
      },
      "id": "apPs89oFSOgu",
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##email_client"
      ],
      "metadata": {
        "id": "-j6txk8TrYiJ"
      },
      "id": "-j6txk8TrYiJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# def email_client(hrefs, listings_lr, coefficients, notes):\n",
        "\n",
        "# # print rows\n",
        "# for _, row in best_n.iterrows():\n",
        "#     print(f\"{model_name} ({row['rank']})\")\n",
        "#     print(f\"Link: {row['href']}\")\n",
        "#     print(f\"Market Value: ${row['predicted_price']:,.0f}\")\n",
        "#     print(f\"Listed Price: ${row[\"listed_price\"]:,}\")\n",
        "#     try:\n",
        "#         print(f\"Negotiated Price: ${row['nego_price']:,.0f}\")\n",
        "#     except KeyError as e:\n",
        "#         pass\n",
        "#     print(f\"Year: {row['year']:.0f}\")\n",
        "#     print(f\"Odometer: {row['odometer']:,.0f},000km\")\n",
        "#     print(f\"Notes:\\n\")\n",
        "\n",
        "\n",
        "# # Produce scatterplot\n",
        "# # Function to format price axis\n",
        "# def price_format(x, _):\n",
        "#     return f'${int(x):,}'\n",
        "\n",
        "# # Plotting\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# # Compute age\n",
        "# other_listings['age'] = 2026 - other_listings['year']\n",
        "# best_n['age'] = 2026 - best_n['year']\n",
        "\n",
        "# # Scatter (Year vs Price)\n",
        "# ax1.scatter(other_listings['year'], other_listings[\"listed_price\"],\n",
        "#             label='Data', color='lightsteelblue', s=20)\n",
        "\n",
        "# for _, row in best_n.iterrows():\n",
        "#     ax1.scatter(row['year'], row[\"listed_price\"], s=70, facecolors='none', linewidths=1.2)\n",
        "#     ax1.text(row['year'], row[\"listed_price\"], str(int(row['rank'])),\n",
        "#              ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#              color='red', alpha=0.7)\n",
        "#     if not pd.isna(row['nego_price']):\n",
        "#         ax1.text(row['year'], row['nego_price'], str(int(row['rank'])),\n",
        "#                  ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#                  color='green', alpha=0.7)\n",
        "\n",
        "# # Regression line (fix odometer at mean)\n",
        "# year_range = np.linspace(other_listings['year'].min(),\n",
        "#                          other_listings['year'].max(), 100)\n",
        "\n",
        "# age_range = 2026 - year_range  # convert back to age for model input\n",
        "# mean_odometer = other_listings['odometer'].mean()\n",
        "\n",
        "# X_line = pd.DataFrame({\n",
        "#     'const': 1,\n",
        "#     'age': age_range,\n",
        "#     'odometer': [mean_odometer] * 100\n",
        "# })\n",
        "\n",
        "# y_line = model.predict(X_line)\n",
        "\n",
        "# ax1.plot(year_range, y_line, label='Regression line')\n",
        "\n",
        "# ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# ax1.yaxis.set_major_formatter(FuncFormatter(price_format))\n",
        "# ax1.set_xlabel('Model Year')\n",
        "# ax1.set_ylabel(\"listed_price\")\n",
        "# ax1.set_title(f\"{model_name} Price vs Year\")\n",
        "\n",
        "# # Scatter (Odometer vs Price)\n",
        "# ax2.scatter(other_listings['odometer'], other_listings[\"listed_price\"],\n",
        "#             label='Data', color='lightsteelblue', s=20)\n",
        "\n",
        "# for _, row in best_n.iterrows():\n",
        "#     ax2.scatter(row['odometer'], row[\"listed_price\"], s=70, facecolors='none', linewidths=1.2)\n",
        "#     ax2.text(row['odometer'], row[\"listed_price\"], str(int(row['rank'])),\n",
        "#              ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#              color='red', alpha=0.7)\n",
        "#     if not pd.isna(row['nego_price']):\n",
        "#         ax2.text(row['odometer'], row['nego_price'], str(int(row['rank'])),\n",
        "#                  ha='center', va='center', fontsize=10, fontweight='bold',\n",
        "#                  color='green', alpha=0.7)\n",
        "\n",
        "# # Regression line (fix age at mean)\n",
        "# odometer_range = np.linspace(other_listings['odometer'].min(),\n",
        "#                              other_listings['odometer'].max(), 100)\n",
        "\n",
        "# mean_age = other_listings['age'].mean()\n",
        "\n",
        "# X_line2 = pd.DataFrame({\n",
        "#     'const': 1,\n",
        "#     'age': [mean_age] * 100,\n",
        "#     'odometer': odometer_range\n",
        "# })\n",
        "\n",
        "# y_line2 = model.predict(X_line2)\n",
        "# ax2.plot(odometer_range, y_line2, label='Regression line')\n",
        "\n",
        "# ax2.yaxis.set_major_formatter(FuncFormatter(price_format))\n",
        "# ax2.set_xlabel('Odometer (kms)')\n",
        "# ax2.set_ylabel(\"listed_price\")\n",
        "# ax2.set_title(f\"{model_name} Price vs Mileage\")\n",
        "\n",
        "\n",
        "# # Legend handles\n",
        "# live_listing_handle = Line2D([], [], marker='o', color='lightsteelblue', linestyle='None', markersize=6, label=f'Listing as of {df1.iloc[0][\"date_scraped\"]}')\n",
        "# listed_price_handle = Line2D([], [], marker='o', color='red', linestyle='None',\n",
        "#                                  markersize=8, label='Listed Price')\n",
        "# negotiated_price_handle = Line2D([], [], marker='o', color='green', linestyle='None',\n",
        "#                                  markersize=8, label='Negotiated Price')\n",
        "\n",
        "# # Apply legend to both subplots\n",
        "# ax1.legend(handles=[live_listing_handle, listed_price_handle, negotiated_price_handle])\n",
        "# ax2.legend(handles=[live_listing_handle, listed_price_handle, negotiated_price_handle])\n",
        "\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "udv17eUKrcwl"
      },
      "id": "udv17eUKrcwl",
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ZAdTyLbASdqQ"
      },
      "id": "ZAdTyLbASdqQ"
    },
    {
      "cell_type": "code",
      "source": [
        "clients=[\n",
        "    {\n",
        "        \"client\":\"anita_c\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"jazz_3\",\n",
        "            \"i30_2\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"magesh_t\",\n",
        "        \"max_listing_price\":13500,\n",
        "        \"max_odometer\":160,\n",
        "        \"model_gens\":[\n",
        "            \"3_3\",\n",
        "            \"civic_9\",\n",
        "            \"i30_2\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"client\":\"raymon_s\",\n",
        "        \"max_listing_price\":11000,\n",
        "        \"max_odometer\":210,\n",
        "        \"model_gens\":[\n",
        "            \"3_2\",\n",
        "            \"civic_8\",\n",
        "            \"i30_2\",\n",
        "            \"city_1\",\n",
        "            \"city_2\",\n",
        "            \"corolla_10\",\n",
        "            \"corolla_11\",\n",
        "        ]\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "Eb8E8j5Fsaxy"
      },
      "id": "Eb8E8j5Fsaxy",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "ae2ea91a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2ea91a",
        "outputId": "8a388b47-4ba0-447d-aa76-6ed885dd1b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, Optional, List\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataframes\n",
        "gen_lookup = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/gen_lookup.csv\")\n",
        "listings = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/listings.csv\")\n",
        "notes = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/notes.csv\", index_col=0)\n",
        "allocations = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/allocations.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "YJKBICNE_iDu"
      },
      "id": "YJKBICNE_iDu",
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statuses = {\n",
        "    None:\"No status saved\",\n",
        "    \"seen\": \"listing has been printed to YAML at least once\",\n",
        "    \"rejected\": \"listing not suitable for any buyer\",\n",
        "    \"sold\": \"sold or on hold\",\n",
        "    \"shortlisted\": \"VA checked listing and looks good\",\n",
        "    \"contacted\": \"Roger has contacted the seller\",\n",
        "    \"message_left\": \"self explanatory\",\n",
        "    \"follow_up\": \"Roger to call seller\",\n",
        "    \"inspection\": \"Inspection booked\",\n",
        "    \"deposit\": \"Deposit left with seller\",\n",
        "    \"purchased\": \"Self explainatory\",\n",
        "    \"bad_inspection\": \"Not recommended after inspection (Roger/Andrew)\",\n",
        "}"
      ],
      "metadata": {
        "id": "whpy5rTojg7Y"
      },
      "id": "whpy5rTojg7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working"
      ],
      "metadata": {
        "id": "d3osOXG0ExJz"
      },
      "id": "d3osOXG0ExJz"
    },
    {
      "cell_type": "code",
      "source": [
        "z = compare_new_listings(listings, gen_lookup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiTdqt2VxNCE",
        "outputId": "67881dc0-e031-4b98-9998-e7faeee88509"
      },
      "id": "BiTdqt2VxNCE",
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/carsales (1).csv    \t n_new=2   \t n_updated=1 \t n_unchanged=5 \t Tot 8\n",
            "/content/carsales (5).csv    \t n_new=9   \t n_updated=3 \t n_unchanged=10 \t Tot 22\n",
            "/content/carsales (7).csv    \t n_new=3   \t n_updated=1 \t n_unchanged=18 \t Tot 22\n",
            "/content/carsales (4).csv    \t n_new=10   \t n_updated=3 \t n_unchanged=9 \t Tot 22\n",
            "/content/carsales (10).csv    \t n_new=5   \t n_updated=0 \t n_unchanged=17 \t Tot 22\n",
            "/content/carsales (9).csv    \t n_new=5   \t n_updated=0 \t n_unchanged=17 \t Tot 22\n",
            "/content/carsales (2).csv    \t n_new=6   \t n_updated=7 \t n_unchanged=9 \t Tot 22\n",
            "/content/carsales (3).csv    \t n_new=10   \t n_updated=1 \t n_unchanged=11 \t Tot 22\n",
            "/content/carsales (6).csv    \t n_new=9   \t n_updated=2 \t n_unchanged=11 \t Tot 22\n",
            "/content/carsales (8).csv    \t n_new=8   \t n_updated=0 \t n_unchanged=14 \t Tot 22\n",
            "/content/carsales.csv    \t n_new=6   \t n_updated=2 \t n_unchanged=6 \t Tot 14\n",
            "/content/facebook (4).csv    \t n_new=32   \t n_updated=1 \t n_unchanged=11 \t Tot 44\n",
            "/content/facebook.csv    \t n_new=33   \t n_updated=2 \t n_unchanged=17 \t Tot 52\n",
            "/content/facebook (3).csv    \t n_new=43   \t n_updated=1 \t n_unchanged=8 \t Tot 52\n",
            "/content/facebook (1).csv    \t n_new=27   \t n_updated=5 \t n_unchanged=20 \t Tot 52\n",
            "/content/facebook (2).csv    \t n_new=42   \t n_updated=1 \t n_unchanged=9 \t Tot 52\n",
            "\t \t \t \t unq_new=248 \t unq_updated=30\t unq_unchanged=190 unq_tot=468\n",
            "WARNING: Column 'model_gen' in enriched_new_listings has 9 missing values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d173a02",
        "outputId": "bc7f3201-0890-4952-af90-1d31cda200ec"
      },
      "source": [
        "# Add new listings to listings dataframe\n",
        "listings = integrate_listings(listings, gen_lookup)"
      ],
      "id": "6d173a02",
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final DataFrame has 1097 unique listings after merging and de-duplication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29cd174a"
      },
      "source": [
        "listings_lr, coefficients = apply_regression(listings)"
      ],
      "id": "29cd174a",
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = allocate_listings(listings_lr, notes, allocations)\n",
        "listings_to_print = list(set(allocations[allocations[\"allocation\"]][\"href\"]))"
      ],
      "metadata": {
        "id": "xHaWD0sWsCbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a191b092-55be-4f59-d229-ac703d2c39b3"
      },
      "id": "xHaWD0sWsCbT",
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 27 new allocation entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "new_cell",
        "outputId": "4e0148e8-702b-4cb4-ac68-e8eb2c03e84f"
      },
      "source": [
        "# Call the updated output_shortlist function\n",
        "notes = write_yaml(listings_to_print, listings_lr, allocations, notes)\n"
      ],
      "id": "new_cell",
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_16cf5040-09de-46e9-86a8-13d2e8a471ae\", \"shortlist.yaml\", 4442)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The YAML file 'shortlist.yaml' has been generated and prompted for download with 22 listings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efded001",
        "outputId": "dc4e2c85-21d0-4f99-816b-610ccbe8f880"
      },
      "source": [
        "with open('/content/shortlist-edited.yaml', 'r') as file:\n",
        "    update_yaml = list(yaml.safe_load_all(file))\n",
        "\n",
        "print(\"YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\")"
      ],
      "id": "efded001",
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML file 'shortlist-edited.yaml' loaded successfully as 'update_yaml'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "author = \"roger\"\n",
        "notes = update_notes(notes, update_yaml, author)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOr7wbC_8yU",
        "outputId": "9b5bbf97-942c-4c24-adb7-4f8a6753e8ec"
      },
      "id": "4rOr7wbC_8yU",
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 12 new entries added to notes DataFrame through update_notes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listings = update_seller(listings, update_yaml)"
      ],
      "metadata": {
        "id": "14AXxgkRA5TY"
      },
      "id": "14AXxgkRA5TY",
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocations = update_allocations(allocations, update_yaml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izxraNyocXHW",
        "outputId": "18ed45f2-e3c1-481a-bab1-b06097b3fec9"
      },
      "id": "izxraNyocXHW",
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De-allocated 0 entries based on YAML updates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.read_csv(\"/content/drive/Shareddrives/market_analysis_v2/listings.csv\")"
      ],
      "metadata": {
        "id": "4lWaF7AIHLjV"
      },
      "id": "4lWaF7AIHLjV",
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assume:\n",
        "# a = old DataFrame\n",
        "# listings = new DataFrame\n",
        "# \"href\" uniquely identifies rows\n",
        "\n",
        "# 1. align on key\n",
        "a_idx = a.set_index(\"href\")\n",
        "l_idx = listings.set_index(\"href\")\n",
        "\n",
        "# 2. shared rows only\n",
        "common_idx = a_idx.index.intersection(l_idx.index)\n",
        "a_common = a_idx.loc[common_idx]\n",
        "l_common = l_idx.loc[common_idx]\n",
        "\n",
        "# 3. define change mask ONCE (NaN-safe)\n",
        "change_mask = (\n",
        "    a_common.ne(l_common)\n",
        "    & ~(a_common.isna() & l_common.isna())\n",
        ")\n",
        "\n",
        "# 4. new / removed rows\n",
        "n_new_rows = len(l_idx.index.difference(a_idx.index))\n",
        "n_removed_rows = len(a_idx.index.difference(l_idx.index))\n",
        "\n",
        "# 5. changed rows (any column)\n",
        "\n",
        "print(f\"Expected (new - removed rows)={len(listings)-len(a)}\")\n",
        "n_changed_rows = change_mask.any(axis=1).sum()\n",
        "\n",
        "print(f\"{n_new_rows=}\")\n",
        "print(f\"{n_removed_rows=}\")\n",
        "print(f\"{n_changed_rows=:.0f}\")\n",
        "\n",
        "# 6. row changes per column\n",
        "row_changes_per_column = change_mask.sum()\n",
        "\n",
        "print(row_changes_per_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rfPyJEhFDW1",
        "outputId": "0183c9af-6e07-42b1-aabd-07009f8b6c32"
      },
      "id": "6rfPyJEhFDW1",
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected (new - removed rows)=229\n",
            "n_new_rows=230\n",
            "n_removed_rows=1\n",
            "n_changed_rows=27\n",
            "age              1\n",
            "date_scraped    27\n",
            "gen              0\n",
            "listed_price    27\n",
            "location         1\n",
            "make             1\n",
            "model            1\n",
            "model_gen        1\n",
            "odometer         1\n",
            "seller           0\n",
            "seller_type      0\n",
            "trim             1\n",
            "year             1\n",
            "dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roger"
      ],
      "metadata": {
        "id": "VfqlksLNmY4W"
      },
      "id": "VfqlksLNmY4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# After\n",
        "\n",
        "new_notes = add_note(\n",
        "    new_notes,\n",
        "    \"roger\",\n",
        "    \"\", #href\n",
        "    status=\"message_left\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "1NTRYmb5mbH_",
        "outputId": "b96cf1ee-d9dd-405e-a6ae-01b063cb1f19"
      },
      "id": "1NTRYmb5mbH_",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'new_notes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2952586502.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m new_notes = add_note(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnew_notes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"roger\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#href\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contacted\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_notes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H4KefG6nb8O"
      },
      "id": "0H4KefG6nb8O",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}